{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrpc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhadarship/learning-to-unjumble/blob/master/mrpc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYQuspjFWqtJ",
        "colab_type": "text"
      },
      "source": [
        "# Setup working directory\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWF9WQibaBPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE91p5EaZXS1",
        "colab_type": "code",
        "outputId": "35b2a479-832e-4fba-eec3-6c8ed62d1fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRJD9PZOWp4X",
        "colab_type": "code",
        "outputId": "5a47594b-48ee-47cf-88e7-06419648fdbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "WORKING_DIR=os.path.join('/content', 'drive', 'My Drive', 'Colab Notebooks', 'NLU')\n",
        "os.path.exists(WORKING_DIR)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qziRV0mBW060",
        "colab_type": "code",
        "outputId": "30262930-7c74-4dfa-938f-bcdfc5d78c8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd $WORKING_DIR"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/NLU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmnJe06SfVMN",
        "colab_type": "text"
      },
      "source": [
        "# Clone transformers repo and checkout version 2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rtNjBbdY7RM",
        "colab_type": "code",
        "outputId": "682a287d-0407-4dc8-a61e-a017361dd33d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!rm -rf transformers\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 26386, done.\u001b[K\n",
            "remote: Total 26386 (delta 0), reused 0 (delta 0), pack-reused 26386\u001b[K\n",
            "Receiving objects: 100% (26386/26386), 15.87 MiB | 6.48 MiB/s, done.\n",
            "Resolving deltas: 100% (18385/18385), done.\n",
            "Checking out files: 100% (675/675), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU2wFP6SbsZf",
        "colab_type": "code",
        "outputId": "f861c291-b253-49e8-fa6d-ab4eed625380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd transformers/"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/NLU/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LON2QVZObFug",
        "colab_type": "code",
        "outputId": "3f6fad4a-a313-45bb-a70c-384f9a10f6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "!git checkout -f v2.7.0"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking out files: 100% (475/475), done.\n",
            "Note: checking out 'v2.7.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 6f5a12a5 Release: v2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnSg5LWjfn4w",
        "colab_type": "text"
      },
      "source": [
        "# Install packages required for running `run_glue.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScFAOBb2bRc-",
        "colab_type": "code",
        "outputId": "a9a40d2f-9633-44ae-d57e-f4420c3d5de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "!pip install -r ./examples/requirements.txt"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 1)) (2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 4)) (0.0.12)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 5)) (5.4.8)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 6)) (1.4.9)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 7)) (0.0.3)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.18.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (46.1.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.6.0.post3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.28.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.3.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (3.6.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (1.7.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (4.41.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.21.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.3.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (19.3.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.51.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDRsdU00kop7",
        "colab_type": "code",
        "outputId": "5ea299da-f01a-451b-ec03-20a8bcdb95e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "!pip install boto3 filelock requests tqdm sentencepiece sacremoses tokenizers"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.90)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.43)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.16.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INV4cGxyf2f5",
        "colab_type": "text"
      },
      "source": [
        "# Download GLUE data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWYcFKd9f6Fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLUE_DIR=\"/tmp/data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AJp5KgQglVh",
        "colab_type": "code",
        "outputId": "8be5c76d-26b3-46ae-818b-f13827e497da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo $GLUE_DIR"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24BX_hKwFhdU",
        "colab_type": "code",
        "outputId": "77d8c21d-9abe-4f2d-97d9-3fdcaf995de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "!python ./utils/download_glue_data.py --help"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: download_glue_data.py [-h] [--data_dir DATA_DIR] [--tasks TASKS]\n",
            "                             [--path_to_mrpc PATH_TO_MRPC]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_dir DATA_DIR   directory to save data to\n",
            "  --tasks TASKS         tasks to download data for as a comma separated string\n",
            "  --path_to_mrpc PATH_TO_MRPC\n",
            "                        path to directory containing extracted MRPC data,\n",
            "                        msr_paraphrase_train.txt and msr_paraphrase_text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAqf1Fe6cZvI",
        "colab_type": "code",
        "outputId": "fedfee9c-2d4c-43dd-eb85-a2fb1254d5e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "!python ./utils/download_glue_data.py --data_dir $GLUE_DIR --tasks MRPC"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing MRPC...\n",
            "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aqkcWzfgFNI",
        "colab_type": "text"
      },
      "source": [
        "# Compute RoBERTa score on MRPC task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFhfVflFdaVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TASK_NAME=\"MRPC\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_1EtLRQ8-WF",
        "colab_type": "code",
        "outputId": "13294e06-4625-4f34-e552-d08c8c8540dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo $TASK_NAME"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MRPC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_9qkZrE8xgk",
        "colab_type": "code",
        "outputId": "50cbcd6f-75c1-434f-bba8-9bfb5a069453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo $PYTHONPATH"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/NLU/transformers/src:/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szrKUidsj9kJ",
        "colab_type": "code",
        "outputId": "4f66b88e-420f-4646-f1ad-43b2ca0c8594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%set_env PYTHONPATH=$WORKING_DIR/transformers/src:/env/python"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=/content/drive/My Drive/Colab Notebooks/NLU/transformers/src:/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YoNLeB9kNi4",
        "colab_type": "code",
        "outputId": "65b25a3f-8427-4aff-ca8a-832296443f97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo $PYTHONPATH"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/NLU/transformers/src:/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUEfsVsjXVlC",
        "colab_type": "code",
        "outputId": "338d7bd8-d660-48d6-a138-fa3404928f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MODEL_DIR = os.path.join('/content', 'drive', 'My Drive', 'Colab Notebooks', 'NLU', 'models', 'random_0.15')\n",
        "os.path.exists(MODEL_DIR)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnxryCpnczcm",
        "colab_type": "code",
        "outputId": "9b434757-517d-488f-d28c-290bdbeb59fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./examples/run_glue.py \\\n",
        "    --model_type roberta \\\n",
        "    --model_name_or_path /content/drive/My\\ Drive/Colab\\ Notebooks/NLU/models/random_0.15 \\\n",
        "    --task_name $TASK_NAME \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=64   \\\n",
        "    --per_gpu_train_batch_size=64   \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --output_dir glue_model \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 02:39:31.472607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "05/15/2020 02:39:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/15/2020 02:39:34 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15/config.json\n",
            "05/15/2020 02:39:34 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenDiscrimination\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 02:39:34 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15/config.json\n",
            "05/15/2020 02:39:34 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenDiscrimination\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 02:39:34 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '/content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/15/2020 02:39:34 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15/added_tokens.json. We won't load it.\n",
            "05/15/2020 02:39:34 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15/vocab.json\n",
            "05/15/2020 02:39:34 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15/merges.txt\n",
            "05/15/2020 02:39:34 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/15/2020 02:39:34 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15/special_tokens_map.json\n",
            "05/15/2020 02:39:34 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15/tokenizer_config.json\n",
            "05/15/2020 02:39:38 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15/pytorch_model.bin\n",
            "05/15/2020 02:39:56 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "05/15/2020 02:39:56 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "05/15/2020 02:40:12 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/tmp/data/MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.0001, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='/content/drive/My Drive/Colab Notebooks/NLU/models/random_0.15', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='glue_model', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "05/15/2020 02:40:12 - INFO - __main__ -   Creating features from dataset file at /tmp/data/MRPC\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   LOOKING AT /tmp/data/MRPC/train.tsv\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   Writing example 0/3668\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   input_ids: 0 1918 1001 6182 1238 39 2138 2156 2661 37 373 22 5 4562 22 2156 9 12507 7018 23817 39 1283 479 2 2 24551 4506 7 123 25 129 22 5 4562 22 2156 1918 1001 6182 1238 39 2138 9 12507 7018 23817 39 1283 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   input_ids: 0 854 26802 1588 102 2164 13976 1758 128 29 137 2183 5 3206 7 11881 10564 11 6708 13 68 132 4 245 325 479 2 2 854 26802 1588 102 2162 13976 1758 128 29 11 7969 13 68 231 6478 153 8 1088 24 7 11881 10564 13 68 112 4 398 325 11 6708 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   input_ids: 0 252 56 1027 41 6859 15 5 3742 15 502 158 2156 1839 5 9145 13 1392 2156 37 355 479 2 2 374 502 158 2156 5 3627 128 29 2203 56 1027 41 6859 15 5 3742 2156 1839 5 16174 13 1392 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   input_ids: 0 8582 321 28020 5050 2156 12765 327 58 62 753 3205 2156 50 204 4 306 7606 2156 23 83 68 204 4 4419 2156 519 656 278 10 638 239 9 83 68 204 4 4390 479 2 2 12765 327 4262 291 3205 2156 50 204 4 401 7606 2156 7 278 10 638 3172 239 23 83 68 204 4 4390 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   guid: train-5\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   input_ids: 0 20 388 1458 68 132 4 1225 2156 50 59 365 135 2156 7 593 273 23 68 733 4 4708 15 5 188 469 3412 3080 479 2 2 14499 359 381 1913 4 327 4262 68 112 4 5449 50 290 135 7 68 733 4 3933 15 5 188 469 3412 3080 15 273 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:40:12 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 02:40:14 - INFO - __main__ -   Saving features into cached file /tmp/data/MRPC/cached_train_random_0.15_128_mrpc\n",
            "05/15/2020 02:40:14 - INFO - __main__ -   ***** Running training *****\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Num examples = 3668\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Num Epochs = 3\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Total optimization steps = 174\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Continuing training from epoch 0\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Continuing training from global step 0\n",
            "05/15/2020 02:40:14 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/58 [00:00<?, ?it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "\n",
            "Iteration:   2% 1/58 [00:01<00:58,  1.03s/it]\u001b[A\n",
            "Iteration:   3% 2/58 [00:01<00:52,  1.07it/s]\u001b[A\n",
            "Iteration:   5% 3/58 [00:02<00:47,  1.16it/s]\u001b[A\n",
            "Iteration:   7% 4/58 [00:03<00:44,  1.23it/s]\u001b[A\n",
            "Iteration:   9% 5/58 [00:03<00:41,  1.28it/s]\u001b[A\n",
            "Iteration:  10% 6/58 [00:04<00:39,  1.32it/s]\u001b[A\n",
            "Iteration:  12% 7/58 [00:05<00:37,  1.35it/s]\u001b[A\n",
            "Iteration:  14% 8/58 [00:05<00:36,  1.37it/s]\u001b[A\n",
            "Iteration:  16% 9/58 [00:06<00:35,  1.38it/s]\u001b[A\n",
            "Iteration:  17% 10/58 [00:07<00:34,  1.39it/s]\u001b[A\n",
            "Iteration:  19% 11/58 [00:08<00:33,  1.40it/s]\u001b[A\n",
            "Iteration:  21% 12/58 [00:08<00:32,  1.40it/s]\u001b[A\n",
            "Iteration:  22% 13/58 [00:09<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  24% 14/58 [00:10<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  26% 15/58 [00:10<00:30,  1.41it/s]\u001b[A\n",
            "Iteration:  28% 16/58 [00:11<00:29,  1.41it/s]\u001b[A\n",
            "Iteration:  29% 17/58 [00:12<00:29,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 18/58 [00:13<00:28,  1.41it/s]\u001b[A\n",
            "Iteration:  33% 19/58 [00:13<00:27,  1.41it/s]\u001b[A\n",
            "Iteration:  34% 20/58 [00:14<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:  36% 21/58 [00:15<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:  38% 22/58 [00:15<00:25,  1.41it/s]\u001b[A\n",
            "Iteration:  40% 23/58 [00:16<00:24,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 24/58 [00:17<00:24,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 25/58 [00:17<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  45% 26/58 [00:18<00:22,  1.42it/s]\u001b[A\n",
            "Iteration:  47% 27/58 [00:19<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  48% 28/58 [00:20<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  50% 29/58 [00:20<00:20,  1.42it/s]\u001b[A\n",
            "Iteration:  52% 30/58 [00:21<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  53% 31/58 [00:22<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  55% 32/58 [00:22<00:18,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 33/58 [00:23<00:17,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 34/58 [00:24<00:17,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 35/58 [00:25<00:16,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 36/58 [00:25<00:15,  1.42it/s]\u001b[A\n",
            "Iteration:  64% 37/58 [00:26<00:14,  1.42it/s]\u001b[A\n",
            "Iteration:  66% 38/58 [00:27<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 39/58 [00:27<00:13,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 40/58 [00:28<00:12,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 41/58 [00:29<00:12,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 42/58 [00:29<00:11,  1.41it/s]\u001b[A\n",
            "Iteration:  74% 43/58 [00:30<00:10,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 44/58 [00:31<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  78% 45/58 [00:32<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  79% 46/58 [00:32<00:08,  1.42it/s]\u001b[A\n",
            "Iteration:  81% 47/58 [00:33<00:07,  1.41it/s]\u001b[A\n",
            "Iteration:  83% 48/58 [00:34<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  84% 49/58 [00:34<00:06,  1.42it/s]\u001b[A\n",
            "Iteration:  86% 50/58 [00:35<00:05,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 51/58 [00:36<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 52/58 [00:37<00:04,  1.41it/s]\u001b[A\n",
            "Iteration:  91% 53/58 [00:37<00:03,  1.41it/s]\u001b[A\n",
            "Iteration:  93% 54/58 [00:38<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 55/58 [00:39<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 56/58 [00:39<00:01,  1.42it/s]\u001b[A\n",
            "Iteration:  98% 57/58 [00:40<00:00,  1.42it/s]\u001b[A\n",
            "Iteration: 100% 58/58 [00:40<00:00,  1.42it/s]\n",
            "Epoch:  33% 1/3 [00:40<01:21, 40.84s/it]\n",
            "Iteration:   0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/58 [00:00<00:40,  1.42it/s]\u001b[A\n",
            "Iteration:   3% 2/58 [00:01<00:39,  1.42it/s]\u001b[A\n",
            "Iteration:   5% 3/58 [00:02<00:38,  1.42it/s]\u001b[A\n",
            "Iteration:   7% 4/58 [00:02<00:38,  1.42it/s]\u001b[A\n",
            "Iteration:   9% 5/58 [00:03<00:37,  1.42it/s]\u001b[A\n",
            "Iteration:  10% 6/58 [00:04<00:36,  1.42it/s]\u001b[A\n",
            "Iteration:  12% 7/58 [00:04<00:36,  1.42it/s]\u001b[A\n",
            "Iteration:  14% 8/58 [00:05<00:35,  1.41it/s]\u001b[A\n",
            "Iteration:  16% 9/58 [00:06<00:34,  1.41it/s]\u001b[A\n",
            "Iteration:  17% 10/58 [00:07<00:33,  1.42it/s]\u001b[A\n",
            "Iteration:  19% 11/58 [00:07<00:33,  1.41it/s]\u001b[A\n",
            "Iteration:  21% 12/58 [00:08<00:32,  1.41it/s]\u001b[A\n",
            "Iteration:  22% 13/58 [00:09<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  24% 14/58 [00:09<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  26% 15/58 [00:10<00:30,  1.41it/s]\u001b[A\n",
            "Iteration:  28% 16/58 [00:11<00:29,  1.41it/s]\u001b[A\n",
            "Iteration:  29% 17/58 [00:12<00:29,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 18/58 [00:12<00:28,  1.41it/s]\u001b[A\n",
            "Iteration:  33% 19/58 [00:13<00:27,  1.41it/s]\u001b[A\n",
            "Iteration:  34% 20/58 [00:14<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:  36% 21/58 [00:14<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:  38% 22/58 [00:15<00:25,  1.41it/s]\u001b[A\n",
            "Iteration:  40% 23/58 [00:16<00:24,  1.42it/s]\u001b[A\n",
            "Iteration:  41% 24/58 [00:16<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 25/58 [00:17<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  45% 26/58 [00:18<00:22,  1.42it/s]\u001b[A\n",
            "Iteration:  47% 27/58 [00:19<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  48% 28/58 [00:19<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  50% 29/58 [00:20<00:20,  1.42it/s]\u001b[A\n",
            "Iteration:  52% 30/58 [00:21<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  53% 31/58 [00:21<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  55% 32/58 [00:22<00:18,  1.42it/s]\u001b[A\n",
            "Iteration:  57% 33/58 [00:23<00:17,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 34/58 [00:24<00:16,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 35/58 [00:24<00:16,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 36/58 [00:25<00:15,  1.42it/s]\u001b[A\n",
            "Iteration:  64% 37/58 [00:26<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 38/58 [00:26<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 39/58 [00:27<00:13,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 40/58 [00:28<00:12,  1.41it/s]\u001b[A\n",
            "Iteration:  71% 41/58 [00:28<00:12,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 42/58 [00:29<00:11,  1.41it/s]\u001b[A\n",
            "Iteration:  74% 43/58 [00:30<00:10,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 44/58 [00:31<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  78% 45/58 [00:31<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  79% 46/58 [00:32<00:08,  1.42it/s]\u001b[A\n",
            "Iteration:  81% 47/58 [00:33<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  83% 48/58 [00:33<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  84% 49/58 [00:34<00:06,  1.41it/s]\u001b[A\n",
            "Iteration:  86% 50/58 [00:35<00:05,  1.41it/s]\u001b[A\n",
            "Iteration:  88% 51/58 [00:36<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 52/58 [00:36<00:04,  1.41it/s]\u001b[A\n",
            "Iteration:  91% 53/58 [00:37<00:03,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 54/58 [00:38<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 55/58 [00:38<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 56/58 [00:39<00:01,  1.41it/s]\u001b[A\n",
            "Iteration:  98% 57/58 [00:40<00:00,  1.41it/s]\u001b[A\n",
            "Iteration: 100% 58/58 [00:40<00:00,  1.43it/s]\n",
            "Epoch:  67% 2/3 [01:21<00:40, 40.76s/it]\n",
            "Iteration:   0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/58 [00:00<00:40,  1.42it/s]\u001b[A\n",
            "Iteration:   3% 2/58 [00:01<00:39,  1.43it/s]\u001b[A\n",
            "Iteration:   5% 3/58 [00:02<00:38,  1.42it/s]\u001b[A\n",
            "Iteration:   7% 4/58 [00:02<00:38,  1.42it/s]\u001b[A\n",
            "Iteration:   9% 5/58 [00:03<00:37,  1.42it/s]\u001b[A\n",
            "Iteration:  10% 6/58 [00:04<00:36,  1.42it/s]\u001b[A\n",
            "Iteration:  12% 7/58 [00:04<00:36,  1.42it/s]\u001b[A\n",
            "Iteration:  14% 8/58 [00:05<00:35,  1.42it/s]\u001b[A\n",
            "Iteration:  16% 9/58 [00:06<00:34,  1.42it/s]\u001b[A\n",
            "Iteration:  17% 10/58 [00:07<00:33,  1.42it/s]\u001b[A\n",
            "Iteration:  19% 11/58 [00:07<00:33,  1.42it/s]\u001b[A\n",
            "Iteration:  21% 12/58 [00:08<00:32,  1.42it/s]\u001b[A\n",
            "Iteration:  22% 13/58 [00:09<00:31,  1.41it/s]\u001b[A\n",
            "Iteration:  24% 14/58 [00:09<00:31,  1.42it/s]\u001b[A\n",
            "Iteration:  26% 15/58 [00:10<00:30,  1.42it/s]\u001b[A\n",
            "Iteration:  28% 16/58 [00:11<00:29,  1.42it/s]\u001b[A\n",
            "Iteration:  29% 17/58 [00:11<00:28,  1.42it/s]\u001b[A\n",
            "Iteration:  31% 18/58 [00:12<00:28,  1.42it/s]\u001b[A\n",
            "Iteration:  33% 19/58 [00:13<00:27,  1.42it/s]\u001b[A\n",
            "Iteration:  34% 20/58 [00:14<00:26,  1.42it/s]\u001b[A\n",
            "Iteration:  36% 21/58 [00:14<00:26,  1.42it/s]\u001b[A\n",
            "Iteration:  38% 22/58 [00:15<00:25,  1.42it/s]\u001b[A\n",
            "Iteration:  40% 23/58 [00:16<00:24,  1.42it/s]\u001b[A\n",
            "Iteration:  41% 24/58 [00:16<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  43% 25/58 [00:17<00:23,  1.42it/s]\u001b[A\n",
            "Iteration:  45% 26/58 [00:18<00:22,  1.42it/s]\u001b[A\n",
            "Iteration:  47% 27/58 [00:19<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  48% 28/58 [00:19<00:21,  1.42it/s]\u001b[A\n",
            "Iteration:  50% 29/58 [00:20<00:20,  1.42it/s]\u001b[A\n",
            "Iteration:  52% 30/58 [00:21<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  53% 31/58 [00:21<00:19,  1.42it/s]\u001b[A\n",
            "Iteration:  55% 32/58 [00:22<00:18,  1.41it/s]\u001b[A\n",
            "Iteration:  57% 33/58 [00:23<00:17,  1.42it/s]\u001b[A\n",
            "Iteration:  59% 34/58 [00:23<00:16,  1.41it/s]\u001b[A\n",
            "Iteration:  60% 35/58 [00:24<00:16,  1.42it/s]\u001b[A\n",
            "Iteration:  62% 36/58 [00:25<00:15,  1.41it/s]\u001b[A\n",
            "Iteration:  64% 37/58 [00:26<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  66% 38/58 [00:26<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 39/58 [00:27<00:13,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 40/58 [00:28<00:12,  1.42it/s]\u001b[A\n",
            "Iteration:  71% 41/58 [00:28<00:12,  1.42it/s]\u001b[A\n",
            "Iteration:  72% 42/58 [00:29<00:11,  1.42it/s]\u001b[A\n",
            "Iteration:  74% 43/58 [00:30<00:10,  1.42it/s]\u001b[A\n",
            "Iteration:  76% 44/58 [00:31<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  78% 45/58 [00:31<00:09,  1.42it/s]\u001b[A\n",
            "Iteration:  79% 46/58 [00:32<00:08,  1.42it/s]\u001b[A\n",
            "Iteration:  81% 47/58 [00:33<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  83% 48/58 [00:33<00:07,  1.42it/s]\u001b[A\n",
            "Iteration:  84% 49/58 [00:34<00:06,  1.42it/s]\u001b[A\n",
            "Iteration:  86% 50/58 [00:35<00:05,  1.42it/s]\u001b[A\n",
            "Iteration:  88% 51/58 [00:35<00:04,  1.42it/s]\u001b[A\n",
            "Iteration:  90% 52/58 [00:36<00:04,  1.41it/s]\u001b[A\n",
            "Iteration:  91% 53/58 [00:37<00:03,  1.42it/s]\u001b[A\n",
            "Iteration:  93% 54/58 [00:38<00:02,  1.42it/s]\u001b[A\n",
            "Iteration:  95% 55/58 [00:38<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 56/58 [00:39<00:01,  1.41it/s]\u001b[A\n",
            "Iteration:  98% 57/58 [00:40<00:00,  1.41it/s]\u001b[A\n",
            "Iteration: 100% 58/58 [00:40<00:00,  1.43it/s]\n",
            "Epoch: 100% 3/3 [02:01<00:00, 40.65s/it]\n",
            "05/15/2020 02:42:16 - INFO - __main__ -    global_step = 174, average loss = 0.40460116754489384\n",
            "05/15/2020 02:42:16 - INFO - __main__ -   Saving model checkpoint to glue_model\n",
            "05/15/2020 02:42:16 - INFO - transformers.configuration_utils -   Configuration saved in glue_model/config.json\n",
            "05/15/2020 02:42:18 - INFO - transformers.modeling_utils -   Model weights saved in glue_model/pytorch_model.bin\n",
            "05/15/2020 02:42:18 - INFO - transformers.configuration_utils -   loading configuration file glue_model/config.json\n",
            "05/15/2020 02:42:18 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 02:42:18 - INFO - transformers.modeling_utils -   loading weights file glue_model/pytorch_model.bin\n",
            "05/15/2020 02:42:22 - INFO - transformers.configuration_utils -   loading configuration file glue_model/config.json\n",
            "05/15/2020 02:42:22 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   Model name 'glue_model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'glue_model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   Didn't find file glue_model/added_tokens.json. We won't load it.\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file glue_model/vocab.json\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file glue_model/merges.txt\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file glue_model/special_tokens_map.json\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file glue_model/tokenizer_config.json\n",
            "05/15/2020 02:42:22 - INFO - transformers.configuration_utils -   loading configuration file glue_model/config.json\n",
            "05/15/2020 02:42:22 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   Model name 'glue_model' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'glue_model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   Didn't find file glue_model/added_tokens.json. We won't load it.\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file glue_model/vocab.json\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file glue_model/merges.txt\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file glue_model/special_tokens_map.json\n",
            "05/15/2020 02:42:22 - INFO - transformers.tokenization_utils -   loading file glue_model/tokenizer_config.json\n",
            "05/15/2020 02:42:22 - INFO - __main__ -   Evaluate the following checkpoints: ['glue_model']\n",
            "05/15/2020 02:42:22 - INFO - transformers.configuration_utils -   loading configuration file glue_model/config.json\n",
            "05/15/2020 02:42:22 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 02:42:23 - INFO - transformers.modeling_utils -   loading weights file glue_model/pytorch_model.bin\n",
            "05/15/2020 02:42:27 - INFO - __main__ -   Creating features from dataset file at /tmp/data/MRPC\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   Writing example 0/408\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   input_ids: 0 91 26 5 689 11131 11637 265 630 128 90 2564 5 138 128 29 251 12 1279 434 1860 479 2 2 22 20 689 11131 11637 265 473 45 2564 84 251 12 1279 434 1860 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   input_ids: 0 15683 1322 10054 26 14822 636 1242 19975 5 7780 4304 8 1415 556 7 634 39 251 107 9 1058 11 5 997 479 2 2 832 1141 26 37 21 22 727 135 639 1655 3516 22 8 1415 556 7 634 39 107 9 1058 11 5 997 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   input_ids: 0 20 1404 21 23 15966 4 6617 4796 136 5 4796 2156 3269 15 5 1852 2156 8 23 112 4 2517 6468 136 5 5092 13638 2156 67 3269 479 2 2 20 1404 21 23 15966 4 5479 4796 14621 975 5457 2156 8077 3269 15 5 1852 2156 8 23 112 4 2517 5339 136 5 5092 13638 3858 597 5457 2156 159 321 4 134 135 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   input_ids: 0 20 16305 12 347 6454 16 2445 454 779 7 2845 114 24 40 18839 10 1984 479 2 2 20 16305 12 347 6454 585 307 14 24 40 2845 11 779 549 7 18839 10 1984 137 5 19050 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   input_ids: 0 440 5461 33 57 278 13 5 2366 50 5 1837 1500 479 2 2 440 5461 33 57 278 13 5 1837 50 2366 1200 2156 53 18966 607 34 4407 45 2181 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 02:42:27 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "05/15/2020 02:42:27 - INFO - __main__ -   Saving features into cached file /tmp/data/MRPC/cached_dev_random_0.15_128_mrpc\n",
            "05/15/2020 02:42:27 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "05/15/2020 02:42:27 - INFO - __main__ -     Num examples = 408\n",
            "05/15/2020 02:42:27 - INFO - __main__ -     Batch size = 64\n",
            "Evaluating: 100% 7/7 [00:01<00:00,  4.77it/s]\n",
            "05/15/2020 02:42:29 - INFO - __main__ -   ***** Eval results  *****\n",
            "05/15/2020 02:42:29 - INFO - __main__ -     acc = 0.8676470588235294\n",
            "05/15/2020 02:42:29 - INFO - __main__ -     acc_and_f1 = 0.886455108359133\n",
            "05/15/2020 02:42:29 - INFO - __main__ -     f1 = 0.9052631578947367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdGtUfYNRu8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}