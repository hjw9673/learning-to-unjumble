{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sts-b.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"16YArT8SiamhdbPHA4HcEY5AnMdzUUPl1","authorship_tag":"ABX9TyN5cFJgldiQD/Z8ZbwGe7J1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qmnJe06SfVMN","colab_type":"text"},"source":["# Install transformers 2.8.0"]},{"cell_type":"code","metadata":{"id":"_rtNjBbdY7RM","colab_type":"code","outputId":"845c8de2-c932-47ab-e93d-afae03d601e1","executionInfo":{"status":"ok","timestamp":1589509099786,"user_tz":240,"elapsed":9308,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":706}},"source":["!pip install transformers==2.8.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers==2.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n","\r\u001b[K     |▋                               | 10kB 22.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 28.7MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 34.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 32.5MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 29.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 32.3MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 26.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 22.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 21.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 22.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 22.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 22.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 22.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.4)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 48.7MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 61.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 45.4MB/s \n","\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n","Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.16.4)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.8.0) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.8.0) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=0e9c5a0cb308ac8e4804fde3ba74859d0ee3a4cbac47741749529b5ec4ba37c1\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.5.2 transformers-2.8.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M_moWpok-qkX","colab_type":"text"},"source":["# Set working directory to the directory containing `download_glue_data.py` and `run_glue.py`"]},{"cell_type":"code","metadata":{"id":"l0VFWNoOAfT-","colab_type":"code","outputId":"21e8c63b-e513-40c5-c7a0-3296344c7252","executionInfo":{"status":"ok","timestamp":1589509099790,"user_tz":240,"elapsed":866,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cd ~/../content/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JU2wFP6SbsZf","colab_type":"code","colab":{}},"source":["import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LON2QVZObFug","colab_type":"code","colab":{}},"source":["WORK_DIR = os.path.join('drive', 'My Drive', 'Colab Notebooks', 'NLU')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mWKKk1x86hQ","colab_type":"code","outputId":"9ea45ef7-e6eb-40b4-8da3-44d32ca47c86","executionInfo":{"status":"ok","timestamp":1589509105314,"user_tz":240,"elapsed":1593,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["os.path.exists(WORK_DIR)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"cU0UdJuH9F2Y","colab_type":"code","outputId":"6e6aaa95-ee79-44a7-81b0-e1b11b651e24","executionInfo":{"status":"ok","timestamp":1589509106327,"user_tz":240,"elapsed":1047,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["cd $WORK_DIR"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/NLU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"INV4cGxyf2f5","colab_type":"text"},"source":["# Download GLUE data"]},{"cell_type":"code","metadata":{"id":"PWYcFKd9f6Fb","colab_type":"code","colab":{}},"source":["GLUE_DIR=\"data/glue\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AJp5KgQglVh","colab_type":"code","outputId":"d02726cd-1d46-4a96-94af-718d22098d87","executionInfo":{"status":"ok","timestamp":1589509118839,"user_tz":240,"elapsed":7120,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!echo $GLUE_DIR"],"execution_count":8,"outputs":[{"output_type":"stream","text":["data/glue\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LFhfVflFdaVB","colab_type":"code","colab":{}},"source":["TASK_NAME=\"STS\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_1EtLRQ8-WF","colab_type":"code","outputId":"a832d1d7-5d1b-473a-aaf9-67ec2eb2f18f","executionInfo":{"status":"ok","timestamp":1589509348451,"user_tz":240,"elapsed":6763,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["!echo $TASK_NAME"],"execution_count":11,"outputs":[{"output_type":"stream","text":["STS\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"24BX_hKwFhdU","colab_type":"code","outputId":"d7341359-937f-45e9-90f2-16ba36312253","executionInfo":{"status":"ok","timestamp":1589509354160,"user_tz":240,"elapsed":10571,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":193}},"source":["!python download_glue_data.py --help"],"execution_count":12,"outputs":[{"output_type":"stream","text":["usage: download_glue_data.py [-h] [--data_dir DATA_DIR] [--tasks TASKS]\n","                             [--path_to_mrpc PATH_TO_MRPC]\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --data_dir DATA_DIR   directory to save data to\n","  --tasks TASKS         tasks to download data for as a comma separated string\n","  --path_to_mrpc PATH_TO_MRPC\n","                        path to directory containing extracted MRPC data,\n","                        msr_paraphrase_train.txt and msr_paraphrase_text.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rAqf1Fe6cZvI","colab_type":"code","outputId":"e35c36f4-2936-4520-872e-002c4b5088ea","executionInfo":{"status":"ok","timestamp":1589509358494,"user_tz":240,"elapsed":13247,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["!python download_glue_data.py --data_dir $GLUE_DIR --tasks $TASK_NAME"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Downloading and extracting STS...\n","\tCompleted!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6aqkcWzfgFNI","colab_type":"text"},"source":["# Compute score for specific GLUE task"]},{"cell_type":"code","metadata":{"id":"z4jsBBTfnwOa","colab_type":"code","outputId":"1024676c-d153-4655-cd3c-67529ee37fc6","executionInfo":{"status":"ok","timestamp":1589509438380,"user_tz":240,"elapsed":1529,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["MODEL_DIR = os.path.join('models', 'roberta_jumbled_token_discrimination_lr_e-4_prob_0.15')\n","os.path.exists(MODEL_DIR)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"eiWazMGIsMk9","colab_type":"code","outputId":"2ac1ee2a-cb7b-4b86-a379-a50d5861c96a","executionInfo":{"status":"ok","timestamp":1589509529651,"user_tz":240,"elapsed":8262,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":316}},"source":["!nvidia-smi"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Fri May 15 02:25:23 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SPpl0f8wAkmf","colab_type":"code","outputId":"15a55684-f074-4f05-887d-96e21661f089","executionInfo":{"status":"ok","timestamp":1589509541063,"user_tz":240,"elapsed":14221,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run_glue.py --help"],"execution_count":20,"outputs":[{"output_type":"stream","text":["2020-05-15 02:25:35.635909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","usage: run_glue.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n","                   --model_name_or_path MODEL_NAME_OR_PATH --task_name\n","                   TASK_NAME --output_dir OUTPUT_DIR\n","                   [--config_name CONFIG_NAME]\n","                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n","                   [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n","                   [--evaluate_during_training] [--do_lower_case]\n","                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n","                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n","                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n","                   [--learning_rate LEARNING_RATE]\n","                   [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n","                   [--max_grad_norm MAX_GRAD_NORM]\n","                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n","                   [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n","                   [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n","                   [--eval_all_checkpoints] [--no_cuda]\n","                   [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n","                   [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n","                   [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n","                   [--server_port SERVER_PORT]\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n","                        other data files) for the task.\n","  --model_type MODEL_TYPE\n","                        Model type selected in the list: distilbert, albert,\n","                        camembert, xlm-roberta, bart, roberta, bert, xlnet,\n","                        flaubert, xlm\n","  --model_name_or_path MODEL_NAME_OR_PATH\n","                        Path to pre-trained model or shortcut name selected in\n","                        the list: distilbert-base-uncased, distilbert-base-\n","                        uncased-distilled-squad, distilbert-base-cased,\n","                        distilbert-base-cased-distilled-squad, distilbert-\n","                        base-german-cased, distilbert-base-multilingual-cased,\n","                        distilbert-base-uncased-finetuned-sst-2-english,\n","                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\n","                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\n","                        albert-xlarge-v2, albert-xxlarge-v2, camembert-base,\n","                        umberto-commoncrawl-cased-v1, umberto-wikipedia-\n","                        uncased-v1, xlm-roberta-base, xlm-roberta-large, xlm-\n","                        roberta-large-finetuned-conll02-dutch, xlm-roberta-\n","                        large-finetuned-conll02-spanish, xlm-roberta-large-\n","                        finetuned-conll03-english, xlm-roberta-large-\n","                        finetuned-conll03-german, bart-large, bart-large-mnli,\n","                        bart-large-cnn, bart-large-xsum, roberta-base,\n","                        roberta-large, roberta-large-mnli, distilroberta-base,\n","                        roberta-base-openai-detector, roberta-large-openai-\n","                        detector, bert-base-uncased, bert-large-uncased, bert-\n","                        base-cased, bert-large-cased, bert-base-multilingual-\n","                        uncased, bert-base-multilingual-cased, bert-base-\n","                        chinese, bert-base-german-cased, bert-large-uncased-\n","                        whole-word-masking, bert-large-cased-whole-word-\n","                        masking, bert-large-uncased-whole-word-masking-\n","                        finetuned-squad, bert-large-cased-whole-word-masking-\n","                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\n","                        base-german-dbmdz-cased, bert-base-german-dbmdz-\n","                        uncased, bert-base-japanese, bert-base-japanese-whole-\n","                        word-masking, bert-base-japanese-char, bert-base-\n","                        japanese-char-whole-word-masking, bert-base-finnish-\n","                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\n","                        dutch-cased, xlnet-base-cased, xlnet-large-cased,\n","                        flaubert-small-cased, flaubert-base-uncased, flaubert-\n","                        base-cased, flaubert-large-cased, xlm-mlm-en-2048,\n","                        xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-\n","                        enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\n","                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\n","                        xlm-mlm-17-1280, xlm-mlm-100-1280\n","  --task_name TASK_NAME\n","                        The name of the task to train selected in the list:\n","                        cola, mnli, mnli-mm, mrpc, sst-2, sts-b, qqp, qnli,\n","                        rte, wnli\n","  --output_dir OUTPUT_DIR\n","                        The output directory where the model predictions and\n","                        checkpoints will be written.\n","  --config_name CONFIG_NAME\n","                        Pretrained config name or path if not the same as\n","                        model_name\n","  --tokenizer_name TOKENIZER_NAME\n","                        Pretrained tokenizer name or path if not the same as\n","                        model_name\n","  --cache_dir CACHE_DIR\n","                        Where do you want to store the pre-trained models\n","                        downloaded from s3\n","  --max_seq_length MAX_SEQ_LENGTH\n","                        The maximum total input sequence length after\n","                        tokenization. Sequences longer than this will be\n","                        truncated, sequences shorter will be padded.\n","  --do_train            Whether to run training.\n","  --do_eval             Whether to run eval on the dev set.\n","  --evaluate_during_training\n","                        Run evaluation during training at each logging step.\n","  --do_lower_case       Set this flag if you are using an uncased model.\n","  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n","                        Batch size per GPU/CPU for training.\n","  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n","                        Batch size per GPU/CPU for evaluation.\n","  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n","                        Number of updates steps to accumulate before\n","                        performing a backward/update pass.\n","  --learning_rate LEARNING_RATE\n","                        The initial learning rate for Adam.\n","  --weight_decay WEIGHT_DECAY\n","                        Weight decay if we apply some.\n","  --adam_epsilon ADAM_EPSILON\n","                        Epsilon for Adam optimizer.\n","  --max_grad_norm MAX_GRAD_NORM\n","                        Max gradient norm.\n","  --num_train_epochs NUM_TRAIN_EPOCHS\n","                        Total number of training epochs to perform.\n","  --max_steps MAX_STEPS\n","                        If > 0: set total number of training steps to perform.\n","                        Override num_train_epochs.\n","  --warmup_steps WARMUP_STEPS\n","                        Linear warmup over warmup_steps.\n","  --logging_steps LOGGING_STEPS\n","                        Log every X updates steps.\n","  --save_steps SAVE_STEPS\n","                        Save checkpoint every X updates steps.\n","  --eval_all_checkpoints\n","                        Evaluate all checkpoints starting with the same prefix\n","                        as model_name ending and ending with step number\n","  --no_cuda             Avoid using CUDA when available\n","  --overwrite_output_dir\n","                        Overwrite the content of the output directory\n","  --overwrite_cache     Overwrite the cached training and evaluation sets\n","  --seed SEED           random seed for initialization\n","  --fp16                Whether to use 16-bit (mixed) precision (through\n","                        NVIDIA apex) instead of 32-bit\n","  --fp16_opt_level FP16_OPT_LEVEL\n","                        For fp16: Apex AMP optimization level selected in\n","                        ['O0', 'O1', 'O2', and 'O3'].See details at\n","                        https://nvidia.github.io/apex/amp.html\n","  --local_rank LOCAL_RANK\n","                        For distributed training: local_rank\n","  --server_ip SERVER_IP\n","                        For distant debugging.\n","  --server_port SERVER_PORT\n","                        For distant debugging.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HnxryCpnczcm","colab_type":"code","outputId":"eccba9a7-b975-4806-97fb-2962efea7013","executionInfo":{"status":"ok","timestamp":1589511777386,"user_tz":240,"elapsed":435169,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run_glue.py \\\n","    --model_type roberta \\\n","    --model_name_or_path $MODEL_DIR \\\n","    --task_name STS-B \\\n","    --do_train \\\n","    --do_eval \\\n","    --data_dir $GLUE_DIR/STS-B \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size 32   \\\n","    --per_gpu_train_batch_size 32   \\\n","    --gradient_accumulation_steps 2\\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 3 \\\n","    --output_dir run \\\n","    --overwrite_output_dir"],"execution_count":27,"outputs":[{"output_type":"stream","text":["2020-05-15 02:55:44.686840: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","05/15/2020 02:55:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","05/15/2020 02:55:46 - INFO - transformers.configuration_utils -   loading configuration file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/config.json\n","05/15/2020 02:55:46 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 1,\n","  \"architectures\": [\n","    \"RobertaForTokenDiscrimination\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"sts-b\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/15/2020 02:55:46 - INFO - transformers.configuration_utils -   loading configuration file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/config.json\n","05/15/2020 02:55:46 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"RobertaForTokenDiscrimination\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/15/2020 02:55:46 - INFO - transformers.tokenization_utils -   Model name 'models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15' is a path, a model identifier, or url to a directory containing tokenizer files.\n","05/15/2020 02:55:46 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/added_tokens.json. We won't load it.\n","05/15/2020 02:55:46 - INFO - transformers.tokenization_utils -   loading file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/vocab.json\n","05/15/2020 02:55:46 - INFO - transformers.tokenization_utils -   loading file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/merges.txt\n","05/15/2020 02:55:46 - INFO - transformers.tokenization_utils -   loading file None\n","05/15/2020 02:55:46 - INFO - transformers.tokenization_utils -   loading file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/special_tokens_map.json\n","05/15/2020 02:55:46 - INFO - transformers.tokenization_utils -   loading file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/tokenizer_config.json\n","05/15/2020 02:55:46 - INFO - transformers.modeling_utils -   loading weights file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/pytorch_model.bin\n","05/15/2020 02:55:51 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","05/15/2020 02:55:51 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n","05/15/2020 02:55:54 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data/glue/STS-B', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=2, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='run', output_mode='regression', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=32, save_steps=500, seed=42, server_ip='', server_port='', task_name='sts-b', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n","05/15/2020 02:55:54 - INFO - __main__ -   Loading features from cached file data/glue/STS-B/cached_train_roberta_jumbled_token_discrimination_lr_e-4_prob_0.15_128_sts-b\n","05/15/2020 02:55:54 - INFO - __main__ -   ***** Running training *****\n","05/15/2020 02:55:54 - INFO - __main__ -     Num examples = 5749\n","05/15/2020 02:55:54 - INFO - __main__ -     Num Epochs = 3\n","05/15/2020 02:55:54 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n","05/15/2020 02:55:54 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","05/15/2020 02:55:54 - INFO - __main__ -     Gradient Accumulation steps = 2\n","05/15/2020 02:55:54 - INFO - __main__ -     Total optimization steps = 270\n","05/15/2020 02:55:54 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n","05/15/2020 02:55:54 - INFO - __main__ -     Continuing training from epoch 0\n","05/15/2020 02:55:54 - INFO - __main__ -     Continuing training from global step 0\n","05/15/2020 02:55:54 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n","Epoch:   0% 0/3 [00:00<?, ?it/s]\n","Iteration:   0% 0/180 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/180 [00:00<02:03,  1.46it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n","\n","Iteration:   1% 2/180 [00:01<02:06,  1.41it/s]\u001b[A\n","Iteration:   2% 3/180 [00:02<02:03,  1.43it/s]\u001b[A\n","Iteration:   2% 4/180 [00:02<02:05,  1.40it/s]\u001b[A\n","Iteration:   3% 5/180 [00:03<02:02,  1.42it/s]\u001b[A\n","Iteration:   3% 6/180 [00:04<02:04,  1.39it/s]\u001b[A\n","Iteration:   4% 7/180 [00:04<02:01,  1.42it/s]\u001b[A\n","Iteration:   4% 8/180 [00:05<02:03,  1.39it/s]\u001b[A\n","Iteration:   5% 9/180 [00:06<02:00,  1.42it/s]\u001b[A\n","Iteration:   6% 10/180 [00:07<02:02,  1.39it/s]\u001b[A\n","Iteration:   6% 11/180 [00:07<01:59,  1.42it/s]\u001b[A\n","Iteration:   7% 12/180 [00:08<02:00,  1.39it/s]\u001b[A\n","Iteration:   7% 13/180 [00:09<01:58,  1.41it/s]\u001b[A\n","Iteration:   8% 14/180 [00:10<01:59,  1.39it/s]\u001b[A\n","Iteration:   8% 15/180 [00:10<01:56,  1.41it/s]\u001b[A\n","Iteration:   9% 16/180 [00:11<01:57,  1.39it/s]\u001b[A\n","Iteration:   9% 17/180 [00:12<01:55,  1.41it/s]\u001b[A\n","Iteration:  10% 18/180 [00:12<01:56,  1.39it/s]\u001b[A\n","Iteration:  11% 19/180 [00:13<01:53,  1.41it/s]\u001b[A\n","Iteration:  11% 20/180 [00:14<01:55,  1.39it/s]\u001b[A\n","Iteration:  12% 21/180 [00:14<01:52,  1.41it/s]\u001b[A\n","Iteration:  12% 22/180 [00:15<01:53,  1.39it/s]\u001b[A\n","Iteration:  13% 23/180 [00:16<01:50,  1.41it/s]\u001b[A\n","Iteration:  13% 24/180 [00:17<01:52,  1.39it/s]\u001b[A\n","Iteration:  14% 25/180 [00:17<01:49,  1.41it/s]\u001b[A\n","Iteration:  14% 26/180 [00:18<01:50,  1.39it/s]\u001b[A\n","Iteration:  15% 27/180 [00:19<01:48,  1.41it/s]\u001b[A\n","Iteration:  16% 28/180 [00:19<01:49,  1.39it/s]\u001b[A\n","Iteration:  16% 29/180 [00:20<01:46,  1.41it/s]\u001b[A\n","Iteration:  17% 30/180 [00:21<01:48,  1.39it/s]\u001b[A\n","Iteration:  17% 31/180 [00:22<01:45,  1.41it/s]\u001b[A\n","Iteration:  18% 32/180 [00:22<01:47,  1.38it/s]\u001b[A\n","Iteration:  18% 33/180 [00:23<01:44,  1.41it/s]\u001b[A\n","Iteration:  19% 34/180 [00:24<01:45,  1.38it/s]\u001b[A\n","Iteration:  19% 35/180 [00:24<01:42,  1.41it/s]\u001b[A\n","Iteration:  20% 36/180 [00:25<01:44,  1.38it/s]\u001b[A\n","Iteration:  21% 37/180 [00:26<01:41,  1.41it/s]\u001b[A\n","Iteration:  21% 38/180 [00:27<01:42,  1.38it/s]\u001b[A\n","Iteration:  22% 39/180 [00:27<01:40,  1.41it/s]\u001b[A\n","Iteration:  22% 40/180 [00:28<01:41,  1.38it/s]\u001b[A\n","Iteration:  23% 41/180 [00:29<01:38,  1.41it/s]\u001b[A\n","Iteration:  23% 42/180 [00:30<01:39,  1.38it/s]\u001b[A\n","Iteration:  24% 43/180 [00:30<01:37,  1.41it/s]\u001b[A\n","Iteration:  24% 44/180 [00:31<01:38,  1.38it/s]\u001b[A\n","Iteration:  25% 45/180 [00:32<01:35,  1.41it/s]\u001b[A\n","Iteration:  26% 46/180 [00:32<01:36,  1.38it/s]\u001b[A\n","Iteration:  26% 47/180 [00:33<01:34,  1.41it/s]\u001b[A\n","Iteration:  27% 48/180 [00:34<01:35,  1.38it/s]\u001b[A\n","Iteration:  27% 49/180 [00:35<01:33,  1.40it/s]\u001b[A\n","Iteration:  28% 50/180 [00:35<01:34,  1.37it/s]\u001b[A\n","Iteration:  28% 51/180 [00:36<01:32,  1.40it/s]\u001b[A\n","Iteration:  29% 52/180 [00:37<01:33,  1.37it/s]\u001b[A\n","Iteration:  29% 53/180 [00:37<01:30,  1.40it/s]\u001b[A\n","Iteration:  30% 54/180 [00:38<01:31,  1.37it/s]\u001b[A\n","Iteration:  31% 55/180 [00:39<01:29,  1.40it/s]\u001b[A\n","Iteration:  31% 56/180 [00:40<01:30,  1.37it/s]\u001b[A\n","Iteration:  32% 57/180 [00:40<01:28,  1.40it/s]\u001b[A\n","Iteration:  32% 58/180 [00:41<01:28,  1.37it/s]\u001b[A\n","Iteration:  33% 59/180 [00:42<01:26,  1.39it/s]\u001b[A\n","Iteration:  33% 60/180 [00:43<01:27,  1.37it/s]\u001b[A\n","Iteration:  34% 61/180 [00:43<01:25,  1.40it/s]\u001b[A\n","Iteration:  34% 62/180 [00:44<01:25,  1.37it/s]\u001b[A\n","Iteration:  35% 63/180 [00:45<01:23,  1.40it/s]\u001b[A\n","Iteration:  36% 64/180 [00:45<01:24,  1.37it/s]\u001b[A\n","Iteration:  36% 65/180 [00:46<01:22,  1.39it/s]\u001b[A\n","Iteration:  37% 66/180 [00:47<01:23,  1.37it/s]\u001b[A\n","Iteration:  37% 67/180 [00:48<01:20,  1.40it/s]\u001b[A\n","Iteration:  38% 68/180 [00:48<01:21,  1.37it/s]\u001b[A\n","Iteration:  38% 69/180 [00:49<01:19,  1.40it/s]\u001b[A\n","Iteration:  39% 70/180 [00:50<01:20,  1.37it/s]\u001b[A\n","Iteration:  39% 71/180 [00:50<01:18,  1.40it/s]\u001b[A\n","Iteration:  40% 72/180 [00:51<01:18,  1.38it/s]\u001b[A\n","Iteration:  41% 73/180 [00:52<01:16,  1.40it/s]\u001b[A\n","Iteration:  41% 74/180 [00:53<01:17,  1.37it/s]\u001b[A\n","Iteration:  42% 75/180 [00:53<01:15,  1.39it/s]\u001b[A\n","Iteration:  42% 76/180 [00:54<01:15,  1.37it/s]\u001b[A\n","Iteration:  43% 77/180 [00:55<01:13,  1.40it/s]\u001b[A\n","Iteration:  43% 78/180 [00:56<01:14,  1.37it/s]\u001b[A\n","Iteration:  44% 79/180 [00:56<01:12,  1.40it/s]\u001b[A\n","Iteration:  44% 80/180 [00:57<01:12,  1.37it/s]\u001b[A\n","Iteration:  45% 81/180 [00:58<01:10,  1.40it/s]\u001b[A\n","Iteration:  46% 82/180 [00:58<01:11,  1.37it/s]\u001b[A\n","Iteration:  46% 83/180 [00:59<01:09,  1.39it/s]\u001b[A\n","Iteration:  47% 84/180 [01:00<01:10,  1.37it/s]\u001b[A\n","Iteration:  47% 85/180 [01:01<01:08,  1.39it/s]\u001b[A\n","Iteration:  48% 86/180 [01:01<01:08,  1.37it/s]\u001b[A\n","Iteration:  48% 87/180 [01:02<01:06,  1.40it/s]\u001b[A\n","Iteration:  49% 88/180 [01:03<01:07,  1.37it/s]\u001b[A\n","Iteration:  49% 89/180 [01:03<01:05,  1.39it/s]\u001b[A\n","Iteration:  50% 90/180 [01:04<01:05,  1.37it/s]\u001b[A\n","Iteration:  51% 91/180 [01:05<01:04,  1.39it/s]\u001b[A\n","Iteration:  51% 92/180 [01:06<01:04,  1.37it/s]\u001b[A\n","Iteration:  52% 93/180 [01:06<01:02,  1.40it/s]\u001b[A\n","Iteration:  52% 94/180 [01:07<01:02,  1.38it/s]\u001b[A\n","Iteration:  53% 95/180 [01:08<01:00,  1.40it/s]\u001b[A\n","Iteration:  53% 96/180 [01:09<01:01,  1.37it/s]\u001b[A\n","Iteration:  54% 97/180 [01:09<00:59,  1.39it/s]\u001b[A\n","Iteration:  54% 98/180 [01:10<00:59,  1.37it/s]\u001b[A\n","Iteration:  55% 99/180 [01:11<00:57,  1.40it/s]\u001b[A\n","Iteration:  56% 100/180 [01:11<00:58,  1.37it/s]\u001b[A\n","Iteration:  56% 101/180 [01:12<00:56,  1.39it/s]\u001b[A\n","Iteration:  57% 102/180 [01:13<00:56,  1.37it/s]\u001b[A\n","Iteration:  57% 103/180 [01:14<00:55,  1.40it/s]\u001b[A\n","Iteration:  58% 104/180 [01:14<00:55,  1.37it/s]\u001b[A\n","Iteration:  58% 105/180 [01:15<00:53,  1.40it/s]\u001b[A\n","Iteration:  59% 106/180 [01:16<00:54,  1.37it/s]\u001b[A\n","Iteration:  59% 107/180 [01:16<00:52,  1.39it/s]\u001b[A\n","Iteration:  60% 108/180 [01:17<00:52,  1.36it/s]\u001b[A\n","Iteration:  61% 109/180 [01:18<00:51,  1.39it/s]\u001b[A\n","Iteration:  61% 110/180 [01:19<00:51,  1.36it/s]\u001b[A\n","Iteration:  62% 111/180 [01:19<00:49,  1.39it/s]\u001b[A\n","Iteration:  62% 112/180 [01:20<00:49,  1.37it/s]\u001b[A\n","Iteration:  63% 113/180 [01:21<00:48,  1.39it/s]\u001b[A\n","Iteration:  63% 114/180 [01:22<00:48,  1.37it/s]\u001b[A\n","Iteration:  64% 115/180 [01:22<00:46,  1.40it/s]\u001b[A\n","Iteration:  64% 116/180 [01:23<00:46,  1.37it/s]\u001b[A\n","Iteration:  65% 117/180 [01:24<00:45,  1.39it/s]\u001b[A\n","Iteration:  66% 118/180 [01:24<00:45,  1.37it/s]\u001b[A\n","Iteration:  66% 119/180 [01:25<00:43,  1.40it/s]\u001b[A\n","Iteration:  67% 120/180 [01:26<00:43,  1.37it/s]\u001b[A\n","Iteration:  67% 121/180 [01:27<00:42,  1.40it/s]\u001b[A\n","Iteration:  68% 122/180 [01:27<00:42,  1.37it/s]\u001b[A\n","Iteration:  68% 123/180 [01:28<00:40,  1.39it/s]\u001b[A\n","Iteration:  69% 124/180 [01:29<00:41,  1.37it/s]\u001b[A\n","Iteration:  69% 125/180 [01:30<00:39,  1.40it/s]\u001b[A\n","Iteration:  70% 126/180 [01:30<00:39,  1.36it/s]\u001b[A\n","Iteration:  71% 127/180 [01:31<00:38,  1.39it/s]\u001b[A\n","Iteration:  71% 128/180 [01:32<00:38,  1.37it/s]\u001b[A\n","Iteration:  72% 129/180 [01:32<00:36,  1.39it/s]\u001b[A\n","Iteration:  72% 130/180 [01:33<00:36,  1.37it/s]\u001b[A\n","Iteration:  73% 131/180 [01:34<00:35,  1.39it/s]\u001b[A\n","Iteration:  73% 132/180 [01:35<00:35,  1.36it/s]\u001b[A\n","Iteration:  74% 133/180 [01:35<00:33,  1.39it/s]\u001b[A\n","Iteration:  74% 134/180 [01:36<00:33,  1.37it/s]\u001b[A\n","Iteration:  75% 135/180 [01:37<00:32,  1.39it/s]\u001b[A\n","Iteration:  76% 136/180 [01:38<00:32,  1.36it/s]\u001b[A\n","Iteration:  76% 137/180 [01:38<00:30,  1.39it/s]\u001b[A\n","Iteration:  77% 138/180 [01:39<00:30,  1.36it/s]\u001b[A\n","Iteration:  77% 139/180 [01:40<00:29,  1.39it/s]\u001b[A\n","Iteration:  78% 140/180 [01:40<00:29,  1.37it/s]\u001b[A\n","Iteration:  78% 141/180 [01:41<00:28,  1.38it/s]\u001b[A\n","Iteration:  79% 142/180 [01:42<00:27,  1.36it/s]\u001b[A\n","Iteration:  79% 143/180 [01:43<00:26,  1.39it/s]\u001b[A\n","Iteration:  80% 144/180 [01:43<00:26,  1.36it/s]\u001b[A\n","Iteration:  81% 145/180 [01:44<00:25,  1.39it/s]\u001b[A\n","Iteration:  81% 146/180 [01:45<00:24,  1.36it/s]\u001b[A\n","Iteration:  82% 147/180 [01:46<00:23,  1.38it/s]\u001b[A\n","Iteration:  82% 148/180 [01:46<00:23,  1.36it/s]\u001b[A\n","Iteration:  83% 149/180 [01:47<00:22,  1.38it/s]\u001b[A\n","Iteration:  83% 150/180 [01:48<00:22,  1.36it/s]\u001b[A\n","Iteration:  84% 151/180 [01:48<00:20,  1.39it/s]\u001b[A\n","Iteration:  84% 152/180 [01:49<00:20,  1.36it/s]\u001b[A\n","Iteration:  85% 153/180 [01:50<00:19,  1.38it/s]\u001b[A\n","Iteration:  86% 154/180 [01:51<00:19,  1.36it/s]\u001b[A\n","Iteration:  86% 155/180 [01:51<00:18,  1.39it/s]\u001b[A\n","Iteration:  87% 156/180 [01:52<00:17,  1.36it/s]\u001b[A\n","Iteration:  87% 157/180 [01:53<00:16,  1.38it/s]\u001b[A\n","Iteration:  88% 158/180 [01:54<00:16,  1.36it/s]\u001b[A\n","Iteration:  88% 159/180 [01:54<00:15,  1.39it/s]\u001b[A\n","Iteration:  89% 160/180 [01:55<00:14,  1.36it/s]\u001b[A\n","Iteration:  89% 161/180 [01:56<00:13,  1.38it/s]\u001b[A\n","Iteration:  90% 162/180 [01:56<00:13,  1.36it/s]\u001b[A\n","Iteration:  91% 163/180 [01:57<00:12,  1.38it/s]\u001b[A\n","Iteration:  91% 164/180 [01:58<00:11,  1.36it/s]\u001b[A\n","Iteration:  92% 165/180 [01:59<00:10,  1.38it/s]\u001b[A\n","Iteration:  92% 166/180 [01:59<00:10,  1.36it/s]\u001b[A\n","Iteration:  93% 167/180 [02:00<00:09,  1.38it/s]\u001b[A\n","Iteration:  93% 168/180 [02:01<00:08,  1.36it/s]\u001b[A\n","Iteration:  94% 169/180 [02:02<00:07,  1.38it/s]\u001b[A\n","Iteration:  94% 170/180 [02:02<00:07,  1.36it/s]\u001b[A\n","Iteration:  95% 171/180 [02:03<00:06,  1.38it/s]\u001b[A\n","Iteration:  96% 172/180 [02:04<00:05,  1.36it/s]\u001b[A\n","Iteration:  96% 173/180 [02:04<00:05,  1.38it/s]\u001b[A\n","Iteration:  97% 174/180 [02:05<00:04,  1.36it/s]\u001b[A\n","Iteration:  97% 175/180 [02:06<00:03,  1.39it/s]\u001b[A\n","Iteration:  98% 176/180 [02:07<00:02,  1.36it/s]\u001b[A\n","Iteration:  98% 177/180 [02:07<00:02,  1.39it/s]\u001b[A\n","Iteration:  99% 178/180 [02:08<00:01,  1.36it/s]\u001b[A\n","Iteration:  99% 179/180 [02:09<00:00,  1.38it/s]\u001b[A\n","Iteration: 100% 180/180 [02:09<00:00,  1.39it/s]\n","Epoch:  33% 1/3 [02:09<04:19, 129.89s/it]\n","Iteration:   0% 0/180 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/180 [00:00<02:04,  1.44it/s]\u001b[A\n","Iteration:   1% 2/180 [00:01<02:06,  1.40it/s]\u001b[A\n","Iteration:   2% 3/180 [00:02<02:04,  1.42it/s]\u001b[A\n","Iteration:   2% 4/180 [00:02<02:07,  1.38it/s]\u001b[A\n","Iteration:   3% 5/180 [00:03<02:05,  1.39it/s]\u001b[A\n","Iteration:   3% 6/180 [00:04<02:08,  1.36it/s]\u001b[A\n","Iteration:   4% 7/180 [00:05<02:05,  1.38it/s]\u001b[A\n","Iteration:   4% 8/180 [00:05<02:06,  1.35it/s]\u001b[A\n","Iteration:   5% 9/180 [00:06<02:03,  1.38it/s]\u001b[A\n","Iteration:   6% 10/180 [00:07<02:05,  1.36it/s]\u001b[A\n","Iteration:   6% 11/180 [00:08<02:02,  1.38it/s]\u001b[A\n","Iteration:   7% 12/180 [00:08<02:04,  1.35it/s]\u001b[A\n","Iteration:   7% 13/180 [00:09<02:01,  1.37it/s]\u001b[A\n","Iteration:   8% 14/180 [00:10<02:03,  1.35it/s]\u001b[A\n","Iteration:   8% 15/180 [00:10<02:00,  1.37it/s]\u001b[A\n","Iteration:   9% 16/180 [00:11<02:01,  1.35it/s]\u001b[A\n","Iteration:   9% 17/180 [00:12<01:58,  1.38it/s]\u001b[A\n","Iteration:  10% 18/180 [00:13<01:59,  1.35it/s]\u001b[A\n","Iteration:  11% 19/180 [00:13<01:57,  1.37it/s]\u001b[A\n","Iteration:  11% 20/180 [00:14<01:58,  1.35it/s]\u001b[A\n","Iteration:  12% 21/180 [00:15<01:56,  1.37it/s]\u001b[A\n","Iteration:  12% 22/180 [00:16<01:57,  1.34it/s]\u001b[A\n","Iteration:  13% 23/180 [00:16<01:54,  1.37it/s]\u001b[A\n","Iteration:  13% 24/180 [00:17<01:56,  1.34it/s]\u001b[A\n","Iteration:  14% 25/180 [00:18<01:53,  1.37it/s]\u001b[A\n","Iteration:  14% 26/180 [00:19<01:54,  1.35it/s]\u001b[A\n","Iteration:  15% 27/180 [00:19<01:51,  1.37it/s]\u001b[A\n","Iteration:  16% 28/180 [00:20<01:52,  1.35it/s]\u001b[A\n","Iteration:  16% 29/180 [00:21<01:50,  1.37it/s]\u001b[A\n","Iteration:  17% 30/180 [00:22<01:51,  1.34it/s]\u001b[A\n","Iteration:  17% 31/180 [00:22<01:49,  1.37it/s]\u001b[A\n","Iteration:  18% 32/180 [00:23<01:50,  1.34it/s]\u001b[A\n","Iteration:  18% 33/180 [00:24<01:47,  1.37it/s]\u001b[A\n","Iteration:  19% 34/180 [00:25<01:49,  1.34it/s]\u001b[A\n","Iteration:  19% 35/180 [00:25<01:46,  1.36it/s]\u001b[A\n","Iteration:  20% 36/180 [00:26<01:47,  1.34it/s]\u001b[A\n","Iteration:  21% 37/180 [00:27<01:44,  1.36it/s]\u001b[A\n","Iteration:  21% 38/180 [00:27<01:45,  1.34it/s]\u001b[A\n","Iteration:  22% 39/180 [00:28<01:43,  1.36it/s]\u001b[A\n","Iteration:  22% 40/180 [00:29<01:44,  1.34it/s]\u001b[A\n","Iteration:  23% 41/180 [00:30<01:42,  1.36it/s]\u001b[A\n","Iteration:  23% 42/180 [00:30<01:43,  1.33it/s]\u001b[A\n","Iteration:  24% 43/180 [00:31<01:40,  1.36it/s]\u001b[A\n","Iteration:  24% 44/180 [00:32<01:42,  1.33it/s]\u001b[A\n","Iteration:  25% 45/180 [00:33<01:39,  1.36it/s]\u001b[A\n","Iteration:  26% 46/180 [00:33<01:40,  1.33it/s]\u001b[A\n","Iteration:  26% 47/180 [00:34<01:38,  1.36it/s]\u001b[A\n","Iteration:  27% 48/180 [00:35<01:39,  1.33it/s]\u001b[A\n","Iteration:  27% 49/180 [00:36<01:36,  1.36it/s]\u001b[A\n","Iteration:  28% 50/180 [00:36<01:37,  1.33it/s]\u001b[A\n","Iteration:  28% 51/180 [00:37<01:35,  1.36it/s]\u001b[A\n","Iteration:  29% 52/180 [00:38<01:36,  1.33it/s]\u001b[A\n","Iteration:  29% 53/180 [00:39<01:33,  1.35it/s]\u001b[A\n","Iteration:  30% 54/180 [00:39<01:34,  1.33it/s]\u001b[A\n","Iteration:  31% 55/180 [00:40<01:32,  1.35it/s]\u001b[A\n","Iteration:  31% 56/180 [00:41<01:33,  1.33it/s]\u001b[A\n","Iteration:  32% 57/180 [00:42<01:31,  1.35it/s]\u001b[A\n","Iteration:  32% 58/180 [00:42<01:32,  1.32it/s]\u001b[A\n","Iteration:  33% 59/180 [00:43<01:29,  1.35it/s]\u001b[A\n","Iteration:  33% 60/180 [00:44<01:30,  1.32it/s]\u001b[A\n","Iteration:  34% 61/180 [00:45<01:27,  1.35it/s]\u001b[A\n","Iteration:  34% 62/180 [00:45<01:28,  1.33it/s]\u001b[A\n","Iteration:  35% 63/180 [00:46<01:26,  1.36it/s]\u001b[A\n","Iteration:  36% 64/180 [00:47<01:26,  1.34it/s]\u001b[A\n","Iteration:  36% 65/180 [00:48<01:24,  1.36it/s]\u001b[A\n","Iteration:  37% 66/180 [00:48<01:25,  1.34it/s]\u001b[A\n","Iteration:  37% 67/180 [00:49<01:23,  1.36it/s]\u001b[A\n","Iteration:  38% 68/180 [00:50<01:24,  1.33it/s]\u001b[A\n","Iteration:  38% 69/180 [00:51<01:22,  1.35it/s]\u001b[A\n","Iteration:  39% 70/180 [00:51<01:22,  1.33it/s]\u001b[A\n","Iteration:  39% 71/180 [00:52<01:20,  1.35it/s]\u001b[A\n","Iteration:  40% 72/180 [00:53<01:21,  1.33it/s]\u001b[A\n","Iteration:  41% 73/180 [00:54<01:19,  1.35it/s]\u001b[A\n","Iteration:  41% 74/180 [00:54<01:19,  1.33it/s]\u001b[A\n","Iteration:  42% 75/180 [00:55<01:17,  1.36it/s]\u001b[A\n","Iteration:  42% 76/180 [00:56<01:17,  1.34it/s]\u001b[A\n","Iteration:  43% 77/180 [00:56<01:15,  1.36it/s]\u001b[A\n","Iteration:  43% 78/180 [00:57<01:15,  1.34it/s]\u001b[A\n","Iteration:  44% 79/180 [00:58<01:13,  1.37it/s]\u001b[A\n","Iteration:  44% 80/180 [00:59<01:14,  1.34it/s]\u001b[A\n","Iteration:  45% 81/180 [00:59<01:12,  1.36it/s]\u001b[A\n","Iteration:  46% 82/180 [01:00<01:13,  1.34it/s]\u001b[A\n","Iteration:  46% 83/180 [01:01<01:11,  1.36it/s]\u001b[A\n","Iteration:  47% 84/180 [01:02<01:11,  1.34it/s]\u001b[A\n","Iteration:  47% 85/180 [01:02<01:09,  1.36it/s]\u001b[A\n","Iteration:  48% 86/180 [01:03<01:10,  1.34it/s]\u001b[A\n","Iteration:  48% 87/180 [01:04<01:08,  1.37it/s]\u001b[A\n","Iteration:  49% 88/180 [01:05<01:08,  1.34it/s]\u001b[A\n","Iteration:  49% 89/180 [01:05<01:06,  1.37it/s]\u001b[A\n","Iteration:  50% 90/180 [01:06<01:06,  1.35it/s]\u001b[A\n","Iteration:  51% 91/180 [01:07<01:05,  1.36it/s]\u001b[A\n","Iteration:  51% 92/180 [01:08<01:05,  1.34it/s]\u001b[A\n","Iteration:  52% 93/180 [01:08<01:03,  1.36it/s]\u001b[A\n","Iteration:  52% 94/180 [01:09<01:04,  1.34it/s]\u001b[A\n","Iteration:  53% 95/180 [01:10<01:02,  1.36it/s]\u001b[A\n","Iteration:  53% 96/180 [01:11<01:02,  1.34it/s]\u001b[A\n","Iteration:  54% 97/180 [01:11<01:00,  1.36it/s]\u001b[A\n","Iteration:  54% 98/180 [01:12<01:01,  1.34it/s]\u001b[A\n","Iteration:  55% 99/180 [01:13<00:59,  1.37it/s]\u001b[A\n","Iteration:  56% 100/180 [01:14<00:59,  1.35it/s]\u001b[A\n","Iteration:  56% 101/180 [01:14<00:57,  1.37it/s]\u001b[A\n","Iteration:  57% 102/180 [01:15<00:58,  1.34it/s]\u001b[A\n","Iteration:  57% 103/180 [01:16<00:56,  1.36it/s]\u001b[A\n","Iteration:  58% 104/180 [01:16<00:56,  1.34it/s]\u001b[A\n","Iteration:  58% 105/180 [01:17<00:54,  1.37it/s]\u001b[A\n","Iteration:  59% 106/180 [01:18<00:55,  1.34it/s]\u001b[A\n","Iteration:  59% 107/180 [01:19<00:53,  1.37it/s]\u001b[A\n","Iteration:  60% 108/180 [01:19<00:53,  1.35it/s]\u001b[A\n","Iteration:  61% 109/180 [01:20<00:51,  1.37it/s]\u001b[A\n","Iteration:  61% 110/180 [01:21<00:52,  1.35it/s]\u001b[A\n","Iteration:  62% 111/180 [01:22<00:50,  1.36it/s]\u001b[A\n","Iteration:  62% 112/180 [01:22<00:50,  1.34it/s]\u001b[A\n","Iteration:  63% 113/180 [01:23<00:49,  1.36it/s]\u001b[A\n","Iteration:  63% 114/180 [01:24<00:49,  1.33it/s]\u001b[A\n","Iteration:  64% 115/180 [01:25<00:47,  1.36it/s]\u001b[A\n","Iteration:  64% 116/180 [01:25<00:47,  1.33it/s]\u001b[A\n","Iteration:  65% 117/180 [01:26<00:46,  1.36it/s]\u001b[A\n","Iteration:  66% 118/180 [01:27<00:46,  1.34it/s]\u001b[A\n","Iteration:  66% 119/180 [01:28<00:44,  1.37it/s]\u001b[A\n","Iteration:  67% 120/180 [01:28<00:44,  1.34it/s]\u001b[A\n","Iteration:  67% 121/180 [01:29<00:43,  1.36it/s]\u001b[A\n","Iteration:  68% 122/180 [01:30<00:43,  1.34it/s]\u001b[A\n","Iteration:  68% 123/180 [01:30<00:41,  1.36it/s]\u001b[A\n","Iteration:  69% 124/180 [01:31<00:41,  1.34it/s]\u001b[A\n","Iteration:  69% 125/180 [01:32<00:40,  1.36it/s]\u001b[A\n","Iteration:  70% 126/180 [01:33<00:40,  1.34it/s]\u001b[A\n","Iteration:  71% 127/180 [01:33<00:38,  1.36it/s]\u001b[A\n","Iteration:  71% 128/180 [01:34<00:38,  1.34it/s]\u001b[A\n","Iteration:  72% 129/180 [01:35<00:37,  1.37it/s]\u001b[A\n","Iteration:  72% 130/180 [01:36<00:37,  1.34it/s]\u001b[A\n","Iteration:  73% 131/180 [01:36<00:35,  1.36it/s]\u001b[A\n","Iteration:  73% 132/180 [01:37<00:35,  1.34it/s]\u001b[A\n","Iteration:  74% 133/180 [01:38<00:34,  1.36it/s]\u001b[A\n","Iteration:  74% 134/180 [01:39<00:34,  1.33it/s]\u001b[A\n","Iteration:  75% 135/180 [01:39<00:32,  1.36it/s]\u001b[A\n","Iteration:  76% 136/180 [01:40<00:32,  1.33it/s]\u001b[A\n","Iteration:  76% 137/180 [01:41<00:31,  1.37it/s]\u001b[A\n","Iteration:  77% 138/180 [01:42<00:31,  1.34it/s]\u001b[A\n","Iteration:  77% 139/180 [01:42<00:30,  1.36it/s]\u001b[A\n","Iteration:  78% 140/180 [01:43<00:29,  1.34it/s]\u001b[A\n","Iteration:  78% 141/180 [01:44<00:28,  1.36it/s]\u001b[A\n","Iteration:  79% 142/180 [01:45<00:28,  1.34it/s]\u001b[A\n","Iteration:  79% 143/180 [01:45<00:27,  1.36it/s]\u001b[A\n","Iteration:  80% 144/180 [01:46<00:26,  1.34it/s]\u001b[A\n","Iteration:  81% 145/180 [01:47<00:25,  1.36it/s]\u001b[A\n","Iteration:  81% 146/180 [01:48<00:25,  1.34it/s]\u001b[A\n","Iteration:  82% 147/180 [01:48<00:24,  1.37it/s]\u001b[A\n","Iteration:  82% 148/180 [01:49<00:23,  1.34it/s]\u001b[A\n","Iteration:  83% 149/180 [01:50<00:22,  1.37it/s]\u001b[A\n","Iteration:  83% 150/180 [01:51<00:22,  1.34it/s]\u001b[A\n","Iteration:  84% 151/180 [01:51<00:21,  1.37it/s]\u001b[A\n","Iteration:  84% 152/180 [01:52<00:20,  1.34it/s]\u001b[A\n","Iteration:  85% 153/180 [01:53<00:19,  1.36it/s]\u001b[A\n","Iteration:  86% 154/180 [01:53<00:19,  1.34it/s]\u001b[A\n","Iteration:  86% 155/180 [01:54<00:18,  1.36it/s]\u001b[A\n","Iteration:  87% 156/180 [01:55<00:17,  1.34it/s]\u001b[A\n","Iteration:  87% 157/180 [01:56<00:16,  1.36it/s]\u001b[A\n","Iteration:  88% 158/180 [01:56<00:16,  1.33it/s]\u001b[A\n","Iteration:  88% 159/180 [01:57<00:15,  1.36it/s]\u001b[A\n","Iteration:  89% 160/180 [01:58<00:14,  1.34it/s]\u001b[A\n","Iteration:  89% 161/180 [01:59<00:13,  1.36it/s]\u001b[A\n","Iteration:  90% 162/180 [01:59<00:13,  1.34it/s]\u001b[A\n","Iteration:  91% 163/180 [02:00<00:12,  1.37it/s]\u001b[A\n","Iteration:  91% 164/180 [02:01<00:11,  1.34it/s]\u001b[A\n","Iteration:  92% 165/180 [02:02<00:11,  1.36it/s]\u001b[A\n","Iteration:  92% 166/180 [02:02<00:10,  1.34it/s]\u001b[A\n","Iteration:  93% 167/180 [02:03<00:09,  1.36it/s]\u001b[A\n","Iteration:  93% 168/180 [02:04<00:08,  1.34it/s]\u001b[A\n","Iteration:  94% 169/180 [02:05<00:08,  1.36it/s]\u001b[A\n","Iteration:  94% 170/180 [02:05<00:07,  1.34it/s]\u001b[A\n","Iteration:  95% 171/180 [02:06<00:06,  1.37it/s]\u001b[A\n","Iteration:  96% 172/180 [02:07<00:05,  1.34it/s]\u001b[A\n","Iteration:  96% 173/180 [02:07<00:05,  1.36it/s]\u001b[A\n","Iteration:  97% 174/180 [02:08<00:04,  1.34it/s]\u001b[A\n","Iteration:  97% 175/180 [02:09<00:03,  1.36it/s]\u001b[A\n","Iteration:  98% 176/180 [02:10<00:02,  1.34it/s]\u001b[A\n","Iteration:  98% 177/180 [02:10<00:02,  1.37it/s]\u001b[A\n","Iteration:  99% 178/180 [02:11<00:01,  1.34it/s]\u001b[A\n","Iteration:  99% 179/180 [02:12<00:00,  1.37it/s]\u001b[A\n","Iteration: 100% 180/180 [02:12<00:00,  1.35it/s]\n","Epoch:  67% 2/3 [04:22<02:10, 130.82s/it]\n","Iteration:   0% 0/180 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1% 1/180 [00:00<02:06,  1.41it/s]\u001b[A\n","Iteration:   1% 2/180 [00:01<02:09,  1.37it/s]\u001b[A\n","Iteration:   2% 3/180 [00:02<02:08,  1.38it/s]\u001b[A\n","Iteration:   2% 4/180 [00:02<02:10,  1.35it/s]\u001b[A\n","Iteration:   3% 5/180 [00:03<02:07,  1.37it/s]\u001b[A\n","Iteration:   3% 6/180 [00:04<02:09,  1.34it/s]\u001b[A\n","Iteration:   4% 7/180 [00:05<02:06,  1.37it/s]\u001b[A\n","Iteration:   4% 8/180 [00:05<02:08,  1.34it/s]\u001b[A\n","Iteration:   5% 9/180 [00:06<02:05,  1.37it/s]\u001b[A\n","Iteration:   6% 10/180 [00:07<02:06,  1.35it/s]\u001b[A\n","Iteration:   6% 11/180 [00:08<02:03,  1.37it/s]\u001b[A\n","Iteration:   7% 12/180 [00:08<02:04,  1.35it/s]\u001b[A\n","Iteration:   7% 13/180 [00:09<02:02,  1.37it/s]\u001b[A\n","Iteration:   8% 14/180 [00:10<02:04,  1.34it/s]\u001b[A\n","Iteration:   8% 15/180 [00:11<02:01,  1.36it/s]\u001b[A\n","Iteration:   9% 16/180 [00:11<02:03,  1.33it/s]\u001b[A\n","Iteration:   9% 17/180 [00:12<01:59,  1.36it/s]\u001b[A\n","Iteration:  10% 18/180 [00:13<02:01,  1.33it/s]\u001b[A\n","Iteration:  11% 19/180 [00:14<01:58,  1.36it/s]\u001b[A\n","Iteration:  11% 20/180 [00:14<01:59,  1.34it/s]\u001b[A\n","Iteration:  12% 21/180 [00:15<01:56,  1.36it/s]\u001b[A\n","Iteration:  12% 22/180 [00:16<01:57,  1.34it/s]\u001b[A\n","Iteration:  13% 23/180 [00:17<01:55,  1.36it/s]\u001b[A\n","Iteration:  13% 24/180 [00:17<01:56,  1.34it/s]\u001b[A\n","Iteration:  14% 25/180 [00:18<01:54,  1.36it/s]\u001b[A\n","Iteration:  14% 26/180 [00:19<01:55,  1.34it/s]\u001b[A\n","Iteration:  15% 27/180 [00:19<01:52,  1.36it/s]\u001b[A\n","Iteration:  16% 28/180 [00:20<01:54,  1.33it/s]\u001b[A\n","Iteration:  16% 29/180 [00:21<01:51,  1.36it/s]\u001b[A\n","Iteration:  17% 30/180 [00:22<01:52,  1.33it/s]\u001b[A\n","Iteration:  17% 31/180 [00:22<01:49,  1.36it/s]\u001b[A\n","Iteration:  18% 32/180 [00:23<01:50,  1.34it/s]\u001b[A\n","Iteration:  18% 33/180 [00:24<01:47,  1.37it/s]\u001b[A\n","Iteration:  19% 34/180 [00:25<01:48,  1.35it/s]\u001b[A\n","Iteration:  19% 35/180 [00:25<01:46,  1.37it/s]\u001b[A\n","Iteration:  20% 36/180 [00:26<01:47,  1.34it/s]\u001b[A\n","Iteration:  21% 37/180 [00:27<01:44,  1.36it/s]\u001b[A\n","Iteration:  21% 38/180 [00:28<01:46,  1.33it/s]\u001b[A\n","Iteration:  22% 39/180 [00:28<01:43,  1.36it/s]\u001b[A\n","Iteration:  22% 40/180 [00:29<01:45,  1.33it/s]\u001b[A\n","Iteration:  23% 41/180 [00:30<01:42,  1.36it/s]\u001b[A\n","Iteration:  23% 42/180 [00:31<01:43,  1.34it/s]\u001b[A\n","Iteration:  24% 43/180 [00:31<01:40,  1.36it/s]\u001b[A\n","Iteration:  24% 44/180 [00:32<01:41,  1.34it/s]\u001b[A\n","Iteration:  25% 45/180 [00:33<01:39,  1.36it/s]\u001b[A\n","Iteration:  26% 46/180 [00:34<01:39,  1.34it/s]\u001b[A\n","Iteration:  26% 47/180 [00:34<01:37,  1.36it/s]\u001b[A\n","Iteration:  27% 48/180 [00:35<01:38,  1.33it/s]\u001b[A\n","Iteration:  27% 49/180 [00:36<01:36,  1.35it/s]\u001b[A\n","Iteration:  28% 50/180 [00:37<01:37,  1.33it/s]\u001b[A\n","Iteration:  28% 51/180 [00:37<01:34,  1.36it/s]\u001b[A\n","Iteration:  29% 52/180 [00:38<01:36,  1.33it/s]\u001b[A\n","Iteration:  29% 53/180 [00:39<01:33,  1.36it/s]\u001b[A\n","Iteration:  30% 54/180 [00:40<01:34,  1.34it/s]\u001b[A\n","Iteration:  31% 55/180 [00:40<01:31,  1.37it/s]\u001b[A\n","Iteration:  31% 56/180 [00:41<01:32,  1.35it/s]\u001b[A\n","Iteration:  32% 57/180 [00:42<01:30,  1.36it/s]\u001b[A\n","Iteration:  32% 58/180 [00:42<01:31,  1.34it/s]\u001b[A\n","Iteration:  33% 59/180 [00:43<01:28,  1.36it/s]\u001b[A\n","Iteration:  33% 60/180 [00:44<01:29,  1.34it/s]\u001b[A\n","Iteration:  34% 61/180 [00:45<01:27,  1.35it/s]\u001b[A\n","Iteration:  34% 62/180 [00:45<01:28,  1.33it/s]\u001b[A\n","Iteration:  35% 63/180 [00:46<01:26,  1.36it/s]\u001b[A\n","Iteration:  36% 64/180 [00:47<01:26,  1.33it/s]\u001b[A\n","Iteration:  36% 65/180 [00:48<01:24,  1.36it/s]\u001b[A\n","Iteration:  37% 66/180 [00:48<01:25,  1.34it/s]\u001b[A\n","Iteration:  37% 67/180 [00:49<01:22,  1.36it/s]\u001b[A\n","Iteration:  38% 68/180 [00:50<01:23,  1.34it/s]\u001b[A\n","Iteration:  38% 69/180 [00:51<01:21,  1.36it/s]\u001b[A\n","Iteration:  39% 70/180 [00:51<01:22,  1.34it/s]\u001b[A\n","Iteration:  39% 71/180 [00:52<01:20,  1.36it/s]\u001b[A\n","Iteration:  40% 72/180 [00:53<01:20,  1.34it/s]\u001b[A\n","Iteration:  41% 73/180 [00:54<01:18,  1.36it/s]\u001b[A\n","Iteration:  41% 74/180 [00:54<01:19,  1.33it/s]\u001b[A\n","Iteration:  42% 75/180 [00:55<01:17,  1.36it/s]\u001b[A\n","Iteration:  42% 76/180 [00:56<01:17,  1.34it/s]\u001b[A\n","Iteration:  43% 77/180 [00:57<01:15,  1.37it/s]\u001b[A\n","Iteration:  43% 78/180 [00:57<01:15,  1.34it/s]\u001b[A\n","Iteration:  44% 79/180 [00:58<01:14,  1.36it/s]\u001b[A\n","Iteration:  44% 80/180 [00:59<01:14,  1.34it/s]\u001b[A\n","Iteration:  45% 81/180 [00:59<01:12,  1.36it/s]\u001b[A\n","Iteration:  46% 82/180 [01:00<01:13,  1.34it/s]\u001b[A\n","Iteration:  46% 83/180 [01:01<01:11,  1.36it/s]\u001b[A\n","Iteration:  47% 84/180 [01:02<01:11,  1.33it/s]\u001b[A\n","Iteration:  47% 85/180 [01:02<01:09,  1.36it/s]\u001b[A\n","Iteration:  48% 86/180 [01:03<01:10,  1.34it/s]\u001b[A\n","Iteration:  48% 87/180 [01:04<01:08,  1.36it/s]\u001b[A\n","Iteration:  49% 88/180 [01:05<01:08,  1.34it/s]\u001b[A\n","Iteration:  49% 89/180 [01:05<01:06,  1.36it/s]\u001b[A\n","Iteration:  50% 90/180 [01:06<01:07,  1.34it/s]\u001b[A\n","Iteration:  51% 91/180 [01:07<01:05,  1.36it/s]\u001b[A\n","Iteration:  51% 92/180 [01:08<01:05,  1.34it/s]\u001b[A\n","Iteration:  52% 93/180 [01:08<01:03,  1.36it/s]\u001b[A\n","Iteration:  52% 94/180 [01:09<01:04,  1.34it/s]\u001b[A\n","Iteration:  53% 95/180 [01:10<01:02,  1.36it/s]\u001b[A\n","Iteration:  53% 96/180 [01:11<01:02,  1.34it/s]\u001b[A\n","Iteration:  54% 97/180 [01:11<01:00,  1.37it/s]\u001b[A\n","Iteration:  54% 98/180 [01:12<01:01,  1.34it/s]\u001b[A\n","Iteration:  55% 99/180 [01:13<00:59,  1.37it/s]\u001b[A\n","Iteration:  56% 100/180 [01:14<00:59,  1.34it/s]\u001b[A\n","Iteration:  56% 101/180 [01:14<00:57,  1.36it/s]\u001b[A\n","Iteration:  57% 102/180 [01:15<00:58,  1.34it/s]\u001b[A\n","Iteration:  57% 103/180 [01:16<00:56,  1.36it/s]\u001b[A\n","Iteration:  58% 104/180 [01:17<00:56,  1.34it/s]\u001b[A\n","Iteration:  58% 105/180 [01:17<00:55,  1.36it/s]\u001b[A\n","Iteration:  59% 106/180 [01:18<00:55,  1.33it/s]\u001b[A\n","Iteration:  59% 107/180 [01:19<00:53,  1.36it/s]\u001b[A\n","Iteration:  60% 108/180 [01:20<00:53,  1.33it/s]\u001b[A\n","Iteration:  61% 109/180 [01:20<00:52,  1.36it/s]\u001b[A\n","Iteration:  61% 110/180 [01:21<00:52,  1.34it/s]\u001b[A\n","Iteration:  62% 111/180 [01:22<00:50,  1.36it/s]\u001b[A\n","Iteration:  62% 112/180 [01:22<00:50,  1.34it/s]\u001b[A\n","Iteration:  63% 113/180 [01:23<00:49,  1.36it/s]\u001b[A\n","Iteration:  63% 114/180 [01:24<00:49,  1.34it/s]\u001b[A\n","Iteration:  64% 115/180 [01:25<00:47,  1.36it/s]\u001b[A\n","Iteration:  64% 116/180 [01:25<00:47,  1.34it/s]\u001b[A\n","Iteration:  65% 117/180 [01:26<00:46,  1.36it/s]\u001b[A\n","Iteration:  66% 118/180 [01:27<00:46,  1.33it/s]\u001b[A\n","Iteration:  66% 119/180 [01:28<00:44,  1.36it/s]\u001b[A\n","Iteration:  67% 120/180 [01:28<00:44,  1.34it/s]\u001b[A\n","Iteration:  67% 121/180 [01:29<00:43,  1.36it/s]\u001b[A\n","Iteration:  68% 122/180 [01:30<00:43,  1.34it/s]\u001b[A\n","Iteration:  68% 123/180 [01:31<00:41,  1.36it/s]\u001b[A\n","Iteration:  69% 124/180 [01:31<00:41,  1.34it/s]\u001b[A\n","Iteration:  69% 125/180 [01:32<00:40,  1.36it/s]\u001b[A\n","Iteration:  70% 126/180 [01:33<00:40,  1.34it/s]\u001b[A\n","Iteration:  71% 127/180 [01:34<00:38,  1.36it/s]\u001b[A\n","Iteration:  71% 128/180 [01:34<00:38,  1.34it/s]\u001b[A\n","Iteration:  72% 129/180 [01:35<00:37,  1.36it/s]\u001b[A\n","Iteration:  72% 130/180 [01:36<00:37,  1.33it/s]\u001b[A\n","Iteration:  73% 131/180 [01:37<00:35,  1.36it/s]\u001b[A\n","Iteration:  73% 132/180 [01:37<00:35,  1.34it/s]\u001b[A\n","Iteration:  74% 133/180 [01:38<00:34,  1.36it/s]\u001b[A\n","Iteration:  74% 134/180 [01:39<00:34,  1.34it/s]\u001b[A\n","Iteration:  75% 135/180 [01:39<00:32,  1.37it/s]\u001b[A\n","Iteration:  76% 136/180 [01:40<00:32,  1.34it/s]\u001b[A\n","Iteration:  76% 137/180 [01:41<00:31,  1.36it/s]\u001b[A\n","Iteration:  77% 138/180 [01:42<00:31,  1.34it/s]\u001b[A\n","Iteration:  77% 139/180 [01:42<00:30,  1.36it/s]\u001b[A\n","Iteration:  78% 140/180 [01:43<00:29,  1.34it/s]\u001b[A\n","Iteration:  78% 141/180 [01:44<00:28,  1.36it/s]\u001b[A\n","Iteration:  79% 142/180 [01:45<00:28,  1.33it/s]\u001b[A\n","Iteration:  79% 143/180 [01:45<00:27,  1.36it/s]\u001b[A\n","Iteration:  80% 144/180 [01:46<00:26,  1.34it/s]\u001b[A\n","Iteration:  81% 145/180 [01:47<00:25,  1.36it/s]\u001b[A\n","Iteration:  81% 146/180 [01:48<00:25,  1.34it/s]\u001b[A\n","Iteration:  82% 147/180 [01:48<00:24,  1.36it/s]\u001b[A\n","Iteration:  82% 148/180 [01:49<00:23,  1.34it/s]\u001b[A\n","Iteration:  83% 149/180 [01:50<00:22,  1.36it/s]\u001b[A\n","Iteration:  83% 150/180 [01:51<00:22,  1.33it/s]\u001b[A\n","Iteration:  84% 151/180 [01:51<00:21,  1.36it/s]\u001b[A\n","Iteration:  84% 152/180 [01:52<00:20,  1.33it/s]\u001b[A\n","Iteration:  85% 153/180 [01:53<00:19,  1.36it/s]\u001b[A\n","Iteration:  86% 154/180 [01:54<00:19,  1.34it/s]\u001b[A\n","Iteration:  86% 155/180 [01:54<00:18,  1.36it/s]\u001b[A\n","Iteration:  87% 156/180 [01:55<00:17,  1.34it/s]\u001b[A\n","Iteration:  87% 157/180 [01:56<00:16,  1.37it/s]\u001b[A\n","Iteration:  88% 158/180 [01:57<00:16,  1.34it/s]\u001b[A\n","Iteration:  88% 159/180 [01:57<00:15,  1.37it/s]\u001b[A\n","Iteration:  89% 160/180 [01:58<00:14,  1.34it/s]\u001b[A\n","Iteration:  89% 161/180 [01:59<00:13,  1.36it/s]\u001b[A\n","Iteration:  90% 162/180 [02:00<00:13,  1.33it/s]\u001b[A\n","Iteration:  91% 163/180 [02:00<00:12,  1.36it/s]\u001b[A\n","Iteration:  91% 164/180 [02:01<00:11,  1.33it/s]\u001b[A\n","Iteration:  92% 165/180 [02:02<00:10,  1.36it/s]\u001b[A\n","Iteration:  92% 166/180 [02:02<00:10,  1.34it/s]\u001b[A\n","Iteration:  93% 167/180 [02:03<00:09,  1.37it/s]\u001b[A\n","Iteration:  93% 168/180 [02:04<00:08,  1.34it/s]\u001b[A\n","Iteration:  94% 169/180 [02:05<00:08,  1.37it/s]\u001b[A\n","Iteration:  94% 170/180 [02:05<00:07,  1.34it/s]\u001b[A\n","Iteration:  95% 171/180 [02:06<00:06,  1.36it/s]\u001b[A\n","Iteration:  96% 172/180 [02:07<00:05,  1.34it/s]\u001b[A\n","Iteration:  96% 173/180 [02:08<00:05,  1.36it/s]\u001b[A\n","Iteration:  97% 174/180 [02:08<00:04,  1.33it/s]\u001b[A\n","Iteration:  97% 175/180 [02:09<00:03,  1.36it/s]\u001b[A\n","Iteration:  98% 176/180 [02:10<00:02,  1.34it/s]\u001b[A\n","Iteration:  98% 177/180 [02:11<00:02,  1.36it/s]\u001b[A\n","Iteration:  99% 178/180 [02:11<00:01,  1.34it/s]\u001b[A\n","Iteration:  99% 179/180 [02:12<00:00,  1.37it/s]\u001b[A\n","Iteration: 100% 180/180 [02:13<00:00,  1.35it/s]\n","Epoch: 100% 3/3 [06:35<00:00, 132.00s/it]\n","05/15/2020 03:02:30 - INFO - __main__ -    global_step = 270, average loss = 0.8202621709141467\n","05/15/2020 03:02:30 - INFO - __main__ -   Saving model checkpoint to run\n","05/15/2020 03:02:30 - INFO - transformers.configuration_utils -   Configuration saved in run/config.json\n","05/15/2020 03:02:32 - INFO - transformers.modeling_utils -   Model weights saved in run/pytorch_model.bin\n","05/15/2020 03:02:32 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n","05/15/2020 03:02:32 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 1,\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"sts-b\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/15/2020 03:02:32 - INFO - transformers.modeling_utils -   loading weights file run/pytorch_model.bin\n","05/15/2020 03:02:36 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n","05/15/2020 03:02:36 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 1,\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"sts-b\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/15/2020 03:02:36 - INFO - transformers.tokenization_utils -   Model name 'run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'run' is a path, a model identifier, or url to a directory containing tokenizer files.\n","05/15/2020 03:02:36 - INFO - transformers.tokenization_utils -   Didn't find file run/added_tokens.json. We won't load it.\n","05/15/2020 03:02:36 - INFO - transformers.tokenization_utils -   loading file run/vocab.json\n","05/15/2020 03:02:36 - INFO - transformers.tokenization_utils -   loading file run/merges.txt\n","05/15/2020 03:02:36 - INFO - transformers.tokenization_utils -   loading file None\n","05/15/2020 03:02:36 - INFO - transformers.tokenization_utils -   loading file run/special_tokens_map.json\n","05/15/2020 03:02:36 - INFO - transformers.tokenization_utils -   loading file run/tokenizer_config.json\n","05/15/2020 03:02:37 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n","05/15/2020 03:02:37 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 1,\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"sts-b\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/15/2020 03:02:37 - INFO - transformers.tokenization_utils -   Model name 'run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'run' is a path, a model identifier, or url to a directory containing tokenizer files.\n","05/15/2020 03:02:37 - INFO - transformers.tokenization_utils -   Didn't find file run/added_tokens.json. We won't load it.\n","05/15/2020 03:02:37 - INFO - transformers.tokenization_utils -   loading file run/vocab.json\n","05/15/2020 03:02:37 - INFO - transformers.tokenization_utils -   loading file run/merges.txt\n","05/15/2020 03:02:37 - INFO - transformers.tokenization_utils -   loading file None\n","05/15/2020 03:02:37 - INFO - transformers.tokenization_utils -   loading file run/special_tokens_map.json\n","05/15/2020 03:02:37 - INFO - transformers.tokenization_utils -   loading file run/tokenizer_config.json\n","05/15/2020 03:02:37 - INFO - __main__ -   Evaluate the following checkpoints: ['run']\n","05/15/2020 03:02:37 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n","05/15/2020 03:02:37 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 1,\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"sts-b\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/15/2020 03:02:37 - INFO - transformers.modeling_utils -   loading weights file run/pytorch_model.bin\n","05/15/2020 03:02:42 - INFO - __main__ -   Loading features from cached file data/glue/STS-B/cached_dev_roberta_jumbled_token_discrimination_lr_e-4_prob_0.15_128_sts-b\n","05/15/2020 03:02:42 - INFO - __main__ -   ***** Running evaluation  *****\n","05/15/2020 03:02:42 - INFO - __main__ -     Num examples = 1500\n","05/15/2020 03:02:42 - INFO - __main__ -     Batch size = 32\n","Evaluating: 100% 47/47 [00:11<00:00,  4.12it/s]\n","05/15/2020 03:02:53 - INFO - __main__ -   ***** Eval results  *****\n","05/15/2020 03:02:53 - INFO - __main__ -     corr = 0.8869839692143657\n","05/15/2020 03:02:53 - INFO - __main__ -     pearson = 0.8875593986710335\n","05/15/2020 03:02:53 - INFO - __main__ -     spearmanr = 0.8864085397576981\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_2xmVAZiqiCK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}