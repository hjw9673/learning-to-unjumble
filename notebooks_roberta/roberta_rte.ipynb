{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta_rte.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmnJe06SfVMN",
        "colab_type": "text"
      },
      "source": [
        "# Clone transformers repo and checkout version 2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rtNjBbdY7RM",
        "colab_type": "code",
        "outputId": "e8d09345-325f-4b85-ef99-635168d86b16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!rm -rf transformers\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 23175, done.\u001b[K\n",
            "remote: Total 23175 (delta 0), reused 0 (delta 0), pack-reused 23175\u001b[K\n",
            "Receiving objects: 100% (23175/23175), 13.39 MiB | 27.81 MiB/s, done.\n",
            "Resolving deltas: 100% (16578/16578), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU2wFP6SbsZf",
        "colab_type": "code",
        "outputId": "dddc881d-2049-4b15-ae32-df9cca5177bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd transformers/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LON2QVZObFug",
        "colab_type": "code",
        "outputId": "6a3906c2-6acf-46a2-e3fe-c0c91b808333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "!git checkout v2.7.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: checking out 'v2.7.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 6f5a12a5 Release: v2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnSg5LWjfn4w",
        "colab_type": "text"
      },
      "source": [
        "# Install packages required for running `run_glue.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScFAOBb2bRc-",
        "colab_type": "code",
        "outputId": "fc8f9c1e-b83f-4811-c937-7e30a7b085e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        }
      },
      "source": [
        "!pip install -r ./examples/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 1)) (2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 4)) (0.0.12)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 5)) (5.4.8)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 6)) (1.4.6)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 7)) (0.0.3)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.27.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (46.0.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.2.5)\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (0.996.5)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (3.6.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (1.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (19.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (4.38.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.21.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.51.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDRsdU00kop7",
        "colab_type": "code",
        "outputId": "7870fef2-86ec-46fb-e7e2-7b8aa23094f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "!pip install boto3 filelock requests tqdm sentencepiece sacremoses tokenizers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.12.27)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.38)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.15.27)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INV4cGxyf2f5",
        "colab_type": "text"
      },
      "source": [
        "# Download GLUE data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWYcFKd9f6Fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLUE_DIR=\"/content/data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AJp5KgQglVh",
        "colab_type": "code",
        "outputId": "5e78331b-572d-4ffc-b6e0-518e8add3e1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!echo $GLUE_DIR"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKV9-fEBhDQ2",
        "colab_type": "code",
        "outputId": "348a436e-2e4c-44cd-8745-e914a552a0b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/transformers'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24BX_hKwFhdU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "e6323834-7268-4f9e-bc5a-c1da99c596e8"
      },
      "source": [
        "!python ./utils/download_glue_data.py --help"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: download_glue_data.py [-h] [--data_dir DATA_DIR] [--tasks TASKS]\n",
            "                             [--path_to_mrpc PATH_TO_MRPC]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_dir DATA_DIR   directory to save data to\n",
            "  --tasks TASKS         tasks to download data for as a comma separated string\n",
            "  --path_to_mrpc PATH_TO_MRPC\n",
            "                        path to directory containing extracted MRPC data,\n",
            "                        msr_paraphrase_train.txt and msr_paraphrase_text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAqf1Fe6cZvI",
        "colab_type": "code",
        "outputId": "fb1f6f9b-ba53-48c3-8fbe-3efa478fa61b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!python ./utils/download_glue_data.py --data_dir $GLUE_DIR --tasks RTE"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aqkcWzfgFNI",
        "colab_type": "text"
      },
      "source": [
        "# Compute RoBERTa score on RTE task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFhfVflFdaVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TASK_NAME=\"RTE\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_1EtLRQ8-WF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c0457877-f60d-46b2-c965-bf0e4fff6070"
      },
      "source": [
        "!echo $TASK_NAME"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RTE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_9qkZrE8xgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aef95f61-e6e0-47ef-e177-a92e39fd270f"
      },
      "source": [
        "!echo $PYTHONPATH"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szrKUidsj9kJ",
        "colab_type": "code",
        "outputId": "aee02826-3dbd-4ec5-b0c6-3c1744c2dcdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%set_env PYTHONPATH=/content/transformers/src:/env/python"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=/content/transformers/src:/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YoNLeB9kNi4",
        "colab_type": "code",
        "outputId": "c39e484e-ae79-4adb-fa67-7dcfd783abaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!echo $PYTHONPATH"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers/src:/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPpl0f8wAkmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7894080-834a-4c9d-cbef-3e85d724befe"
      },
      "source": [
        "!python ./examples/run_glue.py --help"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-30 20:17:05.134152: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: run_glue.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n",
            "                   --model_name_or_path MODEL_NAME_OR_PATH --task_name\n",
            "                   TASK_NAME --output_dir OUTPUT_DIR\n",
            "                   [--config_name CONFIG_NAME]\n",
            "                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                   [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n",
            "                   [--evaluate_during_training] [--do_lower_case]\n",
            "                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                   [--learning_rate LEARNING_RATE]\n",
            "                   [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
            "                   [--max_grad_norm MAX_GRAD_NORM]\n",
            "                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                   [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
            "                   [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
            "                   [--eval_all_checkpoints] [--no_cuda]\n",
            "                   [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n",
            "                   [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                   [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n",
            "                   [--server_port SERVER_PORT]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n",
            "                        other data files) for the task.\n",
            "  --model_type MODEL_TYPE\n",
            "                        Model type selected in the list: distilbert, albert,\n",
            "                        camembert, xlm-roberta, bart, roberta, bert, xlnet,\n",
            "                        flaubert, xlm\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        Path to pre-trained model or shortcut name selected in\n",
            "                        the list: distilbert-base-uncased, distilbert-base-\n",
            "                        uncased-distilled-squad, distilbert-base-cased,\n",
            "                        distilbert-base-cased-distilled-squad, distilbert-\n",
            "                        base-german-cased, distilbert-base-multilingual-cased,\n",
            "                        distilbert-base-uncased-finetuned-sst-2-english,\n",
            "                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\n",
            "                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\n",
            "                        albert-xlarge-v2, albert-xxlarge-v2, camembert-base,\n",
            "                        umberto-commoncrawl-cased-v1, umberto-wikipedia-\n",
            "                        uncased-v1, xlm-roberta-base, xlm-roberta-large, xlm-\n",
            "                        roberta-large-finetuned-conll02-dutch, xlm-roberta-\n",
            "                        large-finetuned-conll02-spanish, xlm-roberta-large-\n",
            "                        finetuned-conll03-english, xlm-roberta-large-\n",
            "                        finetuned-conll03-german, bart-large, bart-large-mnli,\n",
            "                        bart-large-cnn, bart-large-xsum, roberta-base,\n",
            "                        roberta-large, roberta-large-mnli, distilroberta-base,\n",
            "                        roberta-base-openai-detector, roberta-large-openai-\n",
            "                        detector, bert-base-uncased, bert-large-uncased, bert-\n",
            "                        base-cased, bert-large-cased, bert-base-multilingual-\n",
            "                        uncased, bert-base-multilingual-cased, bert-base-\n",
            "                        chinese, bert-base-german-cased, bert-large-uncased-\n",
            "                        whole-word-masking, bert-large-cased-whole-word-\n",
            "                        masking, bert-large-uncased-whole-word-masking-\n",
            "                        finetuned-squad, bert-large-cased-whole-word-masking-\n",
            "                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\n",
            "                        base-german-dbmdz-cased, bert-base-german-dbmdz-\n",
            "                        uncased, bert-base-japanese, bert-base-japanese-whole-\n",
            "                        word-masking, bert-base-japanese-char, bert-base-\n",
            "                        japanese-char-whole-word-masking, bert-base-finnish-\n",
            "                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\n",
            "                        dutch-cased, xlnet-base-cased, xlnet-large-cased,\n",
            "                        flaubert-small-cased, flaubert-base-uncased, flaubert-\n",
            "                        base-cased, flaubert-large-cased, xlm-mlm-en-2048,\n",
            "                        xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-\n",
            "                        enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\n",
            "                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\n",
            "                        xlm-mlm-17-1280, xlm-mlm-100-1280\n",
            "  --task_name TASK_NAME\n",
            "                        The name of the task to train selected in the list:\n",
            "                        cola, mnli, mnli-mm, mrpc, sst-2, sts-b, qqp, qnli,\n",
            "                        rte, wnli\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pre-trained models\n",
            "                        downloaded from s3\n",
            "  --max_seq_length MAX_SEQ_LENGTH\n",
            "                        The maximum total input sequence length after\n",
            "                        tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded.\n",
            "  --do_train            Whether to run training.\n",
            "  --do_eval             Whether to run eval on the dev set.\n",
            "  --evaluate_during_training\n",
            "                        Run evaluation during training at each logging step.\n",
            "  --do_lower_case       Set this flag if you are using an uncased model.\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/CPU for training.\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/CPU for evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for Adam.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay if we apply some.\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for Adam optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --eval_all_checkpoints\n",
            "                        Evaluate all checkpoints starting with the same prefix\n",
            "                        as model_name ending and ending with step number\n",
            "  --no_cuda             Avoid using CUDA when available\n",
            "  --overwrite_output_dir\n",
            "                        Overwrite the content of the output directory\n",
            "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
            "  --seed SEED           random seed for initialization\n",
            "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
            "                        NVIDIA apex) instead of 32-bit\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --server_ip SERVER_IP\n",
            "                        For distant debugging.\n",
            "  --server_port SERVER_PORT\n",
            "                        For distant debugging.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnxryCpnczcm",
        "colab_type": "code",
        "outputId": "b02071a0-2744-41e4-a473-e947d17add4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./examples/run_glue.py \\\n",
        "    --model_type roberta \\\n",
        "    --model_name_or_path roberta-base \\\n",
        "    --task_name $TASK_NAME \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=64   \\\n",
        "    --per_gpu_train_batch_size=64   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --output_dir ../${TASK_NAME}_run \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-03-30 20:17:18.031933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "03/30/2020 20:17:19 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/30/2020 20:17:20 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/30/2020 20:17:20 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/30/2020 20:17:20 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/30/2020 20:17:20 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/30/2020 20:17:21 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/30/2020 20:17:21 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/30/2020 20:17:21 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/30/2020 20:17:24 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "03/30/2020 20:17:24 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "03/30/2020 20:17:28 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/content/data/RTE', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='../', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='rte', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "03/30/2020 20:17:28 - INFO - __main__ -   Creating features from dataset file at /content/data/RTE\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   Writing example 0/2490\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   guid: train-0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   input_ids: 0 440 28054 9 5370 43207 11911 11 3345 3507 4 2 2 28054 9 5370 43207 11911 11 3345 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   input_ids: 0 83 317 9 26130 6 71 8509 610 1206 3082 962 6 1059 10 317 9 4821 6 25 7733 4019 15828 4366 11 3301 1568 7 2458 5 8809 9 92 8509 20742 42171 4 2 2 8509 20742 42171 16 5 92 884 9 5 7733 4019 2197 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   input_ids: 0 1405 16771 179 21 416 2033 7 3951 5 4736 990 6181 1668 1484 6 8 5 138 26 6 302 6 24 40 2268 19 752 5904 5 3302 9 30724 5 1262 13 55 6181 1668 1484 4 2 2 1405 16771 179 64 28 341 7 3951 6181 1668 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   input_ids: 0 19691 324 12701 811 6 834 1031 23 1698 21243 2426 6 10 1131 544 138 14 2607 9844 5 132 12 180 12 279 5490 6924 2534 11 5082 12718 3635 298 412 36 30520 4141 32410 238 26 14 98 444 59 112 6 1497 408 33 829 1416 4 2 2 20 986 766 9 5082 12718 3635 298 412 21 4141 32410 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   input_ids: 0 83 313 16 528 11 461 423 1340 19 5 1900 973 107 536 9 10 7044 1060 403 21 5 78 7 28 3520 15 3295 509 18 22548 2753 11175 4 944 13476 21918 6 545 6 21 3051 7 69 6578 18 790 11 4300 5985 6 17142 9959 6 15 389 779 13668 77 79 9939 4 1405 809 21 423 303 11 10 882 593 7 69 184 4 1206 6192 21274 6 654 6 34 57 1340 19 1900 8 16 528 137 17142 9931 15596 423 4 2 2 1206 6192 21274 16 1238 9 519 9229 10 1816 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:17:28 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "03/30/2020 20:17:30 - INFO - __main__ -   Saving features into cached file /content/data/RTE/cached_train_roberta-base_128_rte\n",
            "03/30/2020 20:17:30 - INFO - __main__ -   ***** Running training *****\n",
            "03/30/2020 20:17:30 - INFO - __main__ -     Num examples = 2490\n",
            "03/30/2020 20:17:30 - INFO - __main__ -     Num Epochs = 3\n",
            "03/30/2020 20:17:30 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n",
            "03/30/2020 20:17:30 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "03/30/2020 20:17:30 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/30/2020 20:17:30 - INFO - __main__ -     Total optimization steps = 117\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   3% 1/39 [00:00<00:27,  1.37it/s]\u001b[A\n",
            "Iteration:   5% 2/39 [00:01<00:26,  1.38it/s]\u001b[A\n",
            "Iteration:   8% 3/39 [00:02<00:25,  1.39it/s]\u001b[A\n",
            "Iteration:  10% 4/39 [00:02<00:25,  1.39it/s]\u001b[A\n",
            "Iteration:  13% 5/39 [00:03<00:24,  1.40it/s]\u001b[A\n",
            "Iteration:  15% 6/39 [00:04<00:23,  1.40it/s]\u001b[A\n",
            "Iteration:  18% 7/39 [00:04<00:22,  1.40it/s]\u001b[A\n",
            "Iteration:  21% 8/39 [00:05<00:22,  1.40it/s]\u001b[A\n",
            "Iteration:  23% 9/39 [00:06<00:21,  1.40it/s]\u001b[A\n",
            "Iteration:  26% 10/39 [00:07<00:20,  1.40it/s]\u001b[A\n",
            "Iteration:  28% 11/39 [00:07<00:19,  1.40it/s]\u001b[A\n",
            "Iteration:  31% 12/39 [00:08<00:19,  1.41it/s]\u001b[A\n",
            "Iteration:  33% 13/39 [00:09<00:18,  1.41it/s]\u001b[A\n",
            "Iteration:  36% 14/39 [00:09<00:17,  1.41it/s]\u001b[A\n",
            "Iteration:  38% 15/39 [00:10<00:17,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 16/39 [00:11<00:16,  1.41it/s]\u001b[A\n",
            "Iteration:  44% 17/39 [00:12<00:15,  1.41it/s]\u001b[A\n",
            "Iteration:  46% 18/39 [00:12<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  49% 19/39 [00:13<00:14,  1.40it/s]\u001b[A\n",
            "Iteration:  51% 20/39 [00:14<00:13,  1.40it/s]\u001b[A\n",
            "Iteration:  54% 21/39 [00:14<00:12,  1.41it/s]\u001b[A\n",
            "Iteration:  56% 22/39 [00:15<00:12,  1.40it/s]\u001b[A\n",
            "Iteration:  59% 23/39 [00:16<00:11,  1.41it/s]\u001b[A\n",
            "Iteration:  62% 24/39 [00:17<00:10,  1.40it/s]\u001b[A\n",
            "Iteration:  64% 25/39 [00:17<00:09,  1.40it/s]\u001b[A\n",
            "Iteration:  67% 26/39 [00:18<00:09,  1.40it/s]\u001b[A\n",
            "Iteration:  69% 27/39 [00:19<00:08,  1.40it/s]\u001b[A\n",
            "Iteration:  72% 28/39 [00:19<00:07,  1.40it/s]\u001b[A\n",
            "Iteration:  74% 29/39 [00:20<00:07,  1.40it/s]\u001b[A\n",
            "Iteration:  77% 30/39 [00:21<00:06,  1.40it/s]\u001b[A\n",
            "Iteration:  79% 31/39 [00:22<00:05,  1.40it/s]\u001b[A\n",
            "Iteration:  82% 32/39 [00:22<00:04,  1.40it/s]\u001b[A\n",
            "Iteration:  85% 33/39 [00:23<00:04,  1.41it/s]\u001b[A\n",
            "Iteration:  87% 34/39 [00:24<00:03,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 35/39 [00:24<00:02,  1.40it/s]\u001b[A\n",
            "Iteration:  92% 36/39 [00:25<00:02,  1.41it/s]\u001b[A\n",
            "Iteration:  95% 37/39 [00:26<00:01,  1.41it/s]\u001b[A\n",
            "Iteration:  97% 38/39 [00:27<00:00,  1.40it/s]\u001b[A\n",
            "Iteration: 100% 39/39 [00:27<00:00,  1.41it/s]\n",
            "Epoch:  33% 1/3 [00:27<00:55, 27.72s/it]\n",
            "Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   3% 1/39 [00:00<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:   5% 2/39 [00:01<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:   8% 3/39 [00:02<00:25,  1.41it/s]\u001b[A\n",
            "Iteration:  10% 4/39 [00:02<00:24,  1.40it/s]\u001b[A\n",
            "Iteration:  13% 5/39 [00:03<00:24,  1.40it/s]\u001b[A\n",
            "Iteration:  15% 6/39 [00:04<00:23,  1.40it/s]\u001b[A\n",
            "Iteration:  18% 7/39 [00:04<00:22,  1.40it/s]\u001b[A\n",
            "Iteration:  21% 8/39 [00:05<00:22,  1.40it/s]\u001b[A\n",
            "Iteration:  23% 9/39 [00:06<00:21,  1.41it/s]\u001b[A\n",
            "Iteration:  26% 10/39 [00:07<00:20,  1.40it/s]\u001b[A\n",
            "Iteration:  28% 11/39 [00:07<00:19,  1.41it/s]\u001b[A\n",
            "Iteration:  31% 12/39 [00:08<00:19,  1.41it/s]\u001b[A\n",
            "Iteration:  33% 13/39 [00:09<00:18,  1.40it/s]\u001b[A\n",
            "Iteration:  36% 14/39 [00:09<00:17,  1.40it/s]\u001b[A\n",
            "Iteration:  38% 15/39 [00:10<00:17,  1.40it/s]\u001b[A\n",
            "Iteration:  41% 16/39 [00:11<00:16,  1.41it/s]\u001b[A\n",
            "Iteration:  44% 17/39 [00:12<00:15,  1.41it/s]\u001b[A\n",
            "Iteration:  46% 18/39 [00:12<00:14,  1.41it/s]\u001b[A\n",
            "Iteration:  49% 19/39 [00:13<00:14,  1.40it/s]\u001b[A\n",
            "Iteration:  51% 20/39 [00:14<00:13,  1.41it/s]\u001b[A\n",
            "Iteration:  54% 21/39 [00:14<00:12,  1.40it/s]\u001b[A\n",
            "Iteration:  56% 22/39 [00:15<00:12,  1.40it/s]\u001b[A\n",
            "Iteration:  59% 23/39 [00:16<00:11,  1.40it/s]\u001b[A\n",
            "Iteration:  62% 24/39 [00:17<00:10,  1.40it/s]\u001b[A\n",
            "Iteration:  64% 25/39 [00:17<00:09,  1.40it/s]\u001b[A\n",
            "Iteration:  67% 26/39 [00:18<00:09,  1.40it/s]\u001b[A\n",
            "Iteration:  69% 27/39 [00:19<00:08,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 28/39 [00:19<00:07,  1.41it/s]\u001b[A\n",
            "Iteration:  74% 29/39 [00:20<00:07,  1.41it/s]\u001b[A\n",
            "Iteration:  77% 30/39 [00:21<00:06,  1.41it/s]\u001b[A\n",
            "Iteration:  79% 31/39 [00:22<00:05,  1.41it/s]\u001b[A\n",
            "Iteration:  82% 32/39 [00:22<00:04,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 33/39 [00:23<00:04,  1.40it/s]\u001b[A\n",
            "Iteration:  87% 34/39 [00:24<00:03,  1.41it/s]\u001b[A\n",
            "Iteration:  90% 35/39 [00:24<00:02,  1.40it/s]\u001b[A\n",
            "Iteration:  92% 36/39 [00:25<00:02,  1.40it/s]\u001b[A\n",
            "Iteration:  95% 37/39 [00:26<00:01,  1.40it/s]\u001b[A\n",
            "Iteration:  97% 38/39 [00:27<00:00,  1.40it/s]\u001b[A\n",
            "Iteration: 100% 39/39 [00:27<00:00,  1.41it/s]\n",
            "Epoch:  67% 2/3 [00:55<00:27, 27.72s/it]\n",
            "Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   3% 1/39 [00:00<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:   5% 2/39 [00:01<00:26,  1.41it/s]\u001b[A\n",
            "Iteration:   8% 3/39 [00:02<00:25,  1.41it/s]\u001b[A\n",
            "Iteration:  10% 4/39 [00:02<00:24,  1.41it/s]\u001b[A\n",
            "Iteration:  13% 5/39 [00:03<00:24,  1.41it/s]\u001b[A\n",
            "Iteration:  15% 6/39 [00:04<00:23,  1.41it/s]\u001b[A\n",
            "Iteration:  18% 7/39 [00:04<00:22,  1.40it/s]\u001b[A\n",
            "Iteration:  21% 8/39 [00:05<00:22,  1.41it/s]\u001b[A\n",
            "Iteration:  23% 9/39 [00:06<00:21,  1.41it/s]\u001b[A\n",
            "Iteration:  26% 10/39 [00:07<00:20,  1.40it/s]\u001b[A\n",
            "Iteration:  28% 11/39 [00:07<00:19,  1.40it/s]\u001b[A\n",
            "Iteration:  31% 12/39 [00:08<00:19,  1.40it/s]\u001b[A\n",
            "Iteration:  33% 13/39 [00:09<00:18,  1.40it/s]\u001b[A\n",
            "Iteration:  36% 14/39 [00:09<00:17,  1.40it/s]\u001b[A\n",
            "Iteration:  38% 15/39 [00:10<00:17,  1.41it/s]\u001b[A\n",
            "Iteration:  41% 16/39 [00:11<00:16,  1.40it/s]\u001b[A\n",
            "Iteration:  44% 17/39 [00:12<00:15,  1.41it/s]\u001b[A\n",
            "Iteration:  46% 18/39 [00:12<00:14,  1.40it/s]\u001b[A\n",
            "Iteration:  49% 19/39 [00:13<00:14,  1.40it/s]\u001b[A\n",
            "Iteration:  51% 20/39 [00:14<00:13,  1.40it/s]\u001b[A\n",
            "Iteration:  54% 21/39 [00:14<00:12,  1.40it/s]\u001b[A\n",
            "Iteration:  56% 22/39 [00:15<00:12,  1.41it/s]\u001b[A\n",
            "Iteration:  59% 23/39 [00:16<00:11,  1.40it/s]\u001b[A\n",
            "Iteration:  62% 24/39 [00:17<00:10,  1.40it/s]\u001b[A\n",
            "Iteration:  64% 25/39 [00:17<00:09,  1.41it/s]\u001b[A\n",
            "Iteration:  67% 26/39 [00:18<00:09,  1.41it/s]\u001b[A\n",
            "Iteration:  69% 27/39 [00:19<00:08,  1.41it/s]\u001b[A\n",
            "Iteration:  72% 28/39 [00:19<00:07,  1.40it/s]\u001b[A\n",
            "Iteration:  74% 29/39 [00:20<00:07,  1.41it/s]\u001b[A\n",
            "Iteration:  77% 30/39 [00:21<00:06,  1.41it/s]\u001b[A\n",
            "Iteration:  79% 31/39 [00:22<00:05,  1.40it/s]\u001b[A\n",
            "Iteration:  82% 32/39 [00:22<00:04,  1.41it/s]\u001b[A\n",
            "Iteration:  85% 33/39 [00:23<00:04,  1.40it/s]\u001b[A\n",
            "Iteration:  87% 34/39 [00:24<00:03,  1.40it/s]\u001b[A\n",
            "Iteration:  90% 35/39 [00:24<00:02,  1.40it/s]\u001b[A\n",
            "Iteration:  92% 36/39 [00:25<00:02,  1.40it/s]\u001b[A\n",
            "Iteration:  95% 37/39 [00:26<00:01,  1.40it/s]\u001b[A\n",
            "Iteration:  97% 38/39 [00:27<00:00,  1.40it/s]\u001b[A\n",
            "Iteration: 100% 39/39 [00:27<00:00,  1.41it/s]\n",
            "Epoch: 100% 3/3 [01:23<00:00, 27.72s/it]\n",
            "03/30/2020 20:18:53 - INFO - __main__ -    global_step = 117, average loss = 0.6388599750323173\n",
            "03/30/2020 20:18:53 - INFO - __main__ -   Saving model checkpoint to ../\n",
            "03/30/2020 20:18:53 - INFO - transformers.configuration_utils -   Configuration saved in ../config.json\n",
            "03/30/2020 20:18:55 - INFO - transformers.modeling_utils -   Model weights saved in ../pytorch_model.bin\n",
            "03/30/2020 20:18:55 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
            "03/30/2020 20:18:55 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/30/2020 20:18:55 - INFO - transformers.modeling_utils -   loading weights file ../pytorch_model.bin\n",
            "03/30/2020 20:18:58 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
            "03/30/2020 20:18:58 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/30/2020 20:18:58 - INFO - transformers.tokenization_utils -   Model name '../' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/30/2020 20:18:58 - INFO - transformers.tokenization_utils -   Didn't find file ../added_tokens.json. We won't load it.\n",
            "03/30/2020 20:18:58 - INFO - transformers.tokenization_utils -   loading file ../vocab.json\n",
            "03/30/2020 20:18:58 - INFO - transformers.tokenization_utils -   loading file ../merges.txt\n",
            "03/30/2020 20:18:58 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/30/2020 20:18:58 - INFO - transformers.tokenization_utils -   loading file ../special_tokens_map.json\n",
            "03/30/2020 20:18:58 - INFO - transformers.tokenization_utils -   loading file ../tokenizer_config.json\n",
            "03/30/2020 20:18:59 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
            "03/30/2020 20:18:59 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/30/2020 20:18:59 - INFO - transformers.tokenization_utils -   Model name '../' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/30/2020 20:18:59 - INFO - transformers.tokenization_utils -   Didn't find file ../added_tokens.json. We won't load it.\n",
            "03/30/2020 20:18:59 - INFO - transformers.tokenization_utils -   loading file ../vocab.json\n",
            "03/30/2020 20:18:59 - INFO - transformers.tokenization_utils -   loading file ../merges.txt\n",
            "03/30/2020 20:18:59 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/30/2020 20:18:59 - INFO - transformers.tokenization_utils -   loading file ../special_tokens_map.json\n",
            "03/30/2020 20:18:59 - INFO - transformers.tokenization_utils -   loading file ../tokenizer_config.json\n",
            "03/30/2020 20:18:59 - INFO - __main__ -   Evaluate the following checkpoints: ['../']\n",
            "03/30/2020 20:18:59 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
            "03/30/2020 20:18:59 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/30/2020 20:18:59 - INFO - transformers.modeling_utils -   loading weights file ../pytorch_model.bin\n",
            "03/30/2020 20:19:02 - INFO - __main__ -   Creating features from dataset file at /content/data/RTE\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   Writing example 0/277\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   input_ids: 0 11014 25459 548 6 5 18141 9 5 2701 5469 25459 548 6 34 962 9 10665 1668 23 1046 3550 6 309 7 5 5469 25459 548 2475 4 2 2 5469 25459 548 56 41 3213 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   input_ids: 0 3507 6 52 122 32 18152 14 16674 32 2086 49 12833 136 5467 4 11817 12 3245 10928 9436 32 16119 1295 3845 87 52 64 283 62 19 92 16674 7 1032 5 92 18746 4 2 2 163 42069 16 1298 5 997 136 16674 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   input_ids: 0 14794 16 122 184 7 103 379 153 82 111 10 24357 1956 14 9108 2219 158 6 151 5657 9 22051 228 183 6 2057 41 7934 8793 15 285 518 4 96 5 375 158 107 6 5 168 34 1381 543 7 3803 940 915 11 5 10383 1293 6 53 103 3278 204 6 151 5657 9 3844 16 314 639 358 183 6 856 8939 154 11 5 2859 25 24 23120 13 951 7 699 24 62 4 85 16 747 5 82 11 5 19125 26075 14 32 2373 2132 4 125 11 103 911 51 32 2190 124 4 96 840 1792 763 6 65 2 2 379 153 5657 9 22051 32 2622 1230 11 14794 4 2\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   input_ids: 0 20 1918 1173 435 11 4367 6 61 1530 59 3490 6 151 6 1074 41 5951 338 9063 6339 6 1481 26586 9874 9766 101 4382 8 27596 4 178 171 224 49 7540 8244 6339 2029 106 10 1472 14 51 32 4371 31 5 1476 9 470 2313 4 125 25 1196 4366 583 5 334 6 103 2498 2065 15475 428 8 7789 11 5253 12 31798 10306 44279 6 51 26 14 1472 9 1078 56 57 17306 4 22 1106 951 11803 8 1072 7 109 402 12103 6 89 18 117 4472 14 18 164 7 912 106 60 26 6469 1745 6 4772 2 2 4367 34 5 934 1918 1173 435 11 5 121 4 104 4 2\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   input_ids: 0 2010 1572 58 15 239 5439 71 41 729 637 11 61 55 87 112 6 151 82 6 217 707 729 2261 6 33 57 848 4 2 2 2010 1572 58 15 239 5439 71 10 637 4401 2050 30 1476 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "03/30/2020 20:19:02 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
            "03/30/2020 20:19:02 - INFO - __main__ -   Saving features into cached file /content/data/RTE/cached_dev_roberta-base_128_rte\n",
            "03/30/2020 20:19:02 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/30/2020 20:19:02 - INFO - __main__ -     Num examples = 277\n",
            "03/30/2020 20:19:02 - INFO - __main__ -     Batch size = 64\n",
            "Evaluating: 100% 5/5 [00:01<00:00,  4.87it/s]\n",
            "03/30/2020 20:19:04 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/30/2020 20:19:04 - INFO - __main__ -     acc = 0.6606498194945848\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}