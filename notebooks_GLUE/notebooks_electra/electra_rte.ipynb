{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "electra_rte.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "mount_file_id": "1KoecUdaNaKqYEbs5EQHBPnYEqRg4g56y",
   "authorship_tag": "ABX9TyMm09ByifXf35LDT0KS+GVz"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmnJe06SfVMN",
    "colab_type": "text"
   },
   "source": [
    "# Install transformers 2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_rtNjBbdY7RM",
    "colab_type": "code",
    "outputId": "44c7073e-71d1-4d0a-ff46-3790a89ca976",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588684974767,
     "user_tz": 240,
     "elapsed": 4028,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    }
   },
   "source": [
    "!pip install transformers==2.8.0"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.5.2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.12.47)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.38.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.86)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.15.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers==2.8.0) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers==2.8.0) (2.8.1)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_moWpok-qkX",
    "colab_type": "text"
   },
   "source": [
    "# Set working directory to the directory containing `download_glue_data.py` and `run_glue.py`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l0VFWNoOAfT-",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "55d6c143-6320-4011-d9de-ffdbd002eb49",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685129825,
     "user_tz": 240,
     "elapsed": 667,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    }
   },
   "source": [
    "cd ~/../content/"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JU2wFP6SbsZf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LON2QVZObFug",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "WORK_DIR = os.path.join('drive', 'My Drive', 'Colab Notebooks', 'NLU')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2mWKKk1x86hQ",
    "colab_type": "code",
    "outputId": "a99ad5f5-0f00-49e6-cb9a-8bb576b12766",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685182625,
     "user_tz": 240,
     "elapsed": 547,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "os.path.exists(WORK_DIR)"
   ],
   "execution_count": 47,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 47
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cU0UdJuH9F2Y",
    "colab_type": "code",
    "outputId": "3ed5d7c4-d97e-43f8-8344-18e9d3a90163",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685185918,
     "user_tz": 240,
     "elapsed": 639,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "cd $WORK_DIR"
   ],
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/NLU\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INV4cGxyf2f5",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Download GLUE data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PWYcFKd9f6Fb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "GLUE_DIR=\"data/glue\""
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_AJp5KgQglVh",
    "colab_type": "code",
    "outputId": "94b2d8c7-829d-49f0-f785-e4f37ff71532",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685195281,
     "user_tz": 240,
     "elapsed": 2119,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "!echo $GLUE_DIR"
   ],
   "execution_count": 50,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "data/glue\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "24BX_hKwFhdU",
    "colab_type": "code",
    "outputId": "94854550-8b27-4f47-a3c1-9f67a64e35b8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685197205,
     "user_tz": 240,
     "elapsed": 2539,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    }
   },
   "source": [
    "!python download_glue_data.py --help"
   ],
   "execution_count": 51,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "usage: download_glue_data.py [-h] [--data_dir DATA_DIR] [--tasks TASKS]\n",
      "                             [--path_to_mrpc PATH_TO_MRPC]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --data_dir DATA_DIR   directory to save data to\n",
      "  --tasks TASKS         tasks to download data for as a comma separated string\n",
      "  --path_to_mrpc PATH_TO_MRPC\n",
      "                        path to directory containing extracted MRPC data,\n",
      "                        msr_paraphrase_train.txt and msr_paraphrase_text.txt\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rAqf1Fe6cZvI",
    "colab_type": "code",
    "outputId": "358e859d-8eed-4497-b0be-52ad353c47fa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685199241,
     "user_tz": 240,
     "elapsed": 2153,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!python download_glue_data.py --data_dir $GLUE_DIR --tasks RTE"
   ],
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading and extracting RTE...\n",
      "\tCompleted!\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aqkcWzfgFNI",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Compute ELECTRA score on RTE task"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LFhfVflFdaVB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "TASK_NAME=\"RTE\""
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q_1EtLRQ8-WF",
    "colab_type": "code",
    "outputId": "ee181acf-24b6-4d3f-e311-aa9a5d63d744",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685205871,
     "user_tz": 240,
     "elapsed": 2434,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "!echo $TASK_NAME"
   ],
   "execution_count": 54,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "RTE\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SPpl0f8wAkmf",
    "colab_type": "code",
    "outputId": "adbd92a9-8fad-4191-eddd-549d59e7fbfb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685210014,
     "user_tz": 240,
     "elapsed": 5189,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "!python run_glue.py --help"
   ],
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2020-05-05 13:26:44.856071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "usage: run_glue.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n",
      "                   --model_name_or_path MODEL_NAME_OR_PATH --task_name\n",
      "                   TASK_NAME --output_dir OUTPUT_DIR\n",
      "                   [--config_name CONFIG_NAME]\n",
      "                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
      "                   [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n",
      "                   [--evaluate_during_training] [--do_lower_case]\n",
      "                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                   [--learning_rate LEARNING_RATE]\n",
      "                   [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
      "                   [--max_grad_norm MAX_GRAD_NORM]\n",
      "                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                   [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
      "                   [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
      "                   [--eval_all_checkpoints] [--no_cuda]\n",
      "                   [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n",
      "                   [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                   [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n",
      "                   [--server_port SERVER_PORT]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n",
      "                        other data files) for the task.\n",
      "  --model_type MODEL_TYPE\n",
      "                        Model type selected in the list: distilbert, albert,\n",
      "                        camembert, xlm-roberta, bart, roberta, bert, xlnet,\n",
      "                        flaubert, xlm\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                        Path to pre-trained model or shortcut name selected in\n",
      "                        the list: distilbert-base-uncased, distilbert-base-\n",
      "                        uncased-distilled-squad, distilbert-base-cased,\n",
      "                        distilbert-base-cased-distilled-squad, distilbert-\n",
      "                        base-german-cased, distilbert-base-multilingual-cased,\n",
      "                        distilbert-base-uncased-finetuned-sst-2-english,\n",
      "                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\n",
      "                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\n",
      "                        albert-xlarge-v2, albert-xxlarge-v2, camembert-base,\n",
      "                        umberto-commoncrawl-cased-v1, umberto-wikipedia-\n",
      "                        uncased-v1, xlm-roberta-base, xlm-roberta-large, xlm-\n",
      "                        roberta-large-finetuned-conll02-dutch, xlm-roberta-\n",
      "                        large-finetuned-conll02-spanish, xlm-roberta-large-\n",
      "                        finetuned-conll03-english, xlm-roberta-large-\n",
      "                        finetuned-conll03-german, bart-large, bart-large-mnli,\n",
      "                        bart-large-cnn, bart-large-xsum, roberta-base,\n",
      "                        roberta-large, roberta-large-mnli, distilroberta-base,\n",
      "                        roberta-base-openai-detector, roberta-large-openai-\n",
      "                        detector, bert-base-uncased, bert-large-uncased, bert-\n",
      "                        base-cased, bert-large-cased, bert-base-multilingual-\n",
      "                        uncased, bert-base-multilingual-cased, bert-base-\n",
      "                        chinese, bert-base-german-cased, bert-large-uncased-\n",
      "                        whole-word-masking, bert-large-cased-whole-word-\n",
      "                        masking, bert-large-uncased-whole-word-masking-\n",
      "                        finetuned-squad, bert-large-cased-whole-word-masking-\n",
      "                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\n",
      "                        base-german-dbmdz-cased, bert-base-german-dbmdz-\n",
      "                        uncased, bert-base-japanese, bert-base-japanese-whole-\n",
      "                        word-masking, bert-base-japanese-char, bert-base-\n",
      "                        japanese-char-whole-word-masking, bert-base-finnish-\n",
      "                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\n",
      "                        dutch-cased, xlnet-base-cased, xlnet-large-cased,\n",
      "                        flaubert-small-cased, flaubert-base-uncased, flaubert-\n",
      "                        base-cased, flaubert-large-cased, xlm-mlm-en-2048,\n",
      "                        xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-\n",
      "                        enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\n",
      "                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\n",
      "                        xlm-mlm-17-1280, xlm-mlm-100-1280\n",
      "  --task_name TASK_NAME\n",
      "                        The name of the task to train selected in the list:\n",
      "                        cola, mnli, mnli-mm, mrpc, sst-2, sts-b, qqp, qnli,\n",
      "                        rte, wnli\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        The output directory where the model predictions and\n",
      "                        checkpoints will be written.\n",
      "  --config_name CONFIG_NAME\n",
      "                        Pretrained config name or path if not the same as\n",
      "                        model_name\n",
      "  --tokenizer_name TOKENIZER_NAME\n",
      "                        Pretrained tokenizer name or path if not the same as\n",
      "                        model_name\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Where do you want to store the pre-trained models\n",
      "                        downloaded from s3\n",
      "  --max_seq_length MAX_SEQ_LENGTH\n",
      "                        The maximum total input sequence length after\n",
      "                        tokenization. Sequences longer than this will be\n",
      "                        truncated, sequences shorter will be padded.\n",
      "  --do_train            Whether to run training.\n",
      "  --do_eval             Whether to run eval on the dev set.\n",
      "  --evaluate_during_training\n",
      "                        Run evaluation during training at each logging step.\n",
      "  --do_lower_case       Set this flag if you are using an uncased model.\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
      "                        Batch size per GPU/CPU for training.\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
      "                        Batch size per GPU/CPU for evaluation.\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate before\n",
      "                        performing a backward/update pass.\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        The initial learning rate for Adam.\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Weight decay if we apply some.\n",
      "  --adam_epsilon ADAM_EPSILON\n",
      "                        Epsilon for Adam optimizer.\n",
      "  --max_grad_norm MAX_GRAD_NORM\n",
      "                        Max gradient norm.\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        Total number of training epochs to perform.\n",
      "  --max_steps MAX_STEPS\n",
      "                        If > 0: set total number of training steps to perform.\n",
      "                        Override num_train_epochs.\n",
      "  --warmup_steps WARMUP_STEPS\n",
      "                        Linear warmup over warmup_steps.\n",
      "  --logging_steps LOGGING_STEPS\n",
      "                        Log every X updates steps.\n",
      "  --save_steps SAVE_STEPS\n",
      "                        Save checkpoint every X updates steps.\n",
      "  --eval_all_checkpoints\n",
      "                        Evaluate all checkpoints starting with the same prefix\n",
      "                        as model_name ending and ending with step number\n",
      "  --no_cuda             Avoid using CUDA when available\n",
      "  --overwrite_output_dir\n",
      "                        Overwrite the content of the output directory\n",
      "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
      "  --seed SEED           random seed for initialization\n",
      "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
      "                        NVIDIA apex) instead of 32-bit\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\n",
      "                        For fp16: Apex AMP optimization level selected in\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
      "                        https://nvidia.github.io/apex/amp.html\n",
      "  --local_rank LOCAL_RANK\n",
      "                        For distributed training: local_rank\n",
      "  --server_ip SERVER_IP\n",
      "                        For distant debugging.\n",
      "  --server_port SERVER_PORT\n",
      "                        For distant debugging.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python run_glue.py \\\n",
    "    --model_type electra \\\n",
    "    --model_name_or_path google/electra-base-discriminator \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_gpu_eval_batch_size=64   \\\n",
    "    --per_gpu_train_batch_size=64   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --output_dir ../${TASK_NAME}_run \\\n",
    "    --overwrite_output_dir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HnxryCpnczcm",
    "colab_type": "code",
    "outputId": "8c25a506-ab96-45d6-a196-93ab49395dea",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1588685577870,
     "user_tz": 240,
     "elapsed": 364336,
     "user": {
      "displayName": "Subhadarshi Panda",
      "photoUrl": "",
      "userId": "08772860912161050283"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [],
   "execution_count": 56,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2020-05-05 13:26:53.025072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "05/05/2020 13:26:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/05/2020 13:26:55 - INFO - filelock -   Lock 139792260045960 acquired on /root/.cache/torch/transformers/9236d197566a7f1be2b2151f5afcc5a8e17f31e1e23c52f3cdf2340019986e78.88ba6e8e7d5a7936e86d6f2551fe19c236dc57c24da163907cd0544e9933f6ee.lock\n",
      "05/05/2020 13:26:55 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpl57_vcd0\n",
      "Downloading: 100% 467/467 [00:00<00:00, 319kB/s]\n",
      "05/05/2020 13:26:56 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/config.json in cache at /root/.cache/torch/transformers/9236d197566a7f1be2b2151f5afcc5a8e17f31e1e23c52f3cdf2340019986e78.88ba6e8e7d5a7936e86d6f2551fe19c236dc57c24da163907cd0544e9933f6ee\n",
      "05/05/2020 13:26:56 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/9236d197566a7f1be2b2151f5afcc5a8e17f31e1e23c52f3cdf2340019986e78.88ba6e8e7d5a7936e86d6f2551fe19c236dc57c24da163907cd0544e9933f6ee\n",
      "05/05/2020 13:26:56 - INFO - filelock -   Lock 139792260045960 released on /root/.cache/torch/transformers/9236d197566a7f1be2b2151f5afcc5a8e17f31e1e23c52f3cdf2340019986e78.88ba6e8e7d5a7936e86d6f2551fe19c236dc57c24da163907cd0544e9933f6ee.lock\n",
      "05/05/2020 13:26:56 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/config.json from cache at /root/.cache/torch/transformers/9236d197566a7f1be2b2151f5afcc5a8e17f31e1e23c52f3cdf2340019986e78.88ba6e8e7d5a7936e86d6f2551fe19c236dc57c24da163907cd0544e9933f6ee\n",
      "05/05/2020 13:26:56 - INFO - transformers.configuration_utils -   Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": \"rte\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/05/2020 13:26:57 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/config.json from cache at /root/.cache/torch/transformers/9236d197566a7f1be2b2151f5afcc5a8e17f31e1e23c52f3cdf2340019986e78.88ba6e8e7d5a7936e86d6f2551fe19c236dc57c24da163907cd0544e9933f6ee\n",
      "05/05/2020 13:26:57 - INFO - transformers.configuration_utils -   Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/05/2020 13:26:58 - INFO - filelock -   Lock 139792260044784 acquired on /root/.cache/torch/transformers/ff085885d4c95651587af553adadd34a26de8a663f2cef709635b48b3bed2bbd.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "05/05/2020 13:26:58 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8pwbyrba\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 317kB/s]\n",
      "05/05/2020 13:26:59 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/vocab.txt in cache at /root/.cache/torch/transformers/ff085885d4c95651587af553adadd34a26de8a663f2cef709635b48b3bed2bbd.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "05/05/2020 13:26:59 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/ff085885d4c95651587af553adadd34a26de8a663f2cef709635b48b3bed2bbd.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "05/05/2020 13:26:59 - INFO - filelock -   Lock 139792260044784 released on /root/.cache/torch/transformers/ff085885d4c95651587af553adadd34a26de8a663f2cef709635b48b3bed2bbd.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "05/05/2020 13:26:59 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/vocab.txt from cache at /root/.cache/torch/transformers/ff085885d4c95651587af553adadd34a26de8a663f2cef709635b48b3bed2bbd.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "05/05/2020 13:27:00 - INFO - filelock -   Lock 139792260013528 acquired on /root/.cache/torch/transformers/3c8e97e5021532563898ceb491dbfbc068ab4cb9eaa31f555990b9993e3228b4.b7514d01ce5acfe02313470cce3175018852a5e8cbcb8784268ab87dc21daf4c.lock\n",
      "05/05/2020 13:27:00 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpkxh9s31g\n",
      "Downloading: 100% 440M/440M [00:38<00:00, 11.4MB/s]\n",
      "05/05/2020 13:27:40 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/pytorch_model.bin in cache at /root/.cache/torch/transformers/3c8e97e5021532563898ceb491dbfbc068ab4cb9eaa31f555990b9993e3228b4.b7514d01ce5acfe02313470cce3175018852a5e8cbcb8784268ab87dc21daf4c\n",
      "05/05/2020 13:27:40 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/3c8e97e5021532563898ceb491dbfbc068ab4cb9eaa31f555990b9993e3228b4.b7514d01ce5acfe02313470cce3175018852a5e8cbcb8784268ab87dc21daf4c\n",
      "05/05/2020 13:27:40 - INFO - filelock -   Lock 139792260013528 released on /root/.cache/torch/transformers/3c8e97e5021532563898ceb491dbfbc068ab4cb9eaa31f555990b9993e3228b4.b7514d01ce5acfe02313470cce3175018852a5e8cbcb8784268ab87dc21daf4c.lock\n",
      "05/05/2020 13:27:40 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/google/electra-base-discriminator/pytorch_model.bin from cache at /root/.cache/torch/transformers/3c8e97e5021532563898ceb491dbfbc068ab4cb9eaa31f555990b9993e3228b4.b7514d01ce5acfe02313470cce3175018852a5e8cbcb8784268ab87dc21daf4c\n",
      "05/05/2020 13:27:43 - INFO - transformers.modeling_utils -   Weights of ElectraForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "05/05/2020 13:27:43 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.embeddings_project.weight', 'electra.embeddings_project.bias']\n",
      "05/05/2020 13:27:45 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data/glue/RTE', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='google/electra-base-discriminator', model_type='electra', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='../', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='rte', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
      "05/05/2020 13:27:45 - INFO - __main__ -   Creating features from dataset file at data/glue/RTE\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   Writing example 0/2490\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   guid: train-0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   input_ids: 101 100 100 1997 100 100 100 1999 100 100 1012 102 100 1997 100 100 100 1999 100 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   guid: train-1\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   input_ids: 101 100 2173 1997 14038 1010 2044 100 100 100 100 2351 1010 2150 1037 2173 1997 7401 1010 2004 100 100 11633 5935 1999 5116 100 2000 2928 1996 8272 1997 2047 100 100 100 1012 102 100 100 100 2003 1996 2047 3003 1997 1996 100 100 100 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   guid: train-2\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   input_ids: 101 100 2001 2525 4844 2000 7438 1996 5305 4355 7388 4456 5022 1010 1998 1996 2194 2056 1010 100 1010 2009 2097 6848 2007 2976 25644 1996 6061 1997 3653 11020 3089 10472 1996 4319 2005 2062 7388 4456 5022 1012 102 100 2064 2022 2109 2000 7438 7388 4456 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   guid: train-3\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   input_ids: 101 100 100 1010 2708 3237 2012 100 1010 1037 2966 2326 2194 2008 7126 15770 1996 1016 1011 2095 1011 2214 100 100 100 1999 100 100 100 100 1006 3839 100 1007 1010 2056 2008 2061 2521 2055 1015 1010 3156 2336 2031 2363 3949 1012 102 100 3025 2171 1997 100 100 100 100 2001 100 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   guid: train-4\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   input_ids: 101 100 2158 2003 2349 1999 2457 2101 5338 2007 1996 4028 2656 2086 3283 1997 1037 10563 3005 2553 2001 1996 2034 2000 2022 2956 2006 100 100 1005 1055 100 1012 100 100 1010 2385 1010 2001 3788 2000 2014 6898 1005 1055 2160 1999 100 1010 100 1010 2006 2382 100 3172 2043 2016 5419 1012 100 2303 2001 2101 2179 1999 1037 2492 2485 2000 2014 2188 1012 100 100 100 1010 2753 1010 2038 2042 5338 2007 4028 1998 2003 2349 2077 100 23007 2101 1012 102 100 100 100 2003 5496 1997 2383 13263 1037 2611 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:27:45 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
      "05/05/2020 13:27:48 - INFO - __main__ -   Saving features into cached file data/glue/RTE/cached_train_electra-base-discriminator_128_rte\n",
      "05/05/2020 13:27:48 - INFO - __main__ -   ***** Running training *****\n",
      "05/05/2020 13:27:48 - INFO - __main__ -     Num examples = 2490\n",
      "05/05/2020 13:27:48 - INFO - __main__ -     Num Epochs = 3\n",
      "05/05/2020 13:27:48 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n",
      "05/05/2020 13:27:48 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/05/2020 13:27:48 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "05/05/2020 13:27:48 - INFO - __main__ -     Total optimization steps = 117\n",
      "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n",
      "\n",
      "Iteration:   3% 1/39 [00:02<01:37,  2.56s/it]\u001b[A\n",
      "Iteration:   5% 2/39 [00:05<01:34,  2.55s/it]\u001b[A\n",
      "Iteration:   8% 3/39 [00:07<01:31,  2.54s/it]\u001b[A\n",
      "Iteration:  10% 4/39 [00:10<01:28,  2.54s/it]\u001b[A\n",
      "Iteration:  13% 5/39 [00:12<01:26,  2.53s/it]\u001b[A\n",
      "Iteration:  15% 6/39 [00:15<01:23,  2.53s/it]\u001b[A\n",
      "Iteration:  18% 7/39 [00:17<01:20,  2.52s/it]\u001b[A\n",
      "Iteration:  21% 8/39 [00:20<01:18,  2.52s/it]\u001b[A\n",
      "Iteration:  23% 9/39 [00:22<01:15,  2.52s/it]\u001b[A\n",
      "Iteration:  26% 10/39 [00:25<01:12,  2.51s/it]\u001b[A\n",
      "Iteration:  28% 11/39 [00:27<01:10,  2.51s/it]\u001b[A\n",
      "Iteration:  31% 12/39 [00:30<01:07,  2.51s/it]\u001b[A\n",
      "Iteration:  33% 13/39 [00:32<01:05,  2.51s/it]\u001b[A\n",
      "Iteration:  36% 14/39 [00:35<01:02,  2.50s/it]\u001b[A\n",
      "Iteration:  38% 15/39 [00:37<01:00,  2.51s/it]\u001b[A\n",
      "Iteration:  41% 16/39 [00:40<00:57,  2.50s/it]\u001b[A\n",
      "Iteration:  44% 17/39 [00:42<00:54,  2.50s/it]\u001b[A\n",
      "Iteration:  46% 18/39 [00:45<00:52,  2.49s/it]\u001b[A\n",
      "Iteration:  49% 19/39 [00:47<00:49,  2.50s/it]\u001b[A\n",
      "Iteration:  51% 20/39 [00:50<00:47,  2.50s/it]\u001b[A\n",
      "Iteration:  54% 21/39 [00:52<00:44,  2.50s/it]\u001b[A\n",
      "Iteration:  56% 22/39 [00:55<00:42,  2.50s/it]\u001b[A\n",
      "Iteration:  59% 23/39 [00:57<00:39,  2.50s/it]\u001b[A\n",
      "Iteration:  62% 24/39 [01:00<00:37,  2.50s/it]\u001b[A\n",
      "Iteration:  64% 25/39 [01:02<00:34,  2.49s/it]\u001b[A\n",
      "Iteration:  67% 26/39 [01:05<00:32,  2.50s/it]\u001b[A\n",
      "Iteration:  69% 27/39 [01:07<00:29,  2.49s/it]\u001b[A\n",
      "Iteration:  72% 28/39 [01:10<00:27,  2.49s/it]\u001b[A\n",
      "Iteration:  74% 29/39 [01:12<00:24,  2.50s/it]\u001b[A\n",
      "Iteration:  77% 30/39 [01:15<00:22,  2.49s/it]\u001b[A\n",
      "Iteration:  79% 31/39 [01:17<00:19,  2.49s/it]\u001b[A\n",
      "Iteration:  82% 32/39 [01:20<00:17,  2.49s/it]\u001b[A\n",
      "Iteration:  85% 33/39 [01:22<00:14,  2.49s/it]\u001b[A\n",
      "Iteration:  87% 34/39 [01:25<00:12,  2.48s/it]\u001b[A\n",
      "Iteration:  90% 35/39 [01:27<00:09,  2.48s/it]\u001b[A\n",
      "Iteration:  92% 36/39 [01:30<00:07,  2.48s/it]\u001b[A\n",
      "Iteration:  95% 37/39 [01:32<00:04,  2.48s/it]\u001b[A\n",
      "Iteration:  97% 38/39 [01:34<00:02,  2.48s/it]\u001b[A\n",
      "Iteration: 100% 39/39 [01:37<00:00,  2.49s/it]\n",
      "Epoch:  33% 1/3 [01:37<03:14, 97.25s/it]\n",
      "Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   3% 1/39 [00:02<01:34,  2.48s/it]\u001b[A\n",
      "Iteration:   5% 2/39 [00:04<01:31,  2.49s/it]\u001b[A\n",
      "Iteration:   8% 3/39 [00:07<01:29,  2.49s/it]\u001b[A\n",
      "Iteration:  10% 4/39 [00:09<01:27,  2.49s/it]\u001b[A\n",
      "Iteration:  13% 5/39 [00:12<01:24,  2.49s/it]\u001b[A\n",
      "Iteration:  15% 6/39 [00:14<01:21,  2.48s/it]\u001b[A\n",
      "Iteration:  18% 7/39 [00:17<01:19,  2.48s/it]\u001b[A\n",
      "Iteration:  21% 8/39 [00:19<01:16,  2.48s/it]\u001b[A\n",
      "Iteration:  23% 9/39 [00:22<01:14,  2.48s/it]\u001b[A\n",
      "Iteration:  26% 10/39 [00:24<01:12,  2.49s/it]\u001b[A\n",
      "Iteration:  28% 11/39 [00:27<01:09,  2.48s/it]\u001b[A\n",
      "Iteration:  31% 12/39 [00:29<01:06,  2.48s/it]\u001b[A\n",
      "Iteration:  33% 13/39 [00:32<01:04,  2.48s/it]\u001b[A\n",
      "Iteration:  36% 14/39 [00:34<01:02,  2.49s/it]\u001b[A\n",
      "Iteration:  38% 15/39 [00:37<00:59,  2.49s/it]\u001b[A\n",
      "Iteration:  41% 16/39 [00:39<00:57,  2.49s/it]\u001b[A\n",
      "Iteration:  44% 17/39 [00:42<00:54,  2.49s/it]\u001b[A\n",
      "Iteration:  46% 18/39 [00:44<00:52,  2.49s/it]\u001b[A\n",
      "Iteration:  49% 19/39 [00:47<00:49,  2.49s/it]\u001b[A\n",
      "Iteration:  51% 20/39 [00:49<00:47,  2.48s/it]\u001b[A\n",
      "Iteration:  54% 21/39 [00:52<00:44,  2.49s/it]\u001b[A\n",
      "Iteration:  56% 22/39 [00:54<00:42,  2.49s/it]\u001b[A\n",
      "Iteration:  59% 23/39 [00:57<00:39,  2.49s/it]\u001b[A\n",
      "Iteration:  62% 24/39 [00:59<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  64% 25/39 [01:02<00:34,  2.48s/it]\u001b[A\n",
      "Iteration:  67% 26/39 [01:04<00:32,  2.48s/it]\u001b[A\n",
      "Iteration:  69% 27/39 [01:07<00:29,  2.48s/it]\u001b[A\n",
      "Iteration:  72% 28/39 [01:09<00:27,  2.49s/it]\u001b[A\n",
      "Iteration:  74% 29/39 [01:12<00:24,  2.49s/it]\u001b[A\n",
      "Iteration:  77% 30/39 [01:14<00:22,  2.49s/it]\u001b[A\n",
      "Iteration:  79% 31/39 [01:17<00:19,  2.49s/it]\u001b[A\n",
      "Iteration:  82% 32/39 [01:19<00:17,  2.49s/it]\u001b[A\n",
      "Iteration:  85% 33/39 [01:22<00:14,  2.49s/it]\u001b[A\n",
      "Iteration:  87% 34/39 [01:24<00:12,  2.49s/it]\u001b[A\n",
      "Iteration:  90% 35/39 [01:26<00:09,  2.48s/it]\u001b[A\n",
      "Iteration:  92% 36/39 [01:29<00:07,  2.48s/it]\u001b[A\n",
      "Iteration:  95% 37/39 [01:31<00:04,  2.49s/it]\u001b[A\n",
      "Iteration:  97% 38/39 [01:34<00:02,  2.49s/it]\u001b[A\n",
      "Iteration: 100% 39/39 [01:36<00:00,  2.48s/it]\n",
      "Epoch:  67% 2/3 [03:13<01:37, 97.10s/it]\n",
      "Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   3% 1/39 [00:02<01:34,  2.48s/it]\u001b[A\n",
      "Iteration:   5% 2/39 [00:04<01:32,  2.49s/it]\u001b[A\n",
      "Iteration:   8% 3/39 [00:07<01:29,  2.49s/it]\u001b[A\n",
      "Iteration:  10% 4/39 [00:09<01:27,  2.49s/it]\u001b[A\n",
      "Iteration:  13% 5/39 [00:12<01:24,  2.48s/it]\u001b[A\n",
      "Iteration:  15% 6/39 [00:14<01:21,  2.48s/it]\u001b[A\n",
      "Iteration:  18% 7/39 [00:17<01:19,  2.48s/it]\u001b[A\n",
      "Iteration:  21% 8/39 [00:19<01:17,  2.48s/it]\u001b[A\n",
      "Iteration:  23% 9/39 [00:22<01:14,  2.48s/it]\u001b[A\n",
      "Iteration:  26% 10/39 [00:24<01:12,  2.48s/it]\u001b[A\n",
      "Iteration:  28% 11/39 [00:27<01:09,  2.48s/it]\u001b[A\n",
      "Iteration:  31% 12/39 [00:29<01:07,  2.48s/it]\u001b[A\n",
      "Iteration:  33% 13/39 [00:32<01:04,  2.48s/it]\u001b[A\n",
      "Iteration:  36% 14/39 [00:34<01:02,  2.48s/it]\u001b[A\n",
      "Iteration:  38% 15/39 [00:37<00:59,  2.48s/it]\u001b[A\n",
      "Iteration:  41% 16/39 [00:39<00:57,  2.49s/it]\u001b[A\n",
      "Iteration:  44% 17/39 [00:42<00:54,  2.49s/it]\u001b[A\n",
      "Iteration:  46% 18/39 [00:44<00:52,  2.49s/it]\u001b[A\n",
      "Iteration:  49% 19/39 [00:47<00:49,  2.49s/it]\u001b[A\n",
      "Iteration:  51% 20/39 [00:49<00:47,  2.49s/it]\u001b[A\n",
      "Iteration:  54% 21/39 [00:52<00:44,  2.48s/it]\u001b[A\n",
      "Iteration:  56% 22/39 [00:54<00:42,  2.49s/it]\u001b[A\n",
      "Iteration:  59% 23/39 [00:57<00:39,  2.48s/it]\u001b[A\n",
      "Iteration:  62% 24/39 [00:59<00:37,  2.48s/it]\u001b[A\n",
      "Iteration:  64% 25/39 [01:02<00:34,  2.48s/it]\u001b[A\n",
      "Iteration:  67% 26/39 [01:04<00:32,  2.49s/it]\u001b[A\n",
      "Iteration:  69% 27/39 [01:07<00:29,  2.48s/it]\u001b[A\n",
      "Iteration:  72% 28/39 [01:09<00:27,  2.48s/it]\u001b[A\n",
      "Iteration:  74% 29/39 [01:12<00:24,  2.48s/it]\u001b[A\n",
      "Iteration:  77% 30/39 [01:14<00:22,  2.48s/it]\u001b[A\n",
      "Iteration:  79% 31/39 [01:17<00:19,  2.48s/it]\u001b[A\n",
      "Iteration:  82% 32/39 [01:19<00:17,  2.48s/it]\u001b[A\n",
      "Iteration:  85% 33/39 [01:21<00:14,  2.48s/it]\u001b[A\n",
      "Iteration:  87% 34/39 [01:24<00:12,  2.48s/it]\u001b[A\n",
      "Iteration:  90% 35/39 [01:26<00:09,  2.49s/it]\u001b[A\n",
      "Iteration:  92% 36/39 [01:29<00:07,  2.49s/it]\u001b[A\n",
      "Iteration:  95% 37/39 [01:31<00:04,  2.48s/it]\u001b[A\n",
      "Iteration:  97% 38/39 [01:34<00:02,  2.48s/it]\u001b[A\n",
      "Iteration: 100% 39/39 [01:36<00:00,  2.48s/it]\n",
      "Epoch: 100% 3/3 [04:50<00:00, 96.90s/it]\n",
      "05/05/2020 13:32:39 - INFO - __main__ -    global_step = 117, average loss = 0.6597126314782689\n",
      "05/05/2020 13:32:39 - INFO - __main__ -   Saving model checkpoint to ../\n",
      "05/05/2020 13:32:39 - INFO - transformers.configuration_utils -   Configuration saved in ../config.json\n",
      "05/05/2020 13:32:41 - INFO - transformers.modeling_utils -   Model weights saved in ../pytorch_model.bin\n",
      "05/05/2020 13:32:41 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
      "05/05/2020 13:32:41 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": \"rte\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/05/2020 13:32:41 - INFO - transformers.modeling_utils -   loading weights file ../pytorch_model.bin\n",
      "05/05/2020 13:32:44 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
      "05/05/2020 13:32:44 - INFO - transformers.configuration_utils -   Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": \"rte\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/05/2020 13:32:44 - INFO - transformers.tokenization_utils -   Model name '../' not found in model shortcut name list (google/electra-small-generator, google/electra-base-generator, google/electra-large-generator, google/electra-small-discriminator, google/electra-base-discriminator, google/electra-large-discriminator). Assuming '../' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "05/05/2020 13:32:44 - INFO - transformers.tokenization_utils -   Didn't find file ../added_tokens.json. We won't load it.\n",
      "05/05/2020 13:32:44 - INFO - transformers.tokenization_utils -   loading file ../vocab.txt\n",
      "05/05/2020 13:32:44 - INFO - transformers.tokenization_utils -   loading file None\n",
      "05/05/2020 13:32:44 - INFO - transformers.tokenization_utils -   loading file ../special_tokens_map.json\n",
      "05/05/2020 13:32:44 - INFO - transformers.tokenization_utils -   loading file ../tokenizer_config.json\n",
      "05/05/2020 13:32:45 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
      "05/05/2020 13:32:45 - INFO - transformers.configuration_utils -   Model config ElectraConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": \"rte\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/05/2020 13:32:45 - INFO - transformers.tokenization_utils -   Model name '../' not found in model shortcut name list (google/electra-small-generator, google/electra-base-generator, google/electra-large-generator, google/electra-small-discriminator, google/electra-base-discriminator, google/electra-large-discriminator). Assuming '../' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "05/05/2020 13:32:45 - INFO - transformers.tokenization_utils -   Didn't find file ../added_tokens.json. We won't load it.\n",
      "05/05/2020 13:32:45 - INFO - transformers.tokenization_utils -   loading file ../vocab.txt\n",
      "05/05/2020 13:32:45 - INFO - transformers.tokenization_utils -   loading file None\n",
      "05/05/2020 13:32:45 - INFO - transformers.tokenization_utils -   loading file ../special_tokens_map.json\n",
      "05/05/2020 13:32:45 - INFO - transformers.tokenization_utils -   loading file ../tokenizer_config.json\n",
      "05/05/2020 13:32:45 - INFO - __main__ -   Evaluate the following checkpoints: ['../']\n",
      "05/05/2020 13:32:45 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
      "05/05/2020 13:32:45 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"ElectraForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embedding_size\": 768,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": \"rte\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/05/2020 13:32:45 - INFO - transformers.modeling_utils -   loading weights file ../pytorch_model.bin\n",
      "05/05/2020 13:32:49 - INFO - __main__ -   Creating features from dataset file at data/glue/RTE\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   Writing example 0/277\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   input_ids: 101 100 100 1010 1996 7794 1997 1996 3364 100 100 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 100 100 100 1012 102 100 100 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   input_ids: 101 100 1010 2057 2085 2024 13648 2008 24479 2024 3974 2037 12353 2114 7355 1012 100 1011 4786 10327 2024 14163 29336 2075 5514 2084 2057 2064 2272 2039 2007 2047 24479 2000 2954 1996 2047 8358 1012 102 100 2003 3045 1996 2162 2114 24479 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   input_ids: 101 100 2003 2085 2188 2000 2070 2321 2454 2111 1011 1037 20934 28242 2075 2313 2008 7137 3155 2184 1010 2199 11000 1997 29132 2566 2154 1010 5128 2019 8216 10178 2006 2270 2578 1012 100 1996 2627 2184 2086 1010 1996 2231 2038 2699 2524 2000 8627 2797 5211 1999 1996 10214 4753 1010 2021 2070 10197 1018 1010 2199 11000 1997 5949 2003 2187 2369 2296 2154 1010 17037 7999 1999 1996 3684 2004 2009 18074 2005 2619 2000 3154 2009 2039 1012 100 2003 2411 1996 2111 1999 1996 3532 4355 27535 2008 2024 5409 5360 1012 100 1999 2070 2752 2027 2024 3554 2067 1012 100 100 1010 2028 1997 102 2321 2454 11000 1997 29132 2024 2550 3679 1999 100 1012 102\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   input_ids: 101 100 100 2451 1999 100 1010 2029 3616 2055 4583 1010 2199 1010 3268 2019 23226 9580 1010 18454 23500 3070 10660 9849 2066 6451 1998 19207 1012 100 2116 2360 2037 16021 7934 9580 3957 2068 1037 3168 2008 2027 2024 5123 2013 1996 4808 1997 100 2554 1012 100 2004 3901 5935 2379 1996 2082 1010 2070 4147 3151 11721 15185 1998 7194 1999 3586 1011 4567 11829 17252 1010 2027 2056 2008 3168 1997 3808 2018 2042 10909 1012 1000 100 2619 20057 1998 4122 2000 2079 2242 5236 1010 2045 1005 1055 2053 3292 2008 1005 1055 2183 2000 2644 2068 1010 1000 2056 100 100 1010 5179 1010 102 100 2038 1996 5221 100 2451 1999 1996 100 1012 100 1012 102\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   input_ids: 101 100 2749 2020 2006 2152 9499 2044 2019 2602 3049 1999 2029 2062 2084 1015 1010 2199 2111 1010 2164 2698 2602 5347 1010 2031 2042 2730 1012 102 100 2749 2020 2006 2152 9499 2044 1037 3049 24563 2011 4808 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/05/2020 13:32:49 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n",
      "05/05/2020 13:32:49 - INFO - __main__ -   Saving features into cached file data/glue/RTE/cached_dev_electra-base-discriminator_128_rte\n",
      "05/05/2020 13:32:49 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "05/05/2020 13:32:49 - INFO - __main__ -     Num examples = 277\n",
      "05/05/2020 13:32:49 - INFO - __main__ -     Batch size = 64\n",
      "Evaluating: 100% 5/5 [00:04<00:00,  1.23it/s]\n",
      "05/05/2020 13:32:53 - INFO - __main__ -   ***** Eval results  *****\n",
      "05/05/2020 13:32:53 - INFO - __main__ -     acc = 0.5595667870036101\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_2xmVAZiqiCK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}