{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone transformers repo and checkout version 2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeewon\\Desktop\\NYU\\DSGA1012\\transformers\n"
     ]
    }
   ],
   "source": [
    "cd transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HEAD is now at 6f5a12a5 Release: v2.7.0\n"
     ]
    }
   ],
   "source": [
    "!git checkout v2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages required for running run_glue.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from -r ./examples/requirements.txt (line 1)) (2.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from -r ./examples/requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from -r ./examples/requirements.txt (line 3)) (0.20.1)\n",
      "Requirement already satisfied: seqeval in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from -r ./examples/requirements.txt (line 4)) (0.0.12)\n",
      "Requirement already satisfied: psutil in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from -r ./examples/requirements.txt (line 5)) (5.7.0)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-1.4.6-py3-none-any.whl (59 kB)\n",
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.0.3-py3-none-any.whl (25 kB)\n",
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-2.1.0-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.15.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.11.3)\n",
      "Requirement already satisfied: six in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.28.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (46.1.3.post20200330)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\Jeewon\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Jeewon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4nmq_jrf\\\\mecab-python3\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Jeewon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4nmq_jrf\\\\mecab-python3\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\Jeewon\\AppData\\Local\\Temp\\pip-wheel-cynbb0y0'\n",
      "       cwd: C:\\Users\\Jeewon\\AppData\\Local\\Temp\\pip-install-4nmq_jrf\\mecab-python3\\\n",
      "  Complete output (9 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.7\n",
      "  creating build\\lib.win-amd64-3.7\\MeCab\n",
      "  copying src\\MeCab\\__init__.py -> build\\lib.win-amd64-3.7\\MeCab\n",
      "  running build_ext\n",
      "  error: [WinError 2] The system cannot find the file specified\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for mecab-python3\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\Jeewon\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Jeewon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4nmq_jrf\\\\mecab-python3\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Jeewon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4nmq_jrf\\\\mecab-python3\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Jeewon\\AppData\\Local\\Temp\\pip-record-6vadascp\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\Jeewon\\Anaconda3\\Include\\mecab-python3'\n",
      "         cwd: C:\\Users\\Jeewon\\AppData\\Local\\Temp\\pip-install-4nmq_jrf\\mecab-python3\\\n",
      "    Complete output (9 lines):\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib.win-amd64-3.7\n",
      "    creating build\\lib.win-amd64-3.7\\MeCab\n",
      "    copying src\\MeCab\\__init__.py -> build\\lib.win-amd64-3.7\\MeCab\n",
      "    running build_ext\n",
      "    error: [WinError 2] The system cannot find the file specified\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\Jeewon\\Anaconda3\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\Jeewon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4nmq_jrf\\\\mecab-python3\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\Jeewon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4nmq_jrf\\\\mecab-python3\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\Jeewon\\AppData\\Local\\Temp\\pip-record-6vadascp\\install-record.txt' --single-version-externally-managed --compile --install-headers 'C:\\Users\\Jeewon\\Anaconda3\\Include\\mecab-python3' Check the logs for full command output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.6.0.post2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: Keras>=2.2.4 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.3.1)\n",
      "Requirement already satisfied: portalocker in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (1.6.0)\n",
      "Collecting mecab-python3\n",
      "  Using cached mecab-python3-0.996.5.tar.gz (65 kB)\n",
      "Requirement already satisfied: typing in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (3.7.4.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.4.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (4.44.1)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (19.3.0)\n",
      "Processing c:\\users\\jeewon\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\\termcolor-1.1.0-py3-none-any.whl\n",
      "Processing c:\\users\\jeewon\\appdata\\local\\pip\\cache\\wheels\\a4\\61\\fd\\c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\\dill-0.3.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: wrapt in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.12.1)\n",
      "Processing c:\\users\\jeewon\\appdata\\local\\pip\\cache\\wheels\\29\\93\\c6\\762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\\promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: future in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.18.2)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-0.21.1-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.25.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (5.3.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.10.0)\n",
      "Requirement already satisfied: pywin32!=226; platform_system == \"Windows\" in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from portalocker->sacrebleu->-r ./examples/requirements.txt (line 6)) (227)\n",
      "Processing c:\\users\\jeewon\\appdata\\local\\pip\\cache\\wheels\\4c\\a1\\71\\5e427276ceeff277fd76878d1b19fbf4587a2845015d86864b\\googleapis_common_protos-1.51.0-py3-none-any.whl\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.0)\n",
      "Building wheels for collected packages: mecab-python3\n",
      "  Building wheel for mecab-python3 (setup.py): started\n",
      "  Building wheel for mecab-python3 (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for mecab-python3\n",
      "Failed to build mecab-python3\n",
      "Installing collected packages: mecab-python3, sacrebleu, rouge-score, termcolor, dill, promise, googleapis-common-protos, tensorflow-metadata, tensorflow-datasets\n",
      "    Running setup.py install for mecab-python3: started\n",
      "    Running setup.py install for mecab-python3: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./examples/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (1.12.36)\n",
      "Requirement already satisfied: filelock in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (2.23.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (4.44.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (0.1.85)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (0.0.38)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from boto3) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from boto3) (0.9.5)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.36 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from boto3) (1.15.36)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: joblib in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from sacremoses) (0.14.1)\n",
      "Requirement already satisfied: click in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from sacremoses) (7.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from sacremoses) (1.14.0)\n",
      "Requirement already satisfied: regex in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from sacremoses) (2020.1.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.36->boto3) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\jeewon\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.36->boto3) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 filelock requests tqdm sentencepiece sacremoses tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download GLUE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GLUE_DIR = os.path.join(os.getcwd(), \"content\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeewon\\Desktop\\NYU\\DSGA1012\\transformers\\content\\data\n"
     ]
    }
   ],
   "source": [
    "!echo $GLUE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Jeewon\\\\Desktop\\\\NYU\\\\DSGA1012\\\\transformers'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: download_glue_data.py [-h] [--data_dir DATA_DIR] [--tasks TASKS]\n",
      "                             [--path_to_mrpc PATH_TO_MRPC]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --data_dir DATA_DIR   directory to save data to\n",
      "  --tasks TASKS         tasks to download data for as a comma separated string\n",
      "  --path_to_mrpc PATH_TO_MRPC\n",
      "                        path to directory containing extracted MRPC data,\n",
      "                        msr_paraphrase_train.txt and msr_paraphrase_text.txt\n"
     ]
    }
   ],
   "source": [
    "!python ./utils/download_glue_data.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting WNLI...\n",
      "\tCompleted!\n"
     ]
    }
   ],
   "source": [
    "!python ./utils/download_glue_data.py --data_dir $GLUE_DIR --tasks WNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute RoBERTa score on WNLI task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME=\"WNLI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WNLI\n"
     ]
    }
   ],
   "source": [
    "!echo $TASK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_glue.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n",
      "                   --model_name_or_path MODEL_NAME_OR_PATH --task_name\n",
      "                   TASK_NAME --output_dir OUTPUT_DIR\n",
      "                   [--config_name CONFIG_NAME]\n",
      "                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
      "                   [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n",
      "                   [--evaluate_during_training] [--do_lower_case]\n",
      "                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                   [--learning_rate LEARNING_RATE]\n",
      "                   [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
      "                   [--max_grad_norm MAX_GRAD_NORM]\n",
      "                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                   [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
      "                   [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
      "                   [--eval_all_checkpoints] [--no_cuda]\n",
      "                   [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n",
      "                   [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                   [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n",
      "                   [--server_port SERVER_PORT]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n",
      "                        other data files) for the task.\n",
      "  --model_type MODEL_TYPE\n",
      "                        Model type selected in the list: distilbert, albert,\n",
      "                        camembert, xlm-roberta, bart, roberta, bert, xlnet,\n",
      "                        flaubert, xlm\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\n",
      "                        Path to pre-trained model or shortcut name selected in\n",
      "                        the list: distilbert-base-uncased, distilbert-base-\n",
      "                        uncased-distilled-squad, distilbert-base-cased,\n",
      "                        distilbert-base-cased-distilled-squad, distilbert-\n",
      "                        base-german-cased, distilbert-base-multilingual-cased,\n",
      "                        distilbert-base-uncased-finetuned-sst-2-english,\n",
      "                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\n",
      "                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\n",
      "                        albert-xlarge-v2, albert-xxlarge-v2, camembert-base,\n",
      "                        umberto-commoncrawl-cased-v1, umberto-wikipedia-\n",
      "                        uncased-v1, xlm-roberta-base, xlm-roberta-large, xlm-\n",
      "                        roberta-large-finetuned-conll02-dutch, xlm-roberta-\n",
      "                        large-finetuned-conll02-spanish, xlm-roberta-large-\n",
      "                        finetuned-conll03-english, xlm-roberta-large-\n",
      "                        finetuned-conll03-german, bart-large, bart-large-mnli,\n",
      "                        bart-large-cnn, bart-large-xsum, roberta-base,\n",
      "                        roberta-large, roberta-large-mnli, distilroberta-base,\n",
      "                        roberta-base-openai-detector, roberta-large-openai-\n",
      "                        detector, bert-base-uncased, bert-large-uncased, bert-\n",
      "                        base-cased, bert-large-cased, bert-base-multilingual-\n",
      "                        uncased, bert-base-multilingual-cased, bert-base-\n",
      "                        chinese, bert-base-german-cased, bert-large-uncased-\n",
      "                        whole-word-masking, bert-large-cased-whole-word-\n",
      "                        masking, bert-large-uncased-whole-word-masking-\n",
      "                        finetuned-squad, bert-large-cased-whole-word-masking-\n",
      "                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\n",
      "                        base-german-dbmdz-cased, bert-base-german-dbmdz-\n",
      "                        uncased, bert-base-japanese, bert-base-japanese-whole-\n",
      "                        word-masking, bert-base-japanese-char, bert-base-\n",
      "                        japanese-char-whole-word-masking, bert-base-finnish-\n",
      "                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\n",
      "                        dutch-cased, xlnet-base-cased, xlnet-large-cased,\n",
      "                        flaubert-small-cased, flaubert-base-uncased, flaubert-\n",
      "                        base-cased, flaubert-large-cased, xlm-mlm-en-2048,\n",
      "                        xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-\n",
      "                        enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\n",
      "                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\n",
      "                        xlm-mlm-17-1280, xlm-mlm-100-1280\n",
      "  --task_name TASK_NAME\n",
      "                        The name of the task to train selected in the list:\n",
      "                        cola, mnli, mnli-mm, mrpc, sst-2, sts-b, qqp, qnli,\n",
      "                        rte, wnli\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        The output directory where the model predictions and\n",
      "                        checkpoints will be written.\n",
      "  --config_name CONFIG_NAME\n",
      "                        Pretrained config name or path if not the same as\n",
      "                        model_name\n",
      "  --tokenizer_name TOKENIZER_NAME\n",
      "                        Pretrained tokenizer name or path if not the same as\n",
      "                        model_name\n",
      "  --cache_dir CACHE_DIR\n",
      "                        Where do you want to store the pre-trained models\n",
      "                        downloaded from s3\n",
      "  --max_seq_length MAX_SEQ_LENGTH\n",
      "                        The maximum total input sequence length after\n",
      "                        tokenization. Sequences longer than this will be\n",
      "                        truncated, sequences shorter will be padded.\n",
      "  --do_train            Whether to run training.\n",
      "  --do_eval             Whether to run eval on the dev set.\n",
      "  --evaluate_during_training\n",
      "                        Run evaluation during training at each logging step.\n",
      "  --do_lower_case       Set this flag if you are using an uncased model.\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
      "                        Batch size per GPU/CPU for training.\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
      "                        Batch size per GPU/CPU for evaluation.\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
      "                        Number of updates steps to accumulate before\n",
      "                        performing a backward/update pass.\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        The initial learning rate for Adam.\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "                        Weight decay if we apply some.\n",
      "  --adam_epsilon ADAM_EPSILON\n",
      "                        Epsilon for Adam optimizer.\n",
      "  --max_grad_norm MAX_GRAD_NORM\n",
      "                        Max gradient norm.\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
      "                        Total number of training epochs to perform.\n",
      "  --max_steps MAX_STEPS\n",
      "                        If > 0: set total number of training steps to perform.\n",
      "                        Override num_train_epochs.\n",
      "  --warmup_steps WARMUP_STEPS\n",
      "                        Linear warmup over warmup_steps.\n",
      "  --logging_steps LOGGING_STEPS\n",
      "                        Log every X updates steps.\n",
      "  --save_steps SAVE_STEPS\n",
      "                        Save checkpoint every X updates steps.\n",
      "  --eval_all_checkpoints\n",
      "                        Evaluate all checkpoints starting with the same prefix\n",
      "                        as model_name ending and ending with step number\n",
      "  --no_cuda             Avoid using CUDA when available\n",
      "  --overwrite_output_dir\n",
      "                        Overwrite the content of the output directory\n",
      "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
      "  --seed SEED           random seed for initialization\n",
      "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
      "                        NVIDIA apex) instead of 32-bit\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\n",
      "                        For fp16: Apex AMP optimization level selected in\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
      "                        https://nvidia.github.io/apex/amp.html\n",
      "  --local_rank LOCAL_RANK\n",
      "                        For distributed training: local_rank\n",
      "  --server_ip SERVER_IP\n",
      "                        For distant debugging.\n",
      "  --server_port SERVER_PORT\n",
      "                        For distant debugging.\n"
     ]
    }
   ],
   "source": [
    "!python ./examples/run_glue.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/03/2020 21:57:27 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "04/03/2020 21:57:27 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at C:\\Users\\Jeewon\\.cache\\torch\\transformers\\e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "04/03/2020 21:57:27 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"wnli\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 21:57:27 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at C:\\Users\\Jeewon\\.cache\\torch\\transformers\\e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "04/03/2020 21:57:27 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 21:57:28 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at C:\\Users\\Jeewon\\.cache\\torch\\transformers\\d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "04/03/2020 21:57:28 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at C:\\Users\\Jeewon\\.cache\\torch\\transformers\\b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "04/03/2020 21:57:28 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at C:\\Users\\Jeewon\\.cache\\torch\\transformers\\228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "04/03/2020 21:57:33 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "04/03/2020 21:57:33 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "04/03/2020 21:57:33 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='C:\\\\Users\\\\Jeewon\\\\Desktop\\\\NYU\\\\DSGA1012\\\\transformers\\\\content\\\\data/WNLI', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=0, no_cuda=False, num_train_epochs=3.0, output_dir='../$WNLI_run', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='wnli', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
      "04/03/2020 21:57:33 - INFO - __main__ -   Creating features from dataset file at C:\\Users\\Jeewon\\Desktop\\NYU\\DSGA1012\\transformers\\content\\data/WNLI\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   Writing example 0/635\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   guid: train-0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   input_ids: 0 38 4889 10 7756 149 10 33129 4 520 38 2468 5 7756 66 6 24 56 10 4683 4 2 2 20 33129 56 10 4683 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   guid: train-1\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   input_ids: 0 610 1705 75 192 5 1289 19 7835 11 760 9 123 142 37 16 98 765 4 2 2 610 16 98 765 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   guid: train-2\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   input_ids: 0 20 249 1128 70 9 5 5188 453 4 252 58 667 7 912 5 1262 721 11 5 3757 4 2 2 20 249 58 667 7 912 5 1262 721 11 5 3757 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   guid: train-3\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   input_ids: 0 2206 3905 7734 18 1246 11 960 4 91 16882 123 13481 4 2 2 2206 16882 123 13481 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   guid: train-4\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   input_ids: 0 520 17734 219 1113 1348 5 11657 6 69 985 21 8416 4 264 21 7316 45 7 32366 69 6 2432 9828 8 10907 124 88 69 16471 4 2 2 985 21 7316 45 7 32366 69 6 2432 9828 8 10907 124 88 69 16471 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:57:33 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "04/03/2020 21:57:33 - INFO - __main__ -   Saving features into cached file C:\\Users\\Jeewon\\Desktop\\NYU\\DSGA1012\\transformers\\content\\data/WNLI\\cached_train_roberta-base_128_wnli\n",
      "04/03/2020 21:57:34 - INFO - __main__ -   ***** Running training *****\n",
      "04/03/2020 21:57:34 - INFO - __main__ -     Num examples = 635\n",
      "04/03/2020 21:57:34 - INFO - __main__ -     Num Epochs = 3\n",
      "04/03/2020 21:57:34 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n",
      "04/03/2020 21:57:34 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/03/2020 21:57:34 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "04/03/2020 21:57:34 - INFO - __main__ -     Total optimization steps = 30\n",
      "\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "\n",
      "Iteration:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Iteration:  10%|#         | 1/10 [01:01<09:14, 61.57s/it]\u001b[A\n",
      "\n",
      "Iteration:  20%|##        | 2/10 [01:58<08:02, 60.27s/it]\u001b[A\n",
      "\n",
      "Iteration:  30%|###       | 3/10 [02:57<06:58, 59.84s/it]\u001b[A\n",
      "\n",
      "Iteration:  40%|####      | 4/10 [04:01<06:06, 61.16s/it]\u001b[A\n",
      "\n",
      "Iteration:  50%|#####     | 5/10 [05:17<05:27, 65.54s/it]\u001b[A\n",
      "\n",
      "Iteration:  60%|######    | 6/10 [07:10<05:18, 79.62s/it]\u001b[A\n",
      "\n",
      "Iteration:  70%|#######   | 7/10 [08:52<04:19, 86.43s/it]\u001b[A\n",
      "\n",
      "Iteration:  80%|########  | 8/10 [10:32<03:01, 90.54s/it]\u001b[A\n",
      "\n",
      "Iteration:  90%|######### | 9/10 [12:09<01:32, 92.31s/it]\u001b[A\n",
      "\n",
      "Iteration: 100%|##########| 10/10 [13:43<00:00, 93.06s/it]\u001b[A\n",
      "Iteration: 100%|##########| 10/10 [13:43<00:00, 82.39s/it]\n",
      "\n",
      "Epoch:  33%|###3      | 1/3 [13:43<27:27, 823.86s/it]\n",
      "\n",
      "Iteration:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Iteration:  10%|#         | 1/10 [01:59<17:51, 119.09s/it]\u001b[A\n",
      "\n",
      "Iteration:  20%|##        | 2/10 [03:53<15:42, 117.82s/it]\u001b[A\n",
      "\n",
      "Iteration:  30%|###       | 3/10 [05:50<13:41, 117.41s/it]\u001b[A\n",
      "\n",
      "Iteration:  40%|####      | 4/10 [07:37<11:25, 114.32s/it]\u001b[A\n",
      "\n",
      "Iteration:  50%|#####     | 5/10 [09:32<09:32, 114.55s/it]\u001b[A\n",
      "\n",
      "Iteration:  60%|######    | 6/10 [11:15<07:24, 111.17s/it]\u001b[A\n",
      "\n",
      "Iteration:  70%|#######   | 7/10 [13:02<05:29, 109.95s/it]\u001b[A\n",
      "\n",
      "Iteration:  80%|########  | 8/10 [14:58<03:43, 111.51s/it]\u001b[A\n",
      "\n",
      "Iteration:  90%|######### | 9/10 [16:44<01:50, 110.06s/it]\u001b[A\n",
      "\n",
      "Iteration: 100%|##########| 10/10 [18:18<00:00, 105.12s/it]\u001b[A\n",
      "Iteration: 100%|##########| 10/10 [18:18<00:00, 109.84s/it]\n",
      "\n",
      "Epoch:  67%|######6   | 2/3 [32:02<15:06, 906.23s/it]\n",
      "\n",
      "Iteration:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Iteration:  10%|#         | 1/10 [01:36<14:27, 96.36s/it]\u001b[A\n",
      "\n",
      "Iteration:  20%|##        | 2/10 [03:07<12:39, 94.92s/it]\u001b[A\n",
      "\n",
      "Iteration:  30%|###       | 3/10 [04:37<10:53, 93.31s/it]\u001b[A\n",
      "\n",
      "Iteration:  40%|####      | 4/10 [06:08<09:14, 92.49s/it]\u001b[A\n",
      "\n",
      "Iteration:  50%|#####     | 5/10 [07:40<07:42, 92.49s/it]\u001b[A\n",
      "\n",
      "Iteration:  60%|######    | 6/10 [09:08<06:04, 91.21s/it]\u001b[A\n",
      "\n",
      "Iteration:  70%|#######   | 7/10 [10:39<04:33, 91.01s/it]\u001b[A\n",
      "\n",
      "Iteration:  80%|########  | 8/10 [12:09<03:01, 90.81s/it]\u001b[A\n",
      "\n",
      "Iteration:  90%|######### | 9/10 [13:36<01:29, 89.72s/it]\u001b[A\n",
      "\n",
      "Iteration: 100%|##########| 10/10 [14:58<00:00, 87.22s/it]\u001b[A\n",
      "Iteration: 100%|##########| 10/10 [14:58<00:00, 89.82s/it]\n",
      "\n",
      "Epoch: 100%|##########| 3/3 [47:00<00:00, 903.84s/it]\n",
      "Epoch: 100%|##########| 3/3 [47:00<00:00, 940.19s/it]\n",
      "04/03/2020 22:44:34 - INFO - __main__ -    global_step = 30, average loss = 0.6944953958193462\n",
      "04/03/2020 22:44:34 - INFO - __main__ -   Saving model checkpoint to ../$WNLI_run\n",
      "04/03/2020 22:44:34 - INFO - transformers.configuration_utils -   Configuration saved in ../$WNLI_run\\config.json\n",
      "04/03/2020 22:44:35 - INFO - transformers.modeling_utils -   Model weights saved in ../$WNLI_run\\pytorch_model.bin\n",
      "04/03/2020 22:44:36 - INFO - transformers.configuration_utils -   loading configuration file ../$WNLI_run\\config.json\n",
      "04/03/2020 22:44:36 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"wnli\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 22:44:36 - INFO - transformers.modeling_utils -   loading weights file ../$WNLI_run\\pytorch_model.bin\n",
      "04/03/2020 22:44:44 - INFO - transformers.configuration_utils -   loading configuration file ../$WNLI_run\\config.json\n",
      "04/03/2020 22:44:44 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"wnli\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 22:44:44 - INFO - transformers.tokenization_utils -   Model name '../$WNLI_run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../$WNLI_run' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "04/03/2020 22:44:44 - INFO - transformers.tokenization_utils -   Didn't find file ../$WNLI_run\\added_tokens.json. We won't load it.\n",
      "04/03/2020 22:44:44 - INFO - transformers.tokenization_utils -   loading file ../$WNLI_run\\vocab.json\n",
      "04/03/2020 22:44:44 - INFO - transformers.tokenization_utils -   loading file ../$WNLI_run\\merges.txt\n",
      "04/03/2020 22:44:44 - INFO - transformers.tokenization_utils -   loading file None\n",
      "04/03/2020 22:44:44 - INFO - transformers.tokenization_utils -   loading file ../$WNLI_run\\special_tokens_map.json\n",
      "04/03/2020 22:44:44 - INFO - transformers.tokenization_utils -   loading file ../$WNLI_run\\tokenizer_config.json\n",
      "04/03/2020 22:44:45 - INFO - transformers.configuration_utils -   loading configuration file ../$WNLI_run\\config.json\n",
      "04/03/2020 22:44:45 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"wnli\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 22:44:45 - INFO - transformers.tokenization_utils -   Model name '../$WNLI_run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../$WNLI_run' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "04/03/2020 22:44:45 - INFO - transformers.tokenization_utils -   Didn't find file ../$WNLI_run\\added_tokens.json. We won't load it.\n",
      "04/03/2020 22:44:45 - INFO - transformers.tokenization_utils -   loading file ../$WNLI_run\\vocab.json\n",
      "04/03/2020 22:44:45 - INFO - transformers.tokenization_utils -   loading file ../$WNLI_run\\merges.txt\n",
      "04/03/2020 22:44:45 - INFO - transformers.tokenization_utils -   loading file None\n",
      "04/03/2020 22:44:45 - INFO - transformers.tokenization_utils -   loading file ../$WNLI_run\\special_tokens_map.json\n",
      "04/03/2020 22:44:45 - INFO - transformers.tokenization_utils -   loading file ../$WNLI_run\\tokenizer_config.json\n",
      "04/03/2020 22:44:45 - INFO - __main__ -   Evaluate the following checkpoints: ['../$WNLI_run']\n",
      "04/03/2020 22:44:45 - INFO - transformers.configuration_utils -   loading configuration file ../$WNLI_run\\config.json\n",
      "04/03/2020 22:44:45 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"wnli\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 22:44:45 - INFO - transformers.modeling_utils -   loading weights file ../$WNLI_run\\pytorch_model.bin\n",
      "04/03/2020 22:44:53 - INFO - __main__ -   Creating features from dataset file at C:\\Users\\Jeewon\\Desktop\\NYU\\DSGA1012\\transformers\\content\\data/WNLI\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   Writing example 0/71\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   input_ids: 0 20 15160 16 3741 29519 19 2549 4 85 34 7 28 17317 4 2 2 20 2549 34 7 28 17317 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   input_ids: 0 7343 6536 15 6470 18 1883 53 79 222 45 1948 4 2 2 6470 222 45 1948 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   input_ids: 0 10472 399 75 120 5800 19 15282 6 54 56 847 69 160 6 142 79 2294 8 11590 7 2724 4 2 2 15282 2294 8 11590 7 2724 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   input_ids: 0 440 65 9447 622 7 28 5074 8 20100 4 125 10 92 892 31 5 589 9 4222 19902 1655 5839 10648 14 14 18 2230 141 24 817 201 619 4 2 2 280 18 2230 141 622 817 201 619 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   input_ids: 0 20 313 1705 75 5258 39 979 142 37 21 98 2016 4 2 2 20 979 21 98 2016 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 22:44:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 22:44:53 - INFO - __main__ -   Saving features into cached file C:\\Users\\Jeewon\\Desktop\\NYU\\DSGA1012\\transformers\\content\\data/WNLI\\cached_dev_roberta-base_128_wnli\n",
      "04/03/2020 22:44:53 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "04/03/2020 22:44:53 - INFO - __main__ -     Num examples = 71\n",
      "04/03/2020 22:44:53 - INFO - __main__ -     Batch size = 64\n",
      "\n",
      "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Evaluating:  50%|#####     | 1/2 [00:23<00:23, 23.04s/it]\n",
      "Evaluating: 100%|##########| 2/2 [00:25<00:00, 16.95s/it]\n",
      "Evaluating: 100%|##########| 2/2 [00:25<00:00, 12.89s/it]\n",
      "04/03/2020 22:45:19 - INFO - __main__ -   ***** Eval results  *****\n",
      "04/03/2020 22:45:19 - INFO - __main__ -     acc = 0.4225352112676056\n"
     ]
    }
   ],
   "source": [
    "!python ./examples/run_glue.py \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_gpu_eval_batch_size=64   \\\n",
    "    --per_gpu_train_batch_size=64   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --output_dir ../${TASK_NAME}_run \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
