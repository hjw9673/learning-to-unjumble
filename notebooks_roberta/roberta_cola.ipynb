{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone transformers repo and checkout version 2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 101, done.\u001b[K\n",
      "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
      "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
      "remote: Total 23479 (delta 46), reused 42 (delta 16), pack-reused 23378\u001b[K\n",
      "\u001b[KReceiving objects: 100% (23479/23479), 13.57 MiB | 4.49 MiB/s, done.\n",
      "\u001b[KResolving deltas: 100% (16765/16765), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf transformers\n",
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anjaliagrawal/content/transformers\n"
     ]
    }
   ],
   "source": [
    "cd transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: switching to 'v2.7.0'.\r\n",
      "\r\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\r\n",
      "changes and commit them, and you can discard any commits you make in this\r\n",
      "state without impacting any branches by switching back to a branch.\r\n",
      "\r\n",
      "If you want to create a new branch to retain commits you create, you may\r\n",
      "do so (now or later) by using -c with the switch command. Example:\r\n",
      "\r\n",
      "  git switch -c <new-branch-name>\r\n",
      "\r\n",
      "Or undo this operation with:\r\n",
      "\r\n",
      "  git switch -\r\n",
      "\r\n",
      "Turn off this advice by setting config variable advice.detachedHead to false\r\n",
      "\r\n",
      "HEAD is now at 6f5a12a5 Release: v2.7.0\r\n"
     ]
    }
   ],
   "source": [
    "!git checkout v2.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages required for running run_glue.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /anaconda3/lib/python3.7/site-packages (from -r ./examples/requirements.txt (line 1)) (2.0)\n",
      "Requirement already satisfied: tensorboard in /anaconda3/lib/python3.7/site-packages (from -r ./examples/requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn in /anaconda3/lib/python3.7/site-packages (from -r ./examples/requirements.txt (line 3)) (0.21.2)\n",
      "Requirement already satisfied: seqeval in /anaconda3/lib/python3.7/site-packages (from -r ./examples/requirements.txt (line 4)) (0.0.12)\n",
      "Requirement already satisfied: psutil in /anaconda3/lib/python3.7/site-packages (from -r ./examples/requirements.txt (line 5)) (5.7.0)\n",
      "Requirement already satisfied: sacrebleu in /anaconda3/lib/python3.7/site-packages (from -r ./examples/requirements.txt (line 6)) (1.4.6)\n",
      "Requirement already satisfied: rouge-score in /anaconda3/lib/python3.7/site-packages (from -r ./examples/requirements.txt (line 7)) (0.0.3)\n",
      "Requirement already satisfied: tensorflow_datasets in /anaconda3/lib/python3.7/site-packages (from -r ./examples/requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.16.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /anaconda3/lib/python3.7/site-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.11.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /anaconda3/lib/python3.7/site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /anaconda3/lib/python3.7/site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (46.0.0.post20200309)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /anaconda3/lib/python3.7/site-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.16.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /anaconda3/lib/python3.7/site-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (0.14.1)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /anaconda3/lib/python3.7/site-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.2.5)\n",
      "Requirement already satisfied: mecab-python3 in /anaconda3/lib/python3.7/site-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (0.996.5)\n",
      "Requirement already satisfied: typing in /anaconda3/lib/python3.7/site-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (3.7.4.1)\n",
      "Requirement already satisfied: portalocker in /anaconda3/lib/python3.7/site-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.7/site-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.4.5)\n",
      "Requirement already satisfied: future in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.18.2)\n",
      "Requirement already satisfied: termcolor in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: promise in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.3)\n",
      "Requirement already satisfied: dill in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.3.1.1)\n",
      "Requirement already satisfied: tensorflow-metadata in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.21.1)\n",
      "Requirement already satisfied: wrapt in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.12.1)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (19.3.0)\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (4.43.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.23.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /anaconda3/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (5.3)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.10.0)\n",
      "Requirement already satisfied: googleapis-common-protos in /anaconda3/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.51.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.25.8)\n",
      "Requirement already satisfied: boto3 in /anaconda3/lib/python3.7/site-packages (1.12.36)\n",
      "Requirement already satisfied: filelock in /anaconda3/lib/python3.7/site-packages (3.0.12)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (2.23.0)\n",
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.7/site-packages (4.43.0)\n",
      "Requirement already satisfied: sentencepiece in /anaconda3/lib/python3.7/site-packages (0.1.85)\n",
      "Requirement already satisfied: sacremoses in /anaconda3/lib/python3.7/site-packages (0.0.38)\n",
      "Requirement already satisfied: tokenizers in /anaconda3/lib/python3.7/site-packages (0.6.0)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.36 in /anaconda3/lib/python3.7/site-packages (from boto3) (1.15.36)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.7/site-packages (from boto3) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /anaconda3/lib/python3.7/site-packages (from boto3) (0.3.3)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from sacremoses) (1.14.0)\n",
      "Requirement already satisfied: joblib in /anaconda3/lib/python3.7/site-packages (from sacremoses) (0.14.1)\n",
      "Requirement already satisfied: click in /anaconda3/lib/python3.7/site-packages (from sacremoses) (7.1.1)\n",
      "Requirement already satisfied: regex in /anaconda3/lib/python3.7/site-packages (from sacremoses) (2020.2.20)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.36->boto3) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.36->boto3) (2.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./examples/requirements.txt\n",
    "!pip install boto3 filelock requests tqdm sentencepiece sacremoses tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download GLUE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anjaliagrawal/content/\r\n"
     ]
    }
   ],
   "source": [
    "GLUE_DIR=\"/Users/anjaliagrawal/content/\"\n",
    "!echo $GLUE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/anjaliagrawal/content/transformers'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting CoLA...\n",
      "\tCompleted!\n"
     ]
    }
   ],
   "source": [
    "!python ./utils/download_glue_data.py --data_dir $GLUE_DIR --tasks CoLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute RoBERTa score on CoLA task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME=\"CoLA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=/Users/anjaliagrawal/content/transformers/src:/\n"
     ]
    }
   ],
   "source": [
    "%set_env PYTHONPATH=/Users/anjaliagrawal/content/transformers/src:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_glue.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\r\n",
      "                   --model_name_or_path MODEL_NAME_OR_PATH --task_name\r\n",
      "                   TASK_NAME --output_dir OUTPUT_DIR\r\n",
      "                   [--config_name CONFIG_NAME]\r\n",
      "                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\r\n",
      "                   [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\r\n",
      "                   [--evaluate_during_training] [--do_lower_case]\r\n",
      "                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\r\n",
      "                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\r\n",
      "                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\r\n",
      "                   [--learning_rate LEARNING_RATE]\r\n",
      "                   [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\r\n",
      "                   [--max_grad_norm MAX_GRAD_NORM]\r\n",
      "                   [--num_train_epochs NUM_TRAIN_EPOCHS]\r\n",
      "                   [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\r\n",
      "                   [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\r\n",
      "                   [--eval_all_checkpoints] [--no_cuda]\r\n",
      "                   [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\r\n",
      "                   [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\r\n",
      "                   [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\r\n",
      "                   [--server_port SERVER_PORT]\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\r\n",
      "                        other data files) for the task.\r\n",
      "  --model_type MODEL_TYPE\r\n",
      "                        Model type selected in the list: distilbert, albert,\r\n",
      "                        camembert, xlm-roberta, bart, roberta, bert, xlnet,\r\n",
      "                        flaubert, xlm\r\n",
      "  --model_name_or_path MODEL_NAME_OR_PATH\r\n",
      "                        Path to pre-trained model or shortcut name selected in\r\n",
      "                        the list: distilbert-base-uncased, distilbert-base-\r\n",
      "                        uncased-distilled-squad, distilbert-base-cased,\r\n",
      "                        distilbert-base-cased-distilled-squad, distilbert-\r\n",
      "                        base-german-cased, distilbert-base-multilingual-cased,\r\n",
      "                        distilbert-base-uncased-finetuned-sst-2-english,\r\n",
      "                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\r\n",
      "                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\r\n",
      "                        albert-xlarge-v2, albert-xxlarge-v2, camembert-base,\r\n",
      "                        umberto-commoncrawl-cased-v1, umberto-wikipedia-\r\n",
      "                        uncased-v1, xlm-roberta-base, xlm-roberta-large, xlm-\r\n",
      "                        roberta-large-finetuned-conll02-dutch, xlm-roberta-\r\n",
      "                        large-finetuned-conll02-spanish, xlm-roberta-large-\r\n",
      "                        finetuned-conll03-english, xlm-roberta-large-\r\n",
      "                        finetuned-conll03-german, bart-large, bart-large-mnli,\r\n",
      "                        bart-large-cnn, bart-large-xsum, roberta-base,\r\n",
      "                        roberta-large, roberta-large-mnli, distilroberta-base,\r\n",
      "                        roberta-base-openai-detector, roberta-large-openai-\r\n",
      "                        detector, bert-base-uncased, bert-large-uncased, bert-\r\n",
      "                        base-cased, bert-large-cased, bert-base-multilingual-\r\n",
      "                        uncased, bert-base-multilingual-cased, bert-base-\r\n",
      "                        chinese, bert-base-german-cased, bert-large-uncased-\r\n",
      "                        whole-word-masking, bert-large-cased-whole-word-\r\n",
      "                        masking, bert-large-uncased-whole-word-masking-\r\n",
      "                        finetuned-squad, bert-large-cased-whole-word-masking-\r\n",
      "                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\r\n",
      "                        base-german-dbmdz-cased, bert-base-german-dbmdz-\r\n",
      "                        uncased, bert-base-japanese, bert-base-japanese-whole-\r\n",
      "                        word-masking, bert-base-japanese-char, bert-base-\r\n",
      "                        japanese-char-whole-word-masking, bert-base-finnish-\r\n",
      "                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\r\n",
      "                        dutch-cased, xlnet-base-cased, xlnet-large-cased,\r\n",
      "                        flaubert-small-cased, flaubert-base-uncased, flaubert-\r\n",
      "                        base-cased, flaubert-large-cased, xlm-mlm-en-2048,\r\n",
      "                        xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-\r\n",
      "                        enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\r\n",
      "                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\r\n",
      "                        xlm-mlm-17-1280, xlm-mlm-100-1280\r\n",
      "  --task_name TASK_NAME\r\n",
      "                        The name of the task to train selected in the list:\r\n",
      "                        cola, mnli, mnli-mm, mrpc, sst-2, sts-b, qqp, qnli,\r\n",
      "                        rte, wnli\r\n",
      "  --output_dir OUTPUT_DIR\r\n",
      "                        The output directory where the model predictions and\r\n",
      "                        checkpoints will be written.\r\n",
      "  --config_name CONFIG_NAME\r\n",
      "                        Pretrained config name or path if not the same as\r\n",
      "                        model_name\r\n",
      "  --tokenizer_name TOKENIZER_NAME\r\n",
      "                        Pretrained tokenizer name or path if not the same as\r\n",
      "                        model_name\r\n",
      "  --cache_dir CACHE_DIR\r\n",
      "                        Where do you want to store the pre-trained models\r\n",
      "                        downloaded from s3\r\n",
      "  --max_seq_length MAX_SEQ_LENGTH\r\n",
      "                        The maximum total input sequence length after\r\n",
      "                        tokenization. Sequences longer than this will be\r\n",
      "                        truncated, sequences shorter will be padded.\r\n",
      "  --do_train            Whether to run training.\r\n",
      "  --do_eval             Whether to run eval on the dev set.\r\n",
      "  --evaluate_during_training\r\n",
      "                        Run evaluation during training at each logging step.\r\n",
      "  --do_lower_case       Set this flag if you are using an uncased model.\r\n",
      "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\r\n",
      "                        Batch size per GPU/CPU for training.\r\n",
      "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\r\n",
      "                        Batch size per GPU/CPU for evaluation.\r\n",
      "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\r\n",
      "                        Number of updates steps to accumulate before\r\n",
      "                        performing a backward/update pass.\r\n",
      "  --learning_rate LEARNING_RATE\r\n",
      "                        The initial learning rate for Adam.\r\n",
      "  --weight_decay WEIGHT_DECAY\r\n",
      "                        Weight decay if we apply some.\r\n",
      "  --adam_epsilon ADAM_EPSILON\r\n",
      "                        Epsilon for Adam optimizer.\r\n",
      "  --max_grad_norm MAX_GRAD_NORM\r\n",
      "                        Max gradient norm.\r\n",
      "  --num_train_epochs NUM_TRAIN_EPOCHS\r\n",
      "                        Total number of training epochs to perform.\r\n",
      "  --max_steps MAX_STEPS\r\n",
      "                        If > 0: set total number of training steps to perform.\r\n",
      "                        Override num_train_epochs.\r\n",
      "  --warmup_steps WARMUP_STEPS\r\n",
      "                        Linear warmup over warmup_steps.\r\n",
      "  --logging_steps LOGGING_STEPS\r\n",
      "                        Log every X updates steps.\r\n",
      "  --save_steps SAVE_STEPS\r\n",
      "                        Save checkpoint every X updates steps.\r\n",
      "  --eval_all_checkpoints\r\n",
      "                        Evaluate all checkpoints starting with the same prefix\r\n",
      "                        as model_name ending and ending with step number\r\n",
      "  --no_cuda             Avoid using CUDA when available\r\n",
      "  --overwrite_output_dir\r\n",
      "                        Overwrite the content of the output directory\r\n",
      "  --overwrite_cache     Overwrite the cached training and evaluation sets\r\n",
      "  --seed SEED           random seed for initialization\r\n",
      "  --fp16                Whether to use 16-bit (mixed) precision (through\r\n",
      "                        NVIDIA apex) instead of 32-bit\r\n",
      "  --fp16_opt_level FP16_OPT_LEVEL\r\n",
      "                        For fp16: Apex AMP optimization level selected in\r\n",
      "                        ['O0', 'O1', 'O2', and 'O3'].See details at\r\n",
      "                        https://nvidia.github.io/apex/amp.html\r\n",
      "  --local_rank LOCAL_RANK\r\n",
      "                        For distributed training: local_rank\r\n",
      "  --server_ip SERVER_IP\r\n",
      "                        For distant debugging.\r\n",
      "  --server_port SERVER_PORT\r\n",
      "                        For distant debugging.\r\n"
     ]
    }
   ],
   "source": [
    "!python ./examples/run_glue.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/03/2020 19:15:26 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "04/03/2020 19:15:26 - INFO - filelock -   Lock 39773435552 acquired on /Users/anjaliagrawal/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
      "04/03/2020 19:15:26 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /Users/anjaliagrawal/.cache/torch/transformers/tmpwfh896a2\n",
      "Downloading: 100%|██████████████████████████████| 524/524 [00:00<00:00, 252kB/s]\n",
      "04/03/2020 19:15:26 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /Users/anjaliagrawal/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "04/03/2020 19:15:26 - INFO - transformers.file_utils -   creating metadata file for /Users/anjaliagrawal/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "04/03/2020 19:15:26 - INFO - filelock -   Lock 39773435552 released on /Users/anjaliagrawal/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
      "04/03/2020 19:15:26 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/anjaliagrawal/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "04/03/2020 19:15:26 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 19:15:26 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/anjaliagrawal/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
      "04/03/2020 19:15:26 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 19:15:27 - INFO - filelock -   Lock 112818886080 acquired on /Users/anjaliagrawal/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
      "04/03/2020 19:15:27 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /Users/anjaliagrawal/.cache/torch/transformers/tmpjoz6nmee\n",
      "Downloading: 100%|███████████████████████████| 899k/899k [00:00<00:00, 3.36MB/s]\n",
      "04/03/2020 19:15:27 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /Users/anjaliagrawal/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "04/03/2020 19:15:27 - INFO - transformers.file_utils -   creating metadata file for /Users/anjaliagrawal/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "04/03/2020 19:15:27 - INFO - filelock -   Lock 112818886080 released on /Users/anjaliagrawal/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
      "04/03/2020 19:15:27 - INFO - filelock -   Lock 112818887032 acquired on /Users/anjaliagrawal/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
      "04/03/2020 19:15:27 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /Users/anjaliagrawal/.cache/torch/transformers/tmpncpqtpb2\n",
      "Downloading: 100%|███████████████████████████| 456k/456k [00:00<00:00, 3.08MB/s]\n",
      "04/03/2020 19:15:27 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /Users/anjaliagrawal/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "04/03/2020 19:15:27 - INFO - transformers.file_utils -   creating metadata file for /Users/anjaliagrawal/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "04/03/2020 19:15:27 - INFO - filelock -   Lock 112818887032 released on /Users/anjaliagrawal/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
      "04/03/2020 19:15:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /Users/anjaliagrawal/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "04/03/2020 19:15:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /Users/anjaliagrawal/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/03/2020 19:15:28 - INFO - filelock -   Lock 48365308392 acquired on /Users/anjaliagrawal/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
      "04/03/2020 19:15:28 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/anjaliagrawal/.cache/torch/transformers/tmp2s_zijb2\n",
      "Downloading: 100%|███████████████████████████| 501M/501M [01:12<00:00, 6.90MB/s]\n",
      "04/03/2020 19:16:40 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /Users/anjaliagrawal/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "04/03/2020 19:16:40 - INFO - transformers.file_utils -   creating metadata file for /Users/anjaliagrawal/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "04/03/2020 19:16:40 - INFO - filelock -   Lock 48365308392 released on /Users/anjaliagrawal/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
      "04/03/2020 19:16:40 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/anjaliagrawal/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "04/03/2020 19:16:44 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "04/03/2020 19:16:44 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "04/03/2020 19:16:44 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='/Users/anjaliagrawal/content//CoLA', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=0, no_cuda=False, num_train_epochs=3.0, output_dir='../', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
      "04/03/2020 19:16:44 - INFO - __main__ -   Creating features from dataset file at /Users/anjaliagrawal/content//CoLA\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   Writing example 0/8551\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   guid: train-0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   input_ids: 0 1541 964 351 75 907 42 1966 6 905 1937 5 220 65 52 15393 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   guid: train-1\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   input_ids: 0 509 55 38283 937 1938 8 38 437 1311 62 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   guid: train-2\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   input_ids: 0 509 55 38283 937 1938 50 38 437 1311 62 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   guid: train-3\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   input_ids: 0 20 55 52 892 47041 6 5 26002 906 51 120 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   guid: train-4\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   input_ids: 0 1053 30 183 5 4905 32 562 22802 330 906 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 19:16:44 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/03/2020 19:16:45 - INFO - __main__ -   Saving features into cached file /Users/anjaliagrawal/content//CoLA/cached_train_roberta-base_128_cola\n",
      "04/03/2020 19:16:46 - INFO - __main__ -   ***** Running training *****\n",
      "04/03/2020 19:16:46 - INFO - __main__ -     Num examples = 8551\n",
      "04/03/2020 19:16:46 - INFO - __main__ -     Num Epochs = 3\n",
      "04/03/2020 19:16:46 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n",
      "04/03/2020 19:16:46 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "04/03/2020 19:16:46 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "04/03/2020 19:16:46 - INFO - __main__ -     Total optimization steps = 402\n",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                        | 0/134 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|▏                               | 1/134 [00:26<58:43, 26.49s/it]\u001b[A\n",
      "Iteration:   1%|▍                               | 2/134 [00:54<59:01, 26.83s/it]\u001b[A\n",
      "Iteration:   2%|▋                               | 3/134 [01:21<58:55, 26.99s/it]\u001b[A\n",
      "Iteration:   3%|▉                               | 4/134 [01:46<57:17, 26.44s/it]\u001b[A\n",
      "Iteration:   4%|█▏                              | 5/134 [02:10<55:10, 25.67s/it]\u001b[A\n",
      "Iteration:   4%|█▍                              | 6/134 [02:35<54:22, 25.49s/it]\u001b[A\n",
      "Iteration:   5%|█▋                              | 7/134 [03:00<53:42, 25.37s/it]\u001b[A\n",
      "Iteration:   6%|█▉                              | 8/134 [03:26<53:36, 25.53s/it]\u001b[A\n",
      "Iteration:   7%|██▏                             | 9/134 [03:51<53:05, 25.49s/it]\u001b[A\n",
      "Iteration:   7%|██▎                            | 10/134 [04:17<52:34, 25.44s/it]\u001b[A\n",
      "Iteration:   8%|██▌                            | 11/134 [04:40<50:36, 24.69s/it]\u001b[A\n",
      "Iteration:   9%|██▊                            | 12/134 [05:04<49:42, 24.45s/it]\u001b[A\n",
      "Iteration:  10%|███                            | 13/134 [05:28<49:14, 24.42s/it]\u001b[A\n",
      "Iteration:  10%|███▏                           | 14/134 [05:58<52:20, 26.17s/it]\u001b[A\n",
      "Iteration:  11%|███▍                           | 15/134 [06:22<50:17, 25.36s/it]\u001b[A\n",
      "Iteration:  12%|███▋                           | 16/134 [06:49<51:03, 25.96s/it]\u001b[A\n",
      "Iteration:  13%|███▉                           | 17/134 [07:13<49:43, 25.50s/it]\u001b[A\n",
      "Iteration:  13%|████▏                          | 18/134 [07:39<49:31, 25.62s/it]\u001b[A\n",
      "Iteration:  14%|████▍                          | 19/134 [08:03<47:53, 24.99s/it]\u001b[A\n",
      "Iteration:  15%|████▋                          | 20/134 [08:27<47:00, 24.74s/it]\u001b[A\n",
      "Iteration:  16%|████▊                          | 21/134 [08:50<45:51, 24.35s/it]\u001b[A\n",
      "Iteration:  16%|█████                          | 22/134 [09:15<45:33, 24.41s/it]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 23/134 [09:39<45:01, 24.34s/it]\u001b[A\n",
      "Iteration:  18%|█████▌                         | 24/134 [10:06<45:48, 24.98s/it]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 25/134 [10:32<46:07, 25.39s/it]\u001b[A\n",
      "Iteration:  19%|██████                         | 26/134 [10:58<46:01, 25.57s/it]\u001b[A\n",
      "Iteration:  20%|██████▏                        | 27/134 [11:23<45:33, 25.54s/it]\u001b[A\n",
      "Iteration:  21%|██████▍                        | 28/134 [11:52<46:49, 26.51s/it]\u001b[A\n",
      "Iteration:  22%|██████▋                        | 29/134 [12:17<45:16, 25.87s/it]\u001b[A\n",
      "Iteration:  22%|██████▉                        | 30/134 [12:40<43:21, 25.01s/it]\u001b[A\n",
      "Iteration:  23%|███████▏                       | 31/134 [13:03<41:55, 24.43s/it]\u001b[A\n",
      "Iteration:  24%|███████▍                       | 32/134 [13:26<40:51, 24.03s/it]\u001b[A\n",
      "Iteration:  25%|███████▋                       | 33/134 [13:49<40:15, 23.91s/it]\u001b[A\n",
      "Iteration:  25%|███████▊                       | 34/134 [14:13<39:36, 23.77s/it]\u001b[A\n",
      "Iteration:  26%|████████                       | 35/134 [14:37<39:18, 23.82s/it]\u001b[A\n",
      "Iteration:  27%|████████▎                      | 36/134 [14:59<38:16, 23.44s/it]\u001b[A\n",
      "Iteration:  28%|████████▌                      | 37/134 [15:22<37:27, 23.17s/it]\u001b[A\n",
      "Iteration:  28%|████████▊                      | 38/134 [15:45<36:51, 23.03s/it]\u001b[A\n",
      "Iteration:  29%|█████████                      | 39/134 [16:08<36:27, 23.03s/it]\u001b[A\n",
      "Iteration:  30%|█████████▎                     | 40/134 [16:31<36:07, 23.06s/it]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 41/134 [16:54<35:38, 23.00s/it]\u001b[A\n",
      "Iteration:  31%|█████████▋                     | 42/134 [17:19<36:29, 23.80s/it]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 43/134 [17:46<37:12, 24.53s/it]\u001b[A\n",
      "Iteration:  33%|██████████▏                    | 44/134 [18:09<36:26, 24.30s/it]\u001b[A\n",
      "Iteration:  34%|██████████▍                    | 45/134 [18:33<35:45, 24.10s/it]\u001b[A\n",
      "Iteration:  34%|██████████▋                    | 46/134 [18:56<34:52, 23.78s/it]\u001b[A\n",
      "Iteration:  35%|██████████▊                    | 47/134 [19:20<34:46, 23.98s/it]\u001b[A\n",
      "Iteration:  36%|███████████                    | 48/134 [19:46<35:13, 24.58s/it]\u001b[A\n",
      "Iteration:  37%|███████████▎                   | 49/134 [20:10<34:36, 24.43s/it]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 50/134 [20:36<34:29, 24.64s/it]\u001b[A\n",
      "Iteration:  38%|███████████▊                   | 51/134 [21:04<35:31, 25.68s/it]\u001b[A\n",
      "Iteration:  39%|████████████                   | 52/134 [21:30<35:28, 25.96s/it]\u001b[A\n",
      "Iteration:  40%|████████████▎                  | 53/134 [21:55<34:39, 25.67s/it]\u001b[A\n",
      "Iteration:  40%|████████████▍                  | 54/134 [22:22<34:41, 26.01s/it]\u001b[A\n",
      "Iteration:  41%|████████████▋                  | 55/134 [22:46<33:33, 25.49s/it]\u001b[A\n",
      "Iteration:  42%|████████████▉                  | 56/134 [23:11<32:46, 25.22s/it]\u001b[A\n",
      "Iteration:  43%|█████████████▏                 | 57/134 [23:34<31:36, 24.62s/it]\u001b[A\n",
      "Iteration:  43%|█████████████▍                 | 58/134 [23:58<30:46, 24.30s/it]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 59/134 [24:22<30:13, 24.18s/it]\u001b[A\n",
      "Iteration:  45%|█████████████▉                 | 60/134 [24:47<30:05, 24.40s/it]\u001b[A\n",
      "Iteration:  46%|██████████████                 | 61/134 [25:10<29:20, 24.11s/it]\u001b[A\n",
      "Iteration:  46%|██████████████▎                | 62/134 [25:32<28:09, 23.47s/it]\u001b[A\n",
      "Iteration:  47%|██████████████▌                | 63/134 [25:55<27:31, 23.26s/it]\u001b[A\n",
      "Iteration:  48%|██████████████▊                | 64/134 [26:16<26:33, 22.77s/it]\u001b[A\n",
      "Iteration:  49%|███████████████                | 65/134 [26:40<26:27, 23.01s/it]\u001b[A\n",
      "Iteration:  49%|███████████████▎               | 66/134 [27:03<26:04, 23.00s/it]\u001b[A\n",
      "Iteration:  50%|███████████████▌               | 67/134 [27:27<25:54, 23.19s/it]\u001b[A\n",
      "Iteration:  51%|███████████████▋               | 68/134 [27:50<25:26, 23.13s/it]\u001b[A\n",
      "Iteration:  51%|███████████████▉               | 69/134 [28:13<25:03, 23.13s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▏              | 70/134 [28:36<24:37, 23.08s/it]\u001b[A\n",
      "Iteration:  53%|████████████████▍              | 71/134 [28:59<24:10, 23.02s/it]\u001b[A\n",
      "Iteration:  54%|████████████████▋              | 72/134 [29:22<23:53, 23.13s/it]\u001b[A\n",
      "Iteration:  54%|████████████████▉              | 73/134 [29:46<23:40, 23.29s/it]\u001b[A\n",
      "Iteration:  55%|█████████████████              | 74/134 [30:09<23:25, 23.42s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 75/134 [30:33<22:58, 23.36s/it]\u001b[A\n",
      "Iteration:  57%|█████████████████▌             | 76/134 [30:55<22:21, 23.13s/it]\u001b[A\n",
      "Iteration:  57%|█████████████████▊             | 77/134 [31:17<21:41, 22.83s/it]\u001b[A\n",
      "Iteration:  58%|██████████████████             | 78/134 [31:39<21:07, 22.63s/it]\u001b[A\n",
      "Iteration:  59%|██████████████████▎            | 79/134 [32:01<20:31, 22.38s/it]\u001b[A\n",
      "Iteration:  60%|██████████████████▌            | 80/134 [32:23<19:56, 22.15s/it]\u001b[A\n",
      "Iteration:  60%|██████████████████▋            | 81/134 [32:45<19:30, 22.08s/it]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 82/134 [33:06<18:59, 21.92s/it]\u001b[A\n",
      "Iteration:  62%|███████████████████▏           | 83/134 [33:28<18:32, 21.82s/it]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 84/134 [33:50<18:10, 21.80s/it]\u001b[A\n",
      "Iteration:  63%|███████████████████▋           | 85/134 [34:11<17:42, 21.68s/it]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 86/134 [34:32<17:16, 21.60s/it]\u001b[A\n",
      "Iteration:  65%|████████████████████▏          | 87/134 [34:54<16:56, 21.63s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  66%|████████████████████▎          | 88/134 [35:16<16:39, 21.73s/it]\u001b[A\n",
      "Iteration:  66%|████████████████████▌          | 89/134 [35:38<16:22, 21.83s/it]\u001b[A\n",
      "Iteration:  67%|████████████████████▊          | 90/134 [36:00<15:54, 21.68s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 91/134 [36:21<15:33, 21.72s/it]\u001b[A\n",
      "Iteration:  69%|█████████████████████▎         | 92/134 [36:43<15:07, 21.62s/it]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 93/134 [37:04<14:45, 21.60s/it]\u001b[A\n",
      "Iteration:  70%|█████████████████████▋         | 94/134 [37:26<14:24, 21.62s/it]\u001b[A\n",
      "Iteration:  71%|█████████████████████▉         | 95/134 [37:48<14:05, 21.68s/it]\u001b[A\n",
      "Iteration:  72%|██████████████████████▏        | 96/134 [38:11<14:01, 22.13s/it]\u001b[A\n",
      "Iteration:  72%|██████████████████████▍        | 97/134 [38:33<13:43, 22.26s/it]\u001b[A\n",
      "Iteration:  73%|██████████████████████▋        | 98/134 [38:56<13:27, 22.44s/it]\u001b[A\n",
      "Iteration:  74%|██████████████████████▉        | 99/134 [39:18<13:00, 22.30s/it]\u001b[A\n",
      "Iteration:  75%|██████████████████████▍       | 100/134 [39:40<12:34, 22.18s/it]\u001b[A\n",
      "Iteration:  75%|██████████████████████▌       | 101/134 [40:02<12:08, 22.09s/it]\u001b[A\n",
      "Iteration:  76%|██████████████████████▊       | 102/134 [40:25<11:54, 22.34s/it]\u001b[A\n",
      "Iteration:  77%|███████████████████████       | 103/134 [40:47<11:30, 22.28s/it]\u001b[A\n",
      "Iteration:  78%|███████████████████████▎      | 104/134 [41:09<11:02, 22.09s/it]\u001b[A\n",
      "Iteration:  78%|███████████████████████▌      | 105/134 [41:30<10:35, 21.92s/it]\u001b[A\n",
      "Iteration:  79%|███████████████████████▋      | 106/134 [41:52<10:09, 21.77s/it]\u001b[A\n",
      "Iteration:  80%|███████████████████████▉      | 107/134 [42:14<09:50, 21.87s/it]\u001b[A\n",
      "Iteration:  81%|████████████████████████▏     | 108/134 [42:36<09:34, 22.10s/it]\u001b[A\n",
      "Iteration:  81%|████████████████████████▍     | 109/134 [42:58<09:07, 21.88s/it]\u001b[A\n",
      "Iteration:  82%|████████████████████████▋     | 110/134 [43:19<08:41, 21.74s/it]\u001b[A\n",
      "Iteration:  83%|████████████████████████▊     | 111/134 [43:41<08:18, 21.68s/it]\u001b[A\n",
      "Iteration:  84%|█████████████████████████     | 112/134 [44:03<07:57, 21.70s/it]\u001b[A\n",
      "Iteration:  84%|█████████████████████████▎    | 113/134 [44:24<07:37, 21.77s/it]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▌    | 114/134 [44:46<07:14, 21.74s/it]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▋    | 115/134 [45:10<07:02, 22.24s/it]\u001b[A\n",
      "Iteration:  87%|█████████████████████████▉    | 116/134 [45:32<06:39, 22.20s/it]\u001b[A\n",
      "Iteration:  87%|██████████████████████████▏   | 117/134 [45:53<06:14, 22.05s/it]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 118/134 [46:15<05:51, 21.95s/it]\u001b[A\n",
      "Iteration:  89%|██████████████████████████▋   | 119/134 [46:37<05:28, 21.91s/it]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▊   | 120/134 [47:00<05:10, 22.20s/it]\u001b[A\n",
      "Iteration:  90%|███████████████████████████   | 121/134 [47:21<04:45, 21.98s/it]\u001b[A\n",
      "Iteration:  91%|███████████████████████████▎  | 122/134 [47:43<04:22, 21.87s/it]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▌  | 123/134 [48:05<04:00, 21.83s/it]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▊  | 124/134 [48:27<03:38, 21.90s/it]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 125/134 [48:49<03:18, 22.02s/it]\u001b[A\n",
      "Iteration:  94%|████████████████████████████▏ | 126/134 [49:11<02:55, 21.99s/it]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 127/134 [49:32<02:33, 21.88s/it]\u001b[A\n",
      "Iteration:  96%|████████████████████████████▋ | 128/134 [49:54<02:10, 21.78s/it]\u001b[A\n",
      "Iteration:  96%|████████████████████████████▉ | 129/134 [50:16<01:48, 21.73s/it]\u001b[A\n",
      "Iteration:  97%|█████████████████████████████ | 130/134 [50:38<01:27, 21.85s/it]\u001b[A\n",
      "Iteration:  98%|█████████████████████████████▎| 131/134 [50:59<01:05, 21.72s/it]\u001b[A\n",
      "Iteration:  99%|█████████████████████████████▌| 132/134 [51:21<00:43, 21.66s/it]\u001b[A\n",
      "Iteration:  99%|█████████████████████████████▊| 133/134 [51:42<00:21, 21.59s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 134/134 [51:56<00:00, 23.26s/it]\u001b[A\n",
      "Epoch:  33%|███████████▎                      | 1/3 [51:56<1:43:52, 3116.35s/it]\n",
      "Iteration:   0%|                                        | 0/134 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|▏                               | 1/134 [00:21<48:05, 21.70s/it]\u001b[A\n",
      "Iteration:   1%|▍                               | 2/134 [00:44<48:15, 21.94s/it]\u001b[A\n",
      "Iteration:   2%|▋                               | 3/134 [01:05<47:48, 21.90s/it]\u001b[A\n",
      "Iteration:   3%|▉                               | 4/134 [01:27<47:11, 21.78s/it]\u001b[A\n",
      "Iteration:   4%|█▏                              | 5/134 [01:49<46:39, 21.70s/it]\u001b[A\n",
      "Iteration:   4%|█▍                              | 6/134 [02:10<46:13, 21.67s/it]\u001b[A\n",
      "Iteration:   5%|█▋                              | 7/134 [02:32<45:42, 21.59s/it]\u001b[A\n",
      "Iteration:   6%|█▉                              | 8/134 [02:53<45:15, 21.55s/it]\u001b[A\n",
      "Iteration:   7%|██▏                             | 9/134 [03:15<45:16, 21.73s/it]\u001b[A\n",
      "Iteration:   7%|██▎                            | 10/134 [03:37<44:44, 21.65s/it]\u001b[A\n",
      "Iteration:   8%|██▌                            | 11/134 [03:58<44:25, 21.67s/it]\u001b[A\n",
      "Iteration:   9%|██▊                            | 12/134 [04:20<43:54, 21.60s/it]\u001b[A\n",
      "Iteration:  10%|███                            | 13/134 [04:41<43:24, 21.53s/it]\u001b[A\n",
      "Iteration:  10%|███▏                           | 14/134 [05:03<43:00, 21.50s/it]\u001b[A\n",
      "Iteration:  11%|███▍                           | 15/134 [05:24<42:39, 21.51s/it]\u001b[A\n",
      "Iteration:  12%|███▋                           | 16/134 [05:46<42:27, 21.59s/it]\u001b[A\n",
      "Iteration:  13%|███▉                           | 17/134 [06:07<42:03, 21.57s/it]\u001b[A\n",
      "Iteration:  13%|████▏                          | 18/134 [06:31<42:43, 22.10s/it]\u001b[A\n",
      "Iteration:  14%|████▍                          | 19/134 [06:56<44:07, 23.02s/it]\u001b[A\n",
      "Iteration:  15%|████▋                          | 20/134 [07:18<43:17, 22.79s/it]\u001b[A\n",
      "Iteration:  16%|████▊                          | 21/134 [07:40<42:37, 22.63s/it]\u001b[A\n",
      "Iteration:  16%|█████                          | 22/134 [08:03<42:10, 22.59s/it]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 23/134 [08:26<41:58, 22.69s/it]\u001b[A\n",
      "Iteration:  18%|█████▌                         | 24/134 [08:50<42:35, 23.23s/it]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 25/134 [09:17<44:15, 24.36s/it]\u001b[A\n",
      "Iteration:  19%|██████                         | 26/134 [09:43<44:28, 24.71s/it]\u001b[A\n",
      "Iteration:  20%|██████▏                        | 27/134 [10:08<44:10, 24.77s/it]\u001b[A\n",
      "Iteration:  21%|██████▍                        | 28/134 [10:32<43:17, 24.51s/it]\u001b[A\n",
      "Iteration:  22%|██████▋                        | 29/134 [10:57<43:27, 24.83s/it]\u001b[A\n",
      "Iteration:  22%|██████▉                        | 30/134 [11:21<42:36, 24.59s/it]\u001b[A\n",
      "Iteration:  23%|███████▏                       | 31/134 [11:51<44:52, 26.14s/it]\u001b[A\n",
      "Iteration:  24%|███████▍                       | 32/134 [12:21<46:13, 27.19s/it]\u001b[A\n",
      "Iteration:  25%|███████▋                       | 33/134 [12:47<45:12, 26.86s/it]\u001b[A\n",
      "Iteration:  25%|███████▊                       | 34/134 [13:12<43:51, 26.31s/it]\u001b[A\n",
      "Iteration:  26%|████████                       | 35/134 [13:37<42:59, 26.06s/it]\u001b[A\n",
      "Iteration:  27%|████████▎                      | 36/134 [14:02<42:09, 25.81s/it]\u001b[A\n",
      "Iteration:  28%|████████▌                      | 37/134 [14:26<40:44, 25.20s/it]\u001b[A\n",
      "Iteration:  28%|████████▊                      | 38/134 [14:50<39:42, 24.81s/it]\u001b[A\n",
      "Iteration:  29%|█████████                      | 39/134 [15:15<39:16, 24.80s/it]\u001b[A\n",
      "Iteration:  30%|█████████▎                     | 40/134 [15:39<38:26, 24.53s/it]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 41/134 [16:04<38:08, 24.60s/it]\u001b[A\n",
      "Iteration:  31%|█████████▋                     | 42/134 [16:28<37:32, 24.49s/it]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 43/134 [16:52<37:11, 24.53s/it]\u001b[A\n",
      "Iteration:  33%|██████████▏                    | 44/134 [17:17<36:55, 24.61s/it]\u001b[A\n",
      "Iteration:  34%|██████████▍                    | 45/134 [17:42<36:34, 24.66s/it]\u001b[A\n",
      "Iteration:  34%|██████████▋                    | 46/134 [18:08<36:42, 25.02s/it]\u001b[A\n",
      "Iteration:  35%|██████████▊                    | 47/134 [18:34<36:40, 25.30s/it]\u001b[A\n",
      "Iteration:  36%|███████████                    | 48/134 [18:58<35:48, 24.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  37%|███████████▎                   | 49/134 [19:22<34:55, 24.65s/it]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 50/134 [19:47<34:37, 24.74s/it]\u001b[A\n",
      "Iteration:  38%|███████████▊                   | 51/134 [20:15<35:29, 25.66s/it]\u001b[A\n",
      "Iteration:  39%|████████████                   | 52/134 [20:39<34:19, 25.12s/it]\u001b[A\n",
      "Iteration:  40%|████████████▎                  | 53/134 [21:02<33:25, 24.77s/it]\u001b[A\n",
      "Iteration:  40%|████████████▍                  | 54/134 [21:26<32:40, 24.51s/it]\u001b[A\n",
      "Iteration:  41%|████████████▋                  | 55/134 [21:51<32:19, 24.55s/it]\u001b[A\n",
      "Iteration:  42%|████████████▉                  | 56/134 [22:16<31:54, 24.54s/it]\u001b[A\n",
      "Iteration:  43%|█████████████▏                 | 57/134 [22:40<31:28, 24.52s/it]\u001b[A\n",
      "Iteration:  43%|█████████████▍                 | 58/134 [23:05<31:07, 24.58s/it]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 59/134 [23:30<30:47, 24.63s/it]\u001b[A\n",
      "Iteration:  45%|█████████████▉                 | 60/134 [23:54<30:13, 24.50s/it]\u001b[A\n",
      "Iteration:  46%|██████████████                 | 61/134 [24:19<30:03, 24.71s/it]\u001b[A\n",
      "Iteration:  46%|██████████████▎                | 62/134 [24:44<29:54, 24.93s/it]\u001b[A\n",
      "Iteration:  47%|██████████████▌                | 63/134 [25:10<29:40, 25.07s/it]\u001b[A\n",
      "Iteration:  48%|██████████████▊                | 64/134 [25:34<29:07, 24.97s/it]\u001b[A\n",
      "Iteration:  49%|███████████████                | 65/134 [25:59<28:39, 24.92s/it]\u001b[A\n",
      "Iteration:  49%|███████████████▎               | 66/134 [26:23<27:59, 24.70s/it]\u001b[A\n",
      "Iteration:  50%|███████████████▌               | 67/134 [26:48<27:34, 24.70s/it]\u001b[A\n",
      "Iteration:  51%|███████████████▋               | 68/134 [27:12<27:02, 24.58s/it]\u001b[A\n",
      "Iteration:  51%|███████████████▉               | 69/134 [27:36<26:13, 24.21s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▏              | 70/134 [27:58<25:03, 23.49s/it]\u001b[A\n",
      "Iteration:  53%|████████████████▍              | 71/134 [28:19<24:03, 22.92s/it]\u001b[A\n",
      "Iteration:  54%|████████████████▋              | 72/134 [28:41<23:28, 22.72s/it]\u001b[A\n",
      "Iteration:  54%|████████████████▉              | 73/134 [29:03<22:42, 22.34s/it]\u001b[A\n",
      "Iteration:  55%|█████████████████              | 74/134 [29:27<22:54, 22.90s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 75/134 [29:52<22:59, 23.37s/it]\u001b[A\n",
      "Iteration:  57%|█████████████████▌             | 76/134 [30:16<22:57, 23.74s/it]\u001b[A\n",
      "Iteration:  57%|█████████████████▊             | 77/134 [30:42<23:09, 24.38s/it]\u001b[A\n",
      "Iteration:  58%|██████████████████             | 78/134 [31:07<22:55, 24.56s/it]\u001b[A\n",
      "Iteration:  59%|██████████████████▎            | 79/134 [31:32<22:43, 24.79s/it]\u001b[A\n",
      "Iteration:  60%|██████████████████▌            | 80/134 [31:57<22:13, 24.70s/it]\u001b[A\n",
      "Iteration:  60%|██████████████████▋            | 81/134 [32:22<22:03, 24.96s/it]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 82/134 [32:45<21:07, 24.38s/it]\u001b[A\n",
      "Iteration:  62%|███████████████████▏           | 83/134 [33:08<20:09, 23.72s/it]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 84/134 [33:31<19:38, 23.57s/it]\u001b[A\n",
      "Iteration:  63%|███████████████████▋           | 85/134 [33:54<19:05, 23.38s/it]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 86/134 [34:16<18:29, 23.12s/it]\u001b[A\n",
      "Iteration:  65%|████████████████████▏          | 87/134 [34:38<17:53, 22.84s/it]\u001b[A\n",
      "Iteration:  66%|████████████████████▎          | 88/134 [35:03<17:57, 23.42s/it]\u001b[A\n",
      "Iteration:  66%|████████████████████▌          | 89/134 [35:29<18:06, 24.15s/it]\u001b[A\n",
      "Iteration:  67%|████████████████████▊          | 90/134 [35:54<17:55, 24.45s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 91/134 [36:18<17:27, 24.36s/it]\u001b[A\n",
      "Iteration:  69%|█████████████████████▎         | 92/134 [36:43<17:02, 24.35s/it]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 93/134 [37:07<16:38, 24.36s/it]\u001b[A\n",
      "Iteration:  70%|█████████████████████▋         | 94/134 [37:31<16:07, 24.19s/it]\u001b[A\n",
      "Iteration:  71%|█████████████████████▉         | 95/134 [37:56<15:50, 24.36s/it]\u001b[A\n",
      "Iteration:  72%|██████████████████████▏        | 96/134 [38:22<15:52, 25.07s/it]\u001b[A\n",
      "Iteration:  72%|██████████████████████▍        | 97/134 [38:48<15:28, 25.09s/it]\u001b[A\n",
      "Iteration:  73%|██████████████████████▋        | 98/134 [39:12<14:58, 24.95s/it]\u001b[A\n",
      "Iteration:  74%|██████████████████████▉        | 99/134 [39:37<14:27, 24.79s/it]\u001b[A\n",
      "Iteration:  75%|██████████████████████▍       | 100/134 [40:01<14:00, 24.73s/it]\u001b[A\n",
      "Iteration:  75%|██████████████████████▌       | 101/134 [40:26<13:37, 24.78s/it]\u001b[A\n",
      "Iteration:  76%|██████████████████████▊       | 102/134 [40:51<13:14, 24.82s/it]\u001b[A\n",
      "Iteration:  77%|███████████████████████       | 103/134 [41:16<12:48, 24.79s/it]\u001b[A\n",
      "Iteration:  78%|███████████████████████▎      | 104/134 [41:40<12:23, 24.77s/it]\u001b[A\n",
      "Iteration:  78%|███████████████████████▌      | 105/134 [42:05<12:00, 24.85s/it]\u001b[A\n",
      "Iteration:  79%|███████████████████████▋      | 106/134 [42:30<11:33, 24.76s/it]\u001b[A\n",
      "Iteration:  80%|███████████████████████▉      | 107/134 [42:55<11:12, 24.92s/it]\u001b[A\n",
      "Iteration:  81%|████████████████████████▏     | 108/134 [43:21<10:51, 25.06s/it]\u001b[A\n",
      "Iteration:  81%|████████████████████████▍     | 109/134 [43:46<10:26, 25.06s/it]\u001b[A\n",
      "Iteration:  82%|████████████████████████▋     | 110/134 [44:10<09:58, 24.93s/it]\u001b[A\n",
      "Iteration:  83%|████████████████████████▊     | 111/134 [44:35<09:29, 24.77s/it]\u001b[A\n",
      "Iteration:  84%|█████████████████████████     | 112/134 [44:59<09:02, 24.65s/it]\u001b[A\n",
      "Iteration:  84%|█████████████████████████▎    | 113/134 [45:24<08:36, 24.62s/it]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▌    | 114/134 [45:51<08:30, 25.54s/it]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▋    | 115/134 [46:17<08:05, 25.53s/it]\u001b[A\n",
      "Iteration:  87%|█████████████████████████▉    | 116/134 [46:42<07:36, 25.34s/it]\u001b[A\n",
      "Iteration:  87%|██████████████████████████▏   | 117/134 [47:07<07:08, 25.20s/it]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 118/134 [47:32<06:45, 25.37s/it]\u001b[A\n",
      "Iteration:  89%|██████████████████████████▋   | 119/134 [47:57<06:16, 25.10s/it]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▊   | 120/134 [48:22<05:52, 25.21s/it]\u001b[A\n",
      "Iteration:  90%|███████████████████████████   | 121/134 [48:47<05:25, 25.07s/it]\u001b[A\n",
      "Iteration:  91%|███████████████████████████▎  | 122/134 [49:13<05:04, 25.40s/it]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▌  | 123/134 [49:38<04:38, 25.33s/it]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▊  | 124/134 [50:05<04:16, 25.61s/it]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 125/134 [50:30<03:48, 25.38s/it]\u001b[A\n",
      "Iteration:  94%|████████████████████████████▏ | 126/134 [50:54<03:20, 25.09s/it]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 127/134 [51:19<02:54, 24.92s/it]\u001b[A\n",
      "Iteration:  96%|████████████████████████████▋ | 128/134 [51:42<02:27, 24.56s/it]\u001b[A\n",
      "Iteration:  96%|████████████████████████████▉ | 129/134 [52:07<02:03, 24.62s/it]\u001b[A\n",
      "Iteration:  97%|█████████████████████████████ | 130/134 [52:33<01:39, 24.92s/it]\u001b[A\n",
      "Iteration:  98%|█████████████████████████████▎| 131/134 [52:58<01:15, 25.01s/it]\u001b[A\n",
      "Iteration:  99%|█████████████████████████████▌| 132/134 [53:22<00:49, 24.86s/it]\u001b[A\n",
      "Iteration:  99%|█████████████████████████████▊| 133/134 [53:47<00:24, 24.78s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 134/134 [54:03<00:00, 24.20s/it]\u001b[A\n",
      "Epoch:  67%|██████████████████████▋           | 2/3 [1:45:59<52:34, 3154.40s/it]\n",
      "Iteration:   0%|                                        | 0/134 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   1%|▏                               | 1/134 [00:24<54:30, 24.59s/it]\u001b[A\n",
      "Iteration:   1%|▍                               | 2/134 [00:48<53:56, 24.52s/it]\u001b[A\n",
      "Iteration:   2%|▋                               | 3/134 [01:13<53:41, 24.59s/it]\u001b[A\n",
      "Iteration:   3%|▉                               | 4/134 [01:39<54:02, 24.94s/it]\u001b[A\n",
      "Iteration:   4%|█▏                              | 5/134 [02:04<53:36, 24.94s/it]\u001b[A\n",
      "Iteration:   4%|█▍                              | 6/134 [02:28<52:45, 24.73s/it]\u001b[A\n",
      "Iteration:   5%|█▋                              | 7/134 [02:52<52:05, 24.61s/it]\u001b[A\n",
      "Iteration:   6%|█▉                              | 8/134 [03:20<53:14, 25.35s/it]\u001b[A\n",
      "Iteration:   7%|██▏                             | 9/134 [03:46<53:12, 25.54s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   7%|██▎                            | 10/134 [04:10<52:09, 25.24s/it]\u001b[A\n",
      "Iteration:   8%|██▌                            | 11/134 [04:35<51:16, 25.01s/it]\u001b[A\n",
      "Iteration:   9%|██▊                            | 12/134 [04:59<50:27, 24.81s/it]\u001b[A\n",
      "Iteration:  10%|███                            | 13/134 [05:23<49:37, 24.61s/it]\u001b[A\n",
      "Iteration:  10%|███▏                           | 14/134 [05:48<49:21, 24.68s/it]\u001b[A\n",
      "Iteration:  11%|███▍                           | 15/134 [06:13<49:13, 24.82s/it]\u001b[A\n",
      "Iteration:  12%|███▋                           | 16/134 [06:38<49:12, 25.02s/it]\u001b[A\n",
      "Iteration:  13%|███▉                           | 17/134 [07:03<48:13, 24.73s/it]\u001b[A\n",
      "Iteration:  13%|████▏                          | 18/134 [07:27<47:40, 24.66s/it]\u001b[A\n",
      "Iteration:  14%|████▍                          | 19/134 [07:52<47:20, 24.70s/it]\u001b[A\n",
      "Iteration:  15%|████▋                          | 20/134 [08:17<47:27, 24.98s/it]\u001b[A\n",
      "Iteration:  16%|████▊                          | 21/134 [08:42<46:58, 24.94s/it]\u001b[A\n",
      "Iteration:  16%|█████                          | 22/134 [09:07<46:26, 24.88s/it]\u001b[A\n",
      "Iteration:  17%|█████▎                         | 23/134 [09:32<46:15, 25.01s/it]\u001b[A\n",
      "Iteration:  18%|█████▌                         | 24/134 [09:58<45:55, 25.05s/it]\u001b[A\n",
      "Iteration:  19%|█████▊                         | 25/134 [10:22<45:02, 24.79s/it]\u001b[A\n",
      "Iteration:  19%|██████                         | 26/134 [10:46<44:28, 24.71s/it]\u001b[A\n",
      "Iteration:  20%|██████▏                        | 27/134 [11:14<45:41, 25.63s/it]\u001b[A\n",
      "Iteration:  21%|██████▍                        | 28/134 [11:39<45:06, 25.54s/it]\u001b[A\n",
      "Iteration:  22%|██████▋                        | 29/134 [12:04<44:04, 25.19s/it]\u001b[A\n",
      "Iteration:  22%|██████▉                        | 30/134 [12:28<43:14, 24.95s/it]\u001b[A\n",
      "Iteration:  23%|███████▏                       | 31/134 [12:53<43:02, 25.07s/it]\u001b[A\n",
      "Iteration:  24%|███████▍                       | 32/134 [13:18<42:13, 24.84s/it]\u001b[A\n",
      "Iteration:  25%|███████▋                       | 33/134 [13:42<41:22, 24.58s/it]\u001b[A\n",
      "Iteration:  25%|███████▊                       | 34/134 [14:04<39:57, 23.97s/it]\u001b[A\n",
      "Iteration:  26%|████████                       | 35/134 [14:26<38:29, 23.33s/it]\u001b[A\n",
      "Iteration:  27%|████████▎                      | 36/134 [14:49<38:03, 23.30s/it]\u001b[A\n",
      "Iteration:  28%|████████▌                      | 37/134 [15:15<38:45, 23.97s/it]\u001b[A\n",
      "Iteration:  28%|████████▊                      | 38/134 [15:38<38:06, 23.82s/it]\u001b[A\n",
      "Iteration:  29%|█████████                      | 39/134 [16:01<36:58, 23.36s/it]\u001b[A\n",
      "Iteration:  30%|█████████▎                     | 40/134 [16:23<36:09, 23.08s/it]\u001b[A\n",
      "Iteration:  31%|█████████▍                     | 41/134 [16:46<35:39, 23.00s/it]\u001b[A\n",
      "Iteration:  31%|█████████▋                     | 42/134 [17:11<36:08, 23.58s/it]\u001b[A\n",
      "Iteration:  32%|█████████▉                     | 43/134 [17:33<35:12, 23.21s/it]\u001b[A\n",
      "Iteration:  33%|██████████▏                    | 44/134 [17:55<34:00, 22.68s/it]\u001b[A\n",
      "Iteration:  34%|██████████▍                    | 45/134 [18:17<33:23, 22.51s/it]\u001b[A\n",
      "Iteration:  34%|██████████▋                    | 46/134 [18:39<32:47, 22.36s/it]\u001b[A\n",
      "Iteration:  35%|██████████▊                    | 47/134 [19:00<32:04, 22.13s/it]\u001b[A\n",
      "Iteration:  36%|███████████                    | 48/134 [19:22<31:31, 21.99s/it]\u001b[A\n",
      "Iteration:  37%|███████████▎                   | 49/134 [19:45<31:30, 22.25s/it]\u001b[A\n",
      "Iteration:  37%|███████████▌                   | 50/134 [20:08<31:22, 22.41s/it]\u001b[A\n",
      "Iteration:  38%|███████████▊                   | 51/134 [20:31<31:14, 22.58s/it]\u001b[A\n",
      "Iteration:  39%|████████████                   | 52/134 [20:52<30:31, 22.34s/it]\u001b[A\n",
      "Iteration:  40%|████████████▎                  | 53/134 [21:16<30:41, 22.73s/it]\u001b[A\n",
      "Iteration:  40%|████████████▍                  | 54/134 [21:38<30:07, 22.60s/it]\u001b[A\n",
      "Iteration:  41%|████████████▋                  | 55/134 [22:00<29:23, 22.32s/it]\u001b[A\n",
      "Iteration:  42%|████████████▉                  | 56/134 [22:21<28:41, 22.08s/it]\u001b[A\n",
      "Iteration:  43%|█████████████▏                 | 57/134 [22:44<28:35, 22.28s/it]\u001b[A\n",
      "Iteration:  43%|█████████████▍                 | 58/134 [23:07<28:16, 22.32s/it]\u001b[A\n",
      "Iteration:  44%|█████████████▋                 | 59/134 [23:30<28:15, 22.60s/it]\u001b[A\n",
      "Iteration:  45%|█████████████▉                 | 60/134 [23:52<27:40, 22.44s/it]\u001b[A\n",
      "Iteration:  46%|██████████████                 | 61/134 [24:17<28:12, 23.19s/it]\u001b[A\n",
      "Iteration:  46%|██████████████▎                | 62/134 [24:39<27:35, 23.00s/it]\u001b[A\n",
      "Iteration:  47%|██████████████▌                | 63/134 [25:01<26:51, 22.70s/it]\u001b[A\n",
      "Iteration:  48%|██████████████▊                | 64/134 [25:23<26:15, 22.50s/it]\u001b[A\n",
      "Iteration:  49%|███████████████                | 65/134 [25:46<25:43, 22.37s/it]\u001b[A\n",
      "Iteration:  49%|███████████████▎               | 66/134 [26:08<25:29, 22.50s/it]\u001b[A\n",
      "Iteration:  50%|███████████████▌               | 67/134 [26:30<24:54, 22.31s/it]\u001b[A\n",
      "Iteration:  51%|███████████████▋               | 68/134 [26:52<24:21, 22.15s/it]\u001b[A\n",
      "Iteration:  51%|███████████████▉               | 69/134 [27:14<23:48, 21.97s/it]\u001b[A\n",
      "Iteration:  52%|████████████████▏              | 70/134 [27:35<23:23, 21.93s/it]\u001b[A\n",
      "Iteration:  53%|████████████████▍              | 71/134 [27:57<23:04, 21.97s/it]\u001b[A\n",
      "Iteration:  54%|████████████████▋              | 72/134 [28:19<22:31, 21.79s/it]\u001b[A\n",
      "Iteration:  54%|████████████████▉              | 73/134 [28:41<22:10, 21.81s/it]\u001b[A\n",
      "Iteration:  55%|█████████████████              | 74/134 [29:02<21:48, 21.81s/it]\u001b[A\n",
      "Iteration:  56%|█████████████████▎             | 75/134 [29:24<21:21, 21.73s/it]\u001b[A\n",
      "Iteration:  57%|█████████████████▌             | 76/134 [29:45<20:54, 21.62s/it]\u001b[A\n",
      "Iteration:  57%|█████████████████▊             | 77/134 [30:11<21:32, 22.68s/it]\u001b[A\n",
      "Iteration:  58%|██████████████████             | 78/134 [30:33<21:05, 22.59s/it]\u001b[A\n",
      "Iteration:  59%|██████████████████▎            | 79/134 [30:55<20:30, 22.38s/it]\u001b[A\n",
      "Iteration:  60%|██████████████████▌            | 80/134 [31:17<20:00, 22.23s/it]\u001b[A\n",
      "Iteration:  60%|██████████████████▋            | 81/134 [31:42<20:19, 23.00s/it]\u001b[A\n",
      "Iteration:  61%|██████████████████▉            | 82/134 [32:04<19:46, 22.82s/it]\u001b[A\n",
      "Iteration:  62%|███████████████████▏           | 83/134 [32:26<19:19, 22.73s/it]\u001b[A\n",
      "Iteration:  63%|███████████████████▍           | 84/134 [32:49<18:51, 22.63s/it]\u001b[A\n",
      "Iteration:  63%|███████████████████▋           | 85/134 [33:11<18:16, 22.38s/it]\u001b[A\n",
      "Iteration:  64%|███████████████████▉           | 86/134 [33:32<17:43, 22.17s/it]\u001b[A\n",
      "Iteration:  65%|████████████████████▏          | 87/134 [33:54<17:12, 21.97s/it]\u001b[A\n",
      "Iteration:  66%|████████████████████▎          | 88/134 [34:16<16:55, 22.08s/it]\u001b[A\n",
      "Iteration:  66%|████████████████████▌          | 89/134 [34:38<16:37, 22.16s/it]\u001b[A\n",
      "Iteration:  67%|████████████████████▊          | 90/134 [35:00<16:08, 22.02s/it]\u001b[A\n",
      "Iteration:  68%|█████████████████████          | 91/134 [35:22<15:40, 21.88s/it]\u001b[A\n",
      "Iteration:  69%|█████████████████████▎         | 92/134 [35:43<15:16, 21.83s/it]\u001b[A\n",
      "Iteration:  69%|█████████████████████▌         | 93/134 [36:05<14:53, 21.79s/it]\u001b[A\n",
      "Iteration:  70%|█████████████████████▋         | 94/134 [36:28<14:39, 22.00s/it]\u001b[A\n",
      "Iteration:  71%|█████████████████████▉         | 95/134 [36:49<14:13, 21.88s/it]\u001b[A\n",
      "Iteration:  72%|██████████████████████▏        | 96/134 [37:11<13:45, 21.73s/it]\u001b[A\n",
      "Iteration:  72%|██████████████████████▍        | 97/134 [37:32<13:20, 21.63s/it]\u001b[A\n",
      "Iteration:  73%|██████████████████████▋        | 98/134 [37:53<12:55, 21.54s/it]\u001b[A\n",
      "Iteration:  74%|██████████████████████▉        | 99/134 [38:15<12:40, 21.73s/it]\u001b[A\n",
      "Iteration:  75%|██████████████████████▍       | 100/134 [38:37<12:16, 21.66s/it]\u001b[A\n",
      "Iteration:  75%|██████████████████████▌       | 101/134 [38:59<11:53, 21.63s/it]\u001b[A\n",
      "Iteration:  76%|██████████████████████▊       | 102/134 [39:20<11:34, 21.70s/it]\u001b[A\n",
      "Iteration:  77%|███████████████████████       | 103/134 [39:42<11:14, 21.76s/it]\u001b[A\n",
      "Iteration:  78%|███████████████████████▎      | 104/134 [40:04<10:53, 21.79s/it]\u001b[A\n",
      "Iteration:  78%|███████████████████████▌      | 105/134 [40:26<10:31, 21.78s/it]\u001b[A\n",
      "Iteration:  79%|███████████████████████▋      | 106/134 [40:48<10:10, 21.81s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  80%|███████████████████████▉      | 107/134 [41:10<09:51, 21.91s/it]\u001b[A\n",
      "Iteration:  81%|████████████████████████▏     | 108/134 [41:32<09:34, 22.09s/it]\u001b[A\n",
      "Iteration:  81%|████████████████████████▍     | 109/134 [41:54<09:09, 22.00s/it]\u001b[A\n",
      "Iteration:  82%|████████████████████████▋     | 110/134 [42:16<08:46, 21.92s/it]\u001b[A\n",
      "Iteration:  83%|████████████████████████▊     | 111/134 [42:41<08:44, 22.80s/it]\u001b[A\n",
      "Iteration:  84%|█████████████████████████     | 112/134 [43:03<08:16, 22.56s/it]\u001b[A\n",
      "Iteration:  84%|█████████████████████████▎    | 113/134 [43:25<07:48, 22.33s/it]\u001b[A\n",
      "Iteration:  85%|█████████████████████████▌    | 114/134 [43:47<07:24, 22.23s/it]\u001b[A\n",
      "Iteration:  86%|█████████████████████████▋    | 115/134 [44:08<06:57, 21.98s/it]\u001b[A\n",
      "Iteration:  87%|█████████████████████████▉    | 116/134 [44:30<06:35, 21.96s/it]\u001b[A\n",
      "Iteration:  87%|██████████████████████████▏   | 117/134 [44:52<06:14, 22.01s/it]\u001b[A\n",
      "Iteration:  88%|██████████████████████████▍   | 118/134 [45:14<05:49, 21.86s/it]\u001b[A\n",
      "Iteration:  89%|██████████████████████████▋   | 119/134 [45:35<05:26, 21.74s/it]\u001b[A\n",
      "Iteration:  90%|██████████████████████████▊   | 120/134 [45:57<05:04, 21.71s/it]\u001b[A\n",
      "Iteration:  90%|███████████████████████████   | 121/134 [46:19<04:43, 21.82s/it]\u001b[A\n",
      "Iteration:  91%|███████████████████████████▎  | 122/134 [46:41<04:22, 21.84s/it]\u001b[A\n",
      "Iteration:  92%|███████████████████████████▌  | 123/134 [47:03<04:02, 22.06s/it]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▊  | 124/134 [47:26<03:42, 22.21s/it]\u001b[A\n",
      "Iteration:  93%|███████████████████████████▉  | 125/134 [47:48<03:19, 22.14s/it]\u001b[A\n",
      "Iteration:  94%|████████████████████████████▏ | 126/134 [48:09<02:55, 21.99s/it]\u001b[A\n",
      "Iteration:  95%|████████████████████████████▍ | 127/134 [48:33<02:37, 22.48s/it]\u001b[A\n",
      "Iteration:  96%|████████████████████████████▋ | 128/134 [48:54<02:13, 22.19s/it]\u001b[A\n",
      "Iteration:  96%|████████████████████████████▉ | 129/134 [49:17<01:51, 22.24s/it]\u001b[A\n",
      "Iteration:  97%|█████████████████████████████ | 130/134 [49:39<01:28, 22.13s/it]\u001b[A\n",
      "Iteration:  98%|█████████████████████████████▎| 131/134 [50:01<01:06, 22.10s/it]\u001b[A\n",
      "Iteration:  99%|█████████████████████████████▌| 132/134 [50:23<00:44, 22.00s/it]\u001b[A\n",
      "Iteration:  99%|█████████████████████████████▊| 133/134 [50:46<00:22, 22.34s/it]\u001b[A\n",
      "Iteration: 100%|██████████████████████████████| 134/134 [50:59<00:00, 22.84s/it]\u001b[A\n",
      "Epoch: 100%|██████████████████████████████████| 3/3 [2:36:59<00:00, 3139.83s/it]\n",
      "04/03/2020 21:53:46 - INFO - __main__ -    global_step = 402, average loss = 0.3990955852453981\n",
      "04/03/2020 21:53:46 - INFO - __main__ -   Saving model checkpoint to ../\n",
      "04/03/2020 21:53:46 - INFO - transformers.configuration_utils -   Configuration saved in ../config.json\n",
      "04/03/2020 21:53:46 - INFO - transformers.modeling_utils -   Model weights saved in ../pytorch_model.bin\n",
      "04/03/2020 21:53:47 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
      "04/03/2020 21:53:47 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 21:53:47 - INFO - transformers.modeling_utils -   loading weights file ../pytorch_model.bin\n",
      "04/03/2020 21:53:50 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
      "04/03/2020 21:53:50 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   Model name '../' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   Didn't find file ../added_tokens.json. We won't load it.\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file ../vocab.json\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file ../merges.txt\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file None\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file ../special_tokens_map.json\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file ../tokenizer_config.json\n",
      "04/03/2020 21:53:50 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
      "04/03/2020 21:53:50 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   Model name '../' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   Didn't find file ../added_tokens.json. We won't load it.\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file ../vocab.json\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file ../merges.txt\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file None\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file ../special_tokens_map.json\n",
      "04/03/2020 21:53:50 - INFO - transformers.tokenization_utils -   loading file ../tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/03/2020 21:53:50 - INFO - __main__ -   Evaluate the following checkpoints: ['../']\n",
      "04/03/2020 21:53:50 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
      "04/03/2020 21:53:50 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "04/03/2020 21:53:50 - INFO - transformers.modeling_utils -   loading weights file ../pytorch_model.bin\n",
      "04/03/2020 21:53:53 - INFO - __main__ -   Creating features from dataset file at /Users/anjaliagrawal/content//CoLA\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   Writing example 0/1043\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   input_ids: 0 20 21362 12783 5 24572 699 9 5 10889 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   input_ids: 0 20 23341 156 5 17434 4140 81 5 25578 607 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   input_ids: 0 20 12418 19495 14902 11702 1329 1495 7082 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   input_ids: 0 318 47 56 18804 55 6 47 74 236 540 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   *** Example ***\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   input_ids: 0 287 47 3529 5 144 6 47 236 5 513 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "04/03/2020 21:53:53 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/03/2020 21:53:53 - INFO - __main__ -   Saving features into cached file /Users/anjaliagrawal/content//CoLA/cached_dev_roberta-base_128_cola\n",
      "04/03/2020 21:53:53 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "04/03/2020 21:53:53 - INFO - __main__ -     Num examples = 1043\n",
      "04/03/2020 21:53:53 - INFO - __main__ -     Batch size = 64\n",
      "Evaluating: 100%|███████████████████████████████| 17/17 [01:42<00:00,  6.02s/it]\n",
      "04/03/2020 21:55:35 - INFO - __main__ -   ***** Eval results  *****\n",
      "04/03/2020 21:55:35 - INFO - __main__ -     mcc = 0.5572696682585848\n"
     ]
    }
   ],
   "source": [
    "!python ./examples/run_glue.py \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path roberta-base \\\n",
    "    --task_name $TASK_NAME \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_gpu_eval_batch_size=64   \\\n",
    "    --per_gpu_train_batch_size=64   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --output_dir ../${TASK_NAME}_run \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
