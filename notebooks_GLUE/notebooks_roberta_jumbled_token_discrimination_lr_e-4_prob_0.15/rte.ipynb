{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rte.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1sSh-Yrt2fTQervm7S_DOddgRMMirO5Kd","authorship_tag":"ABX9TyOmzK6YP1dJ41ndZ5peYah7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qmnJe06SfVMN","colab_type":"text"},"source":["# Install transformers 2.8.0"]},{"cell_type":"code","metadata":{"id":"_rtNjBbdY7RM","colab_type":"code","outputId":"9eecfacd-869b-416b-c5ac-d5214442831b","executionInfo":{"status":"ok","timestamp":1589484359404,"user_tz":240,"elapsed":3929,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":433}},"source":["!pip install transformers==2.8.0"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.6/dist-packages (2.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.4)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.13.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.5.2)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.90)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.43)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n","Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.16.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.8.0) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.8.0) (0.15.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"M_moWpok-qkX","colab_type":"text"},"source":["# Set working directory to the directory containing `download_glue_data.py` and `run_glue.py`"]},{"cell_type":"code","metadata":{"id":"l0VFWNoOAfT-","colab_type":"code","outputId":"15d3052e-f9ab-4a6b-e2be-94f4d95b71b8","executionInfo":{"status":"ok","timestamp":1589484360120,"user_tz":240,"elapsed":373,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd ~/../content/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JU2wFP6SbsZf","colab_type":"code","colab":{}},"source":["import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LON2QVZObFug","colab_type":"code","colab":{}},"source":["WORK_DIR = os.path.join('drive', 'My Drive', 'Colab Notebooks', 'NLU')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mWKKk1x86hQ","colab_type":"code","outputId":"cc168144-2e3c-4c09-bb02-0a2f0887f08c","executionInfo":{"status":"ok","timestamp":1589484364042,"user_tz":240,"elapsed":575,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["os.path.exists(WORK_DIR)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"cU0UdJuH9F2Y","colab_type":"code","outputId":"4f868ac0-5bfa-41a8-aca9-2f27f0163ff7","executionInfo":{"status":"ok","timestamp":1589484365940,"user_tz":240,"elapsed":614,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd $WORK_DIR"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/NLU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"INV4cGxyf2f5","colab_type":"text"},"source":["# Download GLUE data"]},{"cell_type":"code","metadata":{"id":"PWYcFKd9f6Fb","colab_type":"code","colab":{}},"source":["GLUE_DIR=\"data/glue\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AJp5KgQglVh","colab_type":"code","outputId":"23e22ce5-f4ab-4f72-d88c-e30950b17c95","executionInfo":{"status":"ok","timestamp":1589484370916,"user_tz":240,"elapsed":2232,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!echo $GLUE_DIR"],"execution_count":9,"outputs":[{"output_type":"stream","text":["data/glue\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LFhfVflFdaVB","colab_type":"code","colab":{}},"source":["TASK_NAME=\"RTE\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_1EtLRQ8-WF","colab_type":"code","outputId":"c705c98b-0b1f-43be-c0cf-87f931c0e7c1","executionInfo":{"status":"ok","timestamp":1589484379388,"user_tz":240,"elapsed":2201,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!echo $TASK_NAME"],"execution_count":11,"outputs":[{"output_type":"stream","text":["RTE\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"24BX_hKwFhdU","colab_type":"code","outputId":"eb72c33f-da4e-49f6-bbe4-9141faf7a7a2","executionInfo":{"status":"ok","timestamp":1589484382850,"user_tz":240,"elapsed":3258,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":189}},"source":["!python download_glue_data.py --help"],"execution_count":12,"outputs":[{"output_type":"stream","text":["usage: download_glue_data.py [-h] [--data_dir DATA_DIR] [--tasks TASKS]\n","                             [--path_to_mrpc PATH_TO_MRPC]\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --data_dir DATA_DIR   directory to save data to\n","  --tasks TASKS         tasks to download data for as a comma separated string\n","  --path_to_mrpc PATH_TO_MRPC\n","                        path to directory containing extracted MRPC data,\n","                        msr_paraphrase_train.txt and msr_paraphrase_text.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rAqf1Fe6cZvI","colab_type":"code","outputId":"2100de42-9ca7-4d4e-c35f-1247a6c10347","executionInfo":{"status":"ok","timestamp":1589484390892,"user_tz":240,"elapsed":8115,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!python download_glue_data.py --data_dir $GLUE_DIR --tasks $TASK_NAME"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Downloading and extracting RTE...\n","\tCompleted!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6aqkcWzfgFNI","colab_type":"text"},"source":["# Compute score for specific GLUE task"]},{"cell_type":"code","metadata":{"id":"z4jsBBTfnwOa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"745f1791-f424-451d-e5f5-5a337ddba7aa","executionInfo":{"status":"ok","timestamp":1589484620708,"user_tz":240,"elapsed":548,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}}},"source":["MODEL_DIR = os.path.join('models', 'roberta_jumbled_token_discrimination_lr_e-4_prob_0.15')\n","os.path.exists(MODEL_DIR)"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"eiWazMGIsMk9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":310},"outputId":"e16c9055-97a0-4be3-b8c9-29949e59fbaf","executionInfo":{"status":"ok","timestamp":1589485037491,"user_tz":240,"elapsed":2542,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}}},"source":["!nvidia-smi"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Thu May 14 19:37:14 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   66C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SPpl0f8wAkmf","colab_type":"code","outputId":"108a1a20-abf5-4d6b-cf82-70e2fead01de","executionInfo":{"status":"ok","timestamp":1589484631553,"user_tz":240,"elapsed":5660,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run_glue.py --help"],"execution_count":28,"outputs":[{"output_type":"stream","text":["2020-05-14 19:30:27.234454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","usage: run_glue.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n","                   --model_name_or_path MODEL_NAME_OR_PATH --task_name\n","                   TASK_NAME --output_dir OUTPUT_DIR\n","                   [--config_name CONFIG_NAME]\n","                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n","                   [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n","                   [--evaluate_during_training] [--do_lower_case]\n","                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n","                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n","                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n","                   [--learning_rate LEARNING_RATE]\n","                   [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n","                   [--max_grad_norm MAX_GRAD_NORM]\n","                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n","                   [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n","                   [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n","                   [--eval_all_checkpoints] [--no_cuda]\n","                   [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n","                   [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n","                   [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n","                   [--server_port SERVER_PORT]\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n","                        other data files) for the task.\n","  --model_type MODEL_TYPE\n","                        Model type selected in the list: distilbert, albert,\n","                        camembert, xlm-roberta, bart, roberta, bert, xlnet,\n","                        flaubert, xlm\n","  --model_name_or_path MODEL_NAME_OR_PATH\n","                        Path to pre-trained model or shortcut name selected in\n","                        the list: distilbert-base-uncased, distilbert-base-\n","                        uncased-distilled-squad, distilbert-base-cased,\n","                        distilbert-base-cased-distilled-squad, distilbert-\n","                        base-german-cased, distilbert-base-multilingual-cased,\n","                        distilbert-base-uncased-finetuned-sst-2-english,\n","                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\n","                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\n","                        albert-xlarge-v2, albert-xxlarge-v2, camembert-base,\n","                        umberto-commoncrawl-cased-v1, umberto-wikipedia-\n","                        uncased-v1, xlm-roberta-base, xlm-roberta-large, xlm-\n","                        roberta-large-finetuned-conll02-dutch, xlm-roberta-\n","                        large-finetuned-conll02-spanish, xlm-roberta-large-\n","                        finetuned-conll03-english, xlm-roberta-large-\n","                        finetuned-conll03-german, bart-large, bart-large-mnli,\n","                        bart-large-cnn, bart-large-xsum, roberta-base,\n","                        roberta-large, roberta-large-mnli, distilroberta-base,\n","                        roberta-base-openai-detector, roberta-large-openai-\n","                        detector, bert-base-uncased, bert-large-uncased, bert-\n","                        base-cased, bert-large-cased, bert-base-multilingual-\n","                        uncased, bert-base-multilingual-cased, bert-base-\n","                        chinese, bert-base-german-cased, bert-large-uncased-\n","                        whole-word-masking, bert-large-cased-whole-word-\n","                        masking, bert-large-uncased-whole-word-masking-\n","                        finetuned-squad, bert-large-cased-whole-word-masking-\n","                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\n","                        base-german-dbmdz-cased, bert-base-german-dbmdz-\n","                        uncased, bert-base-japanese, bert-base-japanese-whole-\n","                        word-masking, bert-base-japanese-char, bert-base-\n","                        japanese-char-whole-word-masking, bert-base-finnish-\n","                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\n","                        dutch-cased, xlnet-base-cased, xlnet-large-cased,\n","                        flaubert-small-cased, flaubert-base-uncased, flaubert-\n","                        base-cased, flaubert-large-cased, xlm-mlm-en-2048,\n","                        xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-\n","                        enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\n","                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\n","                        xlm-mlm-17-1280, xlm-mlm-100-1280\n","  --task_name TASK_NAME\n","                        The name of the task to train selected in the list:\n","                        cola, mnli, mnli-mm, mrpc, sst-2, sts-b, qqp, qnli,\n","                        rte, wnli\n","  --output_dir OUTPUT_DIR\n","                        The output directory where the model predictions and\n","                        checkpoints will be written.\n","  --config_name CONFIG_NAME\n","                        Pretrained config name or path if not the same as\n","                        model_name\n","  --tokenizer_name TOKENIZER_NAME\n","                        Pretrained tokenizer name or path if not the same as\n","                        model_name\n","  --cache_dir CACHE_DIR\n","                        Where do you want to store the pre-trained models\n","                        downloaded from s3\n","  --max_seq_length MAX_SEQ_LENGTH\n","                        The maximum total input sequence length after\n","                        tokenization. Sequences longer than this will be\n","                        truncated, sequences shorter will be padded.\n","  --do_train            Whether to run training.\n","  --do_eval             Whether to run eval on the dev set.\n","  --evaluate_during_training\n","                        Run evaluation during training at each logging step.\n","  --do_lower_case       Set this flag if you are using an uncased model.\n","  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n","                        Batch size per GPU/CPU for training.\n","  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n","                        Batch size per GPU/CPU for evaluation.\n","  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n","                        Number of updates steps to accumulate before\n","                        performing a backward/update pass.\n","  --learning_rate LEARNING_RATE\n","                        The initial learning rate for Adam.\n","  --weight_decay WEIGHT_DECAY\n","                        Weight decay if we apply some.\n","  --adam_epsilon ADAM_EPSILON\n","                        Epsilon for Adam optimizer.\n","  --max_grad_norm MAX_GRAD_NORM\n","                        Max gradient norm.\n","  --num_train_epochs NUM_TRAIN_EPOCHS\n","                        Total number of training epochs to perform.\n","  --max_steps MAX_STEPS\n","                        If > 0: set total number of training steps to perform.\n","                        Override num_train_epochs.\n","  --warmup_steps WARMUP_STEPS\n","                        Linear warmup over warmup_steps.\n","  --logging_steps LOGGING_STEPS\n","                        Log every X updates steps.\n","  --save_steps SAVE_STEPS\n","                        Save checkpoint every X updates steps.\n","  --eval_all_checkpoints\n","                        Evaluate all checkpoints starting with the same prefix\n","                        as model_name ending and ending with step number\n","  --no_cuda             Avoid using CUDA when available\n","  --overwrite_output_dir\n","                        Overwrite the content of the output directory\n","  --overwrite_cache     Overwrite the cached training and evaluation sets\n","  --seed SEED           random seed for initialization\n","  --fp16                Whether to use 16-bit (mixed) precision (through\n","                        NVIDIA apex) instead of 32-bit\n","  --fp16_opt_level FP16_OPT_LEVEL\n","                        For fp16: Apex AMP optimization level selected in\n","                        ['O0', 'O1', 'O2', and 'O3'].See details at\n","                        https://nvidia.github.io/apex/amp.html\n","  --local_rank LOCAL_RANK\n","                        For distributed training: local_rank\n","  --server_ip SERVER_IP\n","                        For distant debugging.\n","  --server_port SERVER_PORT\n","                        For distant debugging.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HnxryCpnczcm","colab_type":"code","outputId":"083ceddb-6da3-4f27-bef1-1b9d2e7fcf17","executionInfo":{"status":"ok","timestamp":1589484987830,"user_tz":240,"elapsed":351573,"user":{"displayName":"Subhadarshi Panda","photoUrl":"","userId":"08772860912161050283"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python run_glue.py \\\n","    --model_type roberta \\\n","    --model_name_or_path $MODEL_DIR \\\n","    --task_name $TASK_NAME \\\n","    --do_train \\\n","    --do_eval \\\n","    --data_dir $GLUE_DIR/$TASK_NAME \\\n","    --max_seq_length 128 \\\n","    --per_gpu_eval_batch_size=64   \\\n","    --per_gpu_train_batch_size=64   \\\n","    --learning_rate 2e-5 \\\n","    --num_train_epochs 3 \\\n","    --output_dir run \\\n","    --overwrite_output_dir"],"execution_count":29,"outputs":[{"output_type":"stream","text":["2020-05-14 19:30:37.264121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","05/14/2020 19:30:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n","05/14/2020 19:30:40 - INFO - transformers.configuration_utils -   loading configuration file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/config.json\n","05/14/2020 19:30:40 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"RobertaForTokenDiscrimination\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"rte\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/14/2020 19:30:40 - INFO - transformers.configuration_utils -   loading configuration file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/config.json\n","05/14/2020 19:30:40 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"RobertaForTokenDiscrimination\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/14/2020 19:30:40 - INFO - transformers.tokenization_utils -   Model name 'models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15' is a path, a model identifier, or url to a directory containing tokenizer files.\n","05/14/2020 19:30:40 - INFO - transformers.tokenization_utils -   Didn't find file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/added_tokens.json. We won't load it.\n","05/14/2020 19:30:40 - INFO - transformers.tokenization_utils -   loading file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/vocab.json\n","05/14/2020 19:30:40 - INFO - transformers.tokenization_utils -   loading file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/merges.txt\n","05/14/2020 19:30:40 - INFO - transformers.tokenization_utils -   loading file None\n","05/14/2020 19:30:40 - INFO - transformers.tokenization_utils -   loading file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/special_tokens_map.json\n","05/14/2020 19:30:40 - INFO - transformers.tokenization_utils -   loading file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/tokenizer_config.json\n","05/14/2020 19:30:43 - INFO - transformers.modeling_utils -   loading weights file models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15/pytorch_model.bin\n","05/14/2020 19:31:03 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","05/14/2020 19:31:03 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n","05/14/2020 19:31:17 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data/glue/RTE', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='models/roberta_jumbled_token_discrimination_lr_e-4_prob_0.15', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='run', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='rte', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n","05/14/2020 19:31:17 - INFO - __main__ -   Creating features from dataset file at data/glue/RTE\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   Writing example 0/2490\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   guid: train-0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   input_ids: 0 440 28054 9 5370 43207 11911 11 3345 3507 4 2 2 28054 9 5370 43207 11911 11 3345 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   guid: train-1\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   input_ids: 0 83 317 9 26130 6 71 8509 610 1206 3082 962 6 1059 10 317 9 4821 6 25 7733 4019 15828 4366 11 3301 1568 7 2458 5 8809 9 92 8509 20742 42171 4 2 2 8509 20742 42171 16 5 92 884 9 5 7733 4019 2197 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   guid: train-2\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   input_ids: 0 1405 16771 179 21 416 2033 7 3951 5 4736 990 6181 1668 1484 6 8 5 138 26 6 302 6 24 40 2268 19 752 5904 5 3302 9 30724 5 1262 13 55 6181 1668 1484 4 2 2 1405 16771 179 64 28 341 7 3951 6181 1668 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   guid: train-3\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   input_ids: 0 19691 324 12701 811 6 834 1031 23 1698 21243 2426 6 10 1131 544 138 14 2607 9844 5 132 12 180 12 279 5490 6924 2534 11 5082 12718 3635 298 412 36 30520 4141 32410 238 26 14 98 444 59 112 6 1497 408 33 829 1416 4 2 2 20 986 766 9 5082 12718 3635 298 412 21 4141 32410 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   guid: train-4\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   input_ids: 0 83 313 16 528 11 461 423 1340 19 5 1900 973 107 536 9 10 7044 1060 403 21 5 78 7 28 3520 15 3295 509 18 22548 2753 11175 4 944 13476 21918 6 545 6 21 3051 7 69 6578 18 790 11 4300 5985 6 17142 9959 6 15 389 779 13668 77 79 9939 4 1405 809 21 423 303 11 10 882 593 7 69 184 4 1206 6192 21274 6 654 6 34 57 1340 19 1900 8 16 528 137 17142 9931 15596 423 4 2 2 1206 6192 21274 16 1238 9 519 9229 10 1816 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:31:17 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n","05/14/2020 19:31:19 - INFO - __main__ -   Saving features into cached file data/glue/RTE/cached_train_roberta_jumbled_token_discrimination_lr_e-4_prob_0.15_128_rte\n","05/14/2020 19:31:19 - INFO - __main__ -   ***** Running training *****\n","05/14/2020 19:31:19 - INFO - __main__ -     Num examples = 2490\n","05/14/2020 19:31:19 - INFO - __main__ -     Num Epochs = 3\n","05/14/2020 19:31:19 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n","05/14/2020 19:31:19 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n","05/14/2020 19:31:19 - INFO - __main__ -     Gradient Accumulation steps = 1\n","05/14/2020 19:31:19 - INFO - __main__ -     Total optimization steps = 117\n","05/14/2020 19:31:19 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n","05/14/2020 19:31:19 - INFO - __main__ -     Continuing training from epoch 0\n","05/14/2020 19:31:19 - INFO - __main__ -     Continuing training from global step 0\n","05/14/2020 19:31:19 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n","Epoch:   0% 0/3 [00:00<?, ?it/s]\n","Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha)\n","\n","Iteration:   3% 1/39 [00:02<01:43,  2.73s/it]\u001b[A\n","Iteration:   5% 2/39 [00:05<01:37,  2.63s/it]\u001b[A\n","Iteration:   8% 3/39 [00:07<01:32,  2.56s/it]\u001b[A\n","Iteration:  10% 4/39 [00:09<01:27,  2.51s/it]\u001b[A\n","Iteration:  13% 5/39 [00:12<01:24,  2.48s/it]\u001b[A\n","Iteration:  15% 6/39 [00:14<01:20,  2.45s/it]\u001b[A\n","Iteration:  18% 7/39 [00:17<01:17,  2.43s/it]\u001b[A\n","Iteration:  21% 8/39 [00:19<01:15,  2.43s/it]\u001b[A\n","Iteration:  23% 9/39 [00:21<01:12,  2.42s/it]\u001b[A\n","Iteration:  26% 10/39 [00:24<01:10,  2.42s/it]\u001b[A\n","Iteration:  28% 11/39 [00:26<01:07,  2.42s/it]\u001b[A\n","Iteration:  31% 12/39 [00:29<01:05,  2.42s/it]\u001b[A\n","Iteration:  33% 13/39 [00:31<01:02,  2.42s/it]\u001b[A\n","Iteration:  36% 14/39 [00:34<01:00,  2.42s/it]\u001b[A\n","Iteration:  38% 15/39 [00:36<00:58,  2.43s/it]\u001b[A\n","Iteration:  41% 16/39 [00:38<00:55,  2.43s/it]\u001b[A\n","Iteration:  44% 17/39 [00:41<00:53,  2.43s/it]\u001b[A\n","Iteration:  46% 18/39 [00:43<00:51,  2.43s/it]\u001b[A\n","Iteration:  49% 19/39 [00:46<00:48,  2.44s/it]\u001b[A\n","Iteration:  51% 20/39 [00:48<00:46,  2.44s/it]\u001b[A\n","Iteration:  54% 21/39 [00:51<00:43,  2.44s/it]\u001b[A\n","Iteration:  56% 22/39 [00:53<00:41,  2.44s/it]\u001b[A\n","Iteration:  59% 23/39 [00:56<00:39,  2.45s/it]\u001b[A\n","Iteration:  62% 24/39 [00:58<00:36,  2.45s/it]\u001b[A\n","Iteration:  64% 25/39 [01:00<00:34,  2.45s/it]\u001b[A\n","Iteration:  67% 26/39 [01:03<00:31,  2.45s/it]\u001b[A\n","Iteration:  69% 27/39 [01:05<00:29,  2.45s/it]\u001b[A\n","Iteration:  72% 28/39 [01:08<00:26,  2.45s/it]\u001b[A\n","Iteration:  74% 29/39 [01:10<00:24,  2.45s/it]\u001b[A\n","Iteration:  77% 30/39 [01:13<00:22,  2.45s/it]\u001b[A\n","Iteration:  79% 31/39 [01:15<00:19,  2.45s/it]\u001b[A\n","Iteration:  82% 32/39 [01:18<00:17,  2.45s/it]\u001b[A\n","Iteration:  85% 33/39 [01:20<00:14,  2.45s/it]\u001b[A\n","Iteration:  87% 34/39 [01:22<00:12,  2.44s/it]\u001b[A\n","Iteration:  90% 35/39 [01:25<00:09,  2.44s/it]\u001b[A\n","Iteration:  92% 36/39 [01:27<00:07,  2.44s/it]\u001b[A\n","Iteration:  95% 37/39 [01:30<00:04,  2.44s/it]\u001b[A\n","Iteration:  97% 38/39 [01:32<00:02,  2.44s/it]\u001b[A\n","Iteration: 100% 39/39 [01:34<00:00,  2.43s/it]\n","Epoch:  33% 1/3 [01:34<03:09, 94.94s/it]\n","Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A\n","Iteration:   3% 1/39 [00:02<01:32,  2.44s/it]\u001b[A\n","Iteration:   5% 2/39 [00:04<01:30,  2.44s/it]\u001b[A\n","Iteration:   8% 3/39 [00:07<01:27,  2.44s/it]\u001b[A\n","Iteration:  10% 4/39 [00:09<01:25,  2.44s/it]\u001b[A\n","Iteration:  13% 5/39 [00:12<01:22,  2.44s/it]\u001b[A\n","Iteration:  15% 6/39 [00:14<01:20,  2.44s/it]\u001b[A\n","Iteration:  18% 7/39 [00:17<01:18,  2.44s/it]\u001b[A\n","Iteration:  21% 8/39 [00:19<01:15,  2.44s/it]\u001b[A\n","Iteration:  23% 9/39 [00:21<01:13,  2.44s/it]\u001b[A\n","Iteration:  26% 10/39 [00:24<01:10,  2.44s/it]\u001b[A\n","Iteration:  28% 11/39 [00:26<01:08,  2.44s/it]\u001b[A\n","Iteration:  31% 12/39 [00:29<01:05,  2.44s/it]\u001b[A\n","Iteration:  33% 13/39 [00:31<01:03,  2.45s/it]\u001b[A\n","Iteration:  36% 14/39 [00:34<01:01,  2.44s/it]\u001b[A\n","Iteration:  38% 15/39 [00:36<00:58,  2.44s/it]\u001b[A\n","Iteration:  41% 16/39 [00:39<00:56,  2.44s/it]\u001b[A\n","Iteration:  44% 17/39 [00:41<00:53,  2.44s/it]\u001b[A\n","Iteration:  46% 18/39 [00:43<00:51,  2.44s/it]\u001b[A\n","Iteration:  49% 19/39 [00:46<00:48,  2.44s/it]\u001b[A\n","Iteration:  51% 20/39 [00:48<00:46,  2.45s/it]\u001b[A\n","Iteration:  54% 21/39 [00:51<00:44,  2.45s/it]\u001b[A\n","Iteration:  56% 22/39 [00:53<00:41,  2.45s/it]\u001b[A\n","Iteration:  59% 23/39 [00:56<00:39,  2.45s/it]\u001b[A\n","Iteration:  62% 24/39 [00:58<00:36,  2.44s/it]\u001b[A\n","Iteration:  64% 25/39 [01:01<00:34,  2.44s/it]\u001b[A\n","Iteration:  67% 26/39 [01:03<00:31,  2.44s/it]\u001b[A\n","Iteration:  69% 27/39 [01:05<00:29,  2.44s/it]\u001b[A\n","Iteration:  72% 28/39 [01:08<00:26,  2.44s/it]\u001b[A\n","Iteration:  74% 29/39 [01:10<00:24,  2.44s/it]\u001b[A\n","Iteration:  77% 30/39 [01:13<00:21,  2.44s/it]\u001b[A\n","Iteration:  79% 31/39 [01:15<00:19,  2.44s/it]\u001b[A\n","Iteration:  82% 32/39 [01:18<00:17,  2.43s/it]\u001b[A\n","Iteration:  85% 33/39 [01:20<00:14,  2.44s/it]\u001b[A\n","Iteration:  87% 34/39 [01:23<00:12,  2.44s/it]\u001b[A\n","Iteration:  90% 35/39 [01:25<00:09,  2.44s/it]\u001b[A\n","Iteration:  92% 36/39 [01:27<00:07,  2.44s/it]\u001b[A\n","Iteration:  95% 37/39 [01:30<00:04,  2.44s/it]\u001b[A\n","Iteration:  97% 38/39 [01:32<00:02,  2.44s/it]\u001b[A\n","Iteration: 100% 39/39 [01:34<00:00,  2.44s/it]\n","Epoch:  67% 2/3 [03:09<01:34, 94.95s/it]\n","Iteration:   0% 0/39 [00:00<?, ?it/s]\u001b[A\n","Iteration:   3% 1/39 [00:02<01:32,  2.44s/it]\u001b[A\n","Iteration:   5% 2/39 [00:04<01:30,  2.44s/it]\u001b[A\n","Iteration:   8% 3/39 [00:07<01:27,  2.44s/it]\u001b[A\n","Iteration:  10% 4/39 [00:09<01:25,  2.44s/it]\u001b[A\n","Iteration:  13% 5/39 [00:12<01:22,  2.44s/it]\u001b[A\n","Iteration:  15% 6/39 [00:14<01:20,  2.44s/it]\u001b[A\n","Iteration:  18% 7/39 [00:17<01:17,  2.44s/it]\u001b[A\n","Iteration:  21% 8/39 [00:19<01:15,  2.43s/it]\u001b[A\n","Iteration:  23% 9/39 [00:21<01:13,  2.44s/it]\u001b[A\n","Iteration:  26% 10/39 [00:24<01:10,  2.44s/it]\u001b[A\n","Iteration:  28% 11/39 [00:26<01:08,  2.44s/it]\u001b[A\n","Iteration:  31% 12/39 [00:29<01:05,  2.44s/it]\u001b[A\n","Iteration:  33% 13/39 [00:31<01:03,  2.44s/it]\u001b[A\n","Iteration:  36% 14/39 [00:34<01:00,  2.44s/it]\u001b[A\n","Iteration:  38% 15/39 [00:36<00:58,  2.44s/it]\u001b[A\n","Iteration:  41% 16/39 [00:39<00:56,  2.44s/it]\u001b[A\n","Iteration:  44% 17/39 [00:41<00:53,  2.44s/it]\u001b[A\n","Iteration:  46% 18/39 [00:43<00:51,  2.44s/it]\u001b[A\n","Iteration:  49% 19/39 [00:46<00:48,  2.44s/it]\u001b[A\n","Iteration:  51% 20/39 [00:48<00:46,  2.44s/it]\u001b[A\n","Iteration:  54% 21/39 [00:51<00:43,  2.44s/it]\u001b[A\n","Iteration:  56% 22/39 [00:53<00:41,  2.44s/it]\u001b[A\n","Iteration:  59% 23/39 [00:56<00:39,  2.44s/it]\u001b[A\n","Iteration:  62% 24/39 [00:58<00:36,  2.44s/it]\u001b[A\n","Iteration:  64% 25/39 [01:00<00:34,  2.44s/it]\u001b[A\n","Iteration:  67% 26/39 [01:03<00:31,  2.44s/it]\u001b[A\n","Iteration:  69% 27/39 [01:05<00:29,  2.44s/it]\u001b[A\n","Iteration:  72% 28/39 [01:08<00:26,  2.44s/it]\u001b[A\n","Iteration:  74% 29/39 [01:10<00:24,  2.44s/it]\u001b[A\n","Iteration:  77% 30/39 [01:13<00:21,  2.44s/it]\u001b[A\n","Iteration:  79% 31/39 [01:15<00:19,  2.44s/it]\u001b[A\n","Iteration:  82% 32/39 [01:18<00:17,  2.44s/it]\u001b[A\n","Iteration:  85% 33/39 [01:20<00:14,  2.44s/it]\u001b[A\n","Iteration:  87% 34/39 [01:22<00:12,  2.44s/it]\u001b[A\n","Iteration:  90% 35/39 [01:25<00:09,  2.44s/it]\u001b[A\n","Iteration:  92% 36/39 [01:27<00:07,  2.44s/it]\u001b[A\n","Iteration:  95% 37/39 [01:30<00:04,  2.44s/it]\u001b[A\n","Iteration:  97% 38/39 [01:32<00:02,  2.44s/it]\u001b[A\n","Iteration: 100% 39/39 [01:34<00:00,  2.43s/it]\n","Epoch: 100% 3/3 [04:44<00:00, 94.94s/it]\n","05/14/2020 19:36:04 - INFO - __main__ -    global_step = 117, average loss = 0.6303541410682548\n","05/14/2020 19:36:04 - INFO - __main__ -   Saving model checkpoint to run\n","05/14/2020 19:36:04 - INFO - transformers.configuration_utils -   Configuration saved in run/config.json\n","05/14/2020 19:36:06 - INFO - transformers.modeling_utils -   Model weights saved in run/pytorch_model.bin\n","05/14/2020 19:36:07 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n","05/14/2020 19:36:07 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"rte\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/14/2020 19:36:07 - INFO - transformers.modeling_utils -   loading weights file run/pytorch_model.bin\n","05/14/2020 19:36:13 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n","05/14/2020 19:36:13 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"rte\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   Model name 'run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'run' is a path, a model identifier, or url to a directory containing tokenizer files.\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   Didn't find file run/added_tokens.json. We won't load it.\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file run/vocab.json\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file run/merges.txt\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file None\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file run/special_tokens_map.json\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file run/tokenizer_config.json\n","05/14/2020 19:36:13 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n","05/14/2020 19:36:13 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"rte\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   Model name 'run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'run' is a path, a model identifier, or url to a directory containing tokenizer files.\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   Didn't find file run/added_tokens.json. We won't load it.\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file run/vocab.json\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file run/merges.txt\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file None\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file run/special_tokens_map.json\n","05/14/2020 19:36:13 - INFO - transformers.tokenization_utils -   loading file run/tokenizer_config.json\n","05/14/2020 19:36:13 - INFO - __main__ -   Evaluate the following checkpoints: ['run']\n","05/14/2020 19:36:13 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n","05/14/2020 19:36:13 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n","  \"_num_labels\": 2,\n","  \"architectures\": [\n","    \"RobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bad_words_ids\": null,\n","  \"bos_token_id\": 0,\n","  \"decoder_start_token_id\": null,\n","  \"do_sample\": false,\n","  \"early_stopping\": false,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"rte\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"is_encoder_decoder\": false,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"length_penalty\": 1.0,\n","  \"max_length\": 20,\n","  \"max_position_embeddings\": 514,\n","  \"min_length\": 0,\n","  \"model_type\": \"roberta\",\n","  \"no_repeat_ngram_size\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_beams\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_return_sequences\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": null,\n","  \"pruned_heads\": {},\n","  \"repetition_penalty\": 1.0,\n","  \"task_specific_params\": null,\n","  \"temperature\": 1.0,\n","  \"top_k\": 50,\n","  \"top_p\": 1.0,\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 1,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 50265\n","}\n","\n","05/14/2020 19:36:13 - INFO - transformers.modeling_utils -   loading weights file run/pytorch_model.bin\n","05/14/2020 19:36:20 - INFO - __main__ -   Creating features from dataset file at data/glue/RTE\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   Writing example 0/277\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   guid: dev-0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   input_ids: 0 11014 25459 548 6 5 18141 9 5 2701 5469 25459 548 6 34 962 9 10665 1668 23 1046 3550 6 309 7 5 5469 25459 548 2475 4 2 2 5469 25459 548 56 41 3213 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   guid: dev-1\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   input_ids: 0 3507 6 52 122 32 18152 14 16674 32 2086 49 12833 136 5467 4 11817 12 3245 10928 9436 32 16119 1295 3845 87 52 64 283 62 19 92 16674 7 1032 5 92 18746 4 2 2 163 42069 16 1298 5 997 136 16674 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   guid: dev-2\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   input_ids: 0 14794 16 122 184 7 103 379 153 82 111 10 24357 1956 14 9108 2219 158 6 151 5657 9 22051 228 183 6 2057 41 7934 8793 15 285 518 4 96 5 375 158 107 6 5 168 34 1381 543 7 3803 940 915 11 5 10383 1293 6 53 103 3278 204 6 151 5657 9 3844 16 314 639 358 183 6 856 8939 154 11 5 2859 25 24 23120 13 951 7 699 24 62 4 85 16 747 5 82 11 5 19125 26075 14 32 2373 2132 4 125 11 103 911 51 32 2190 124 4 96 840 1792 763 6 65 2 2 379 153 5657 9 22051 32 2622 1230 11 14794 4 2\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   guid: dev-3\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   input_ids: 0 20 1918 1173 435 11 4367 6 61 1530 59 3490 6 151 6 1074 41 5951 338 9063 6339 6 1481 26586 9874 9766 101 4382 8 27596 4 178 171 224 49 7540 8244 6339 2029 106 10 1472 14 51 32 4371 31 5 1476 9 470 2313 4 125 25 1196 4366 583 5 334 6 103 2498 2065 15475 428 8 7789 11 5253 12 31798 10306 44279 6 51 26 14 1472 9 1078 56 57 17306 4 22 1106 951 11803 8 1072 7 109 402 12103 6 89 18 117 4472 14 18 164 7 912 106 60 26 6469 1745 6 4772 2 2 4367 34 5 934 1918 1173 435 11 5 121 4 104 4 2\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   label: not_entailment (id = 1)\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   *** Example ***\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   guid: dev-4\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   input_ids: 0 2010 1572 58 15 239 5439 71 41 729 637 11 61 55 87 112 6 151 82 6 217 707 729 2261 6 33 57 848 4 2 2 2010 1572 58 15 239 5439 71 10 637 4401 2050 30 1476 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","05/14/2020 19:36:20 - INFO - transformers.data.processors.glue -   label: entailment (id = 0)\n","05/14/2020 19:36:21 - INFO - __main__ -   Saving features into cached file data/glue/RTE/cached_dev_roberta_jumbled_token_discrimination_lr_e-4_prob_0.15_128_rte\n","05/14/2020 19:36:21 - INFO - __main__ -   ***** Running evaluation  *****\n","05/14/2020 19:36:21 - INFO - __main__ -     Num examples = 277\n","05/14/2020 19:36:21 - INFO - __main__ -     Batch size = 64\n","Evaluating: 100% 5/5 [00:03<00:00,  1.28it/s]\n","05/14/2020 19:36:25 - INFO - __main__ -   ***** Eval results  *****\n","05/14/2020 19:36:25 - INFO - __main__ -     acc = 0.703971119133574\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_2xmVAZiqiCK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}