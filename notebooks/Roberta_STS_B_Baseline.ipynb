{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roberta-STS-B-Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeaXI_L0zrsn",
        "colab_type": "code",
        "outputId": "e42fbdcc-7f1c-41c7-b4d8-79d7c8517cc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "  #!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8kGpsyo0ByJ",
        "colab_type": "code",
        "outputId": "b3321c6c-22a5-43f4-9eb5-ad6b829d3bff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/transformers"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC80P8b_0FTi",
        "colab_type": "code",
        "outputId": "5e00b377-bba9-46b4-d73a-ad2285a26cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r ./examples/requirements.txt\n",
        "!pip install boto3 filelock requests tqdm sentencepiece sacremoses tokenizers"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 1)) (2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 4)) (0.0.12)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 5)) (5.4.8)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 6)) (1.4.6)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 7)) (0.0.3)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.27.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (46.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.6.0.post2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.2.5)\n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (0.996.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (1.6.0)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (3.6.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (19.3.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.21.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (4.38.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.51.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.12.33)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.38)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.15.33)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7Q3nMTc0GUS",
        "colab_type": "code",
        "outputId": "47994559-608c-4a77-96b6-65c63cb93167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "GLUE_DIR=\"content/\"\n",
        "!python ./utils/download_glue_data.py --data_dir $GLUE_DIR --tasks STS"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting STS...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb7T3qct0Rsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TASK_NAME=\"STS-B\"\n",
        "GLUE_DIR=\"content/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd11nN8h0TVI",
        "colab_type": "code",
        "outputId": "68426ac9-a08b-4c12-b0f8-43ee70d8f89e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%set_env PYTHONPATH=/content/transformers/src:/"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=/content/transformers/src:/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkRlpVLl0U3U",
        "colab_type": "code",
        "outputId": "91a506b8-946f-4f67-c5a8-3e42998fc3d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python ./examples/run_glue.py \\\n",
        "    --model_type roberta \\\n",
        "    --model_name_or_path roberta-base \\\n",
        "    --task_name $TASK_NAME \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=64   \\\n",
        "    --per_gpu_train_batch_size=64   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --output_dir ../${TASK_NAME}_run \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 15:25:49.013045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "04/05/2020 15:25:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/05/2020 15:25:51 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "04/05/2020 15:25:51 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 1,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sts-b\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 15:25:51 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "04/05/2020 15:25:51 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 15:25:52 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "04/05/2020 15:25:52 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "04/05/2020 15:25:52 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "04/05/2020 15:25:56 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "04/05/2020 15:25:56 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "04/05/2020 15:26:00 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='content//STS-B', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='../-B_run', output_mode='regression', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='sts-b', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "04/05/2020 15:26:00 - INFO - __main__ -   Creating features from dataset file at content//STS-B\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   Writing example 0/5749\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   guid: train-0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   input_ids: 0 83 3286 16 602 160 4 2 2 660 935 3286 16 602 160 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   label: 5.000 (id = 5)\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   input_ids: 0 83 313 16 816 10 739 2342 4467 4 2 2 83 313 16 816 10 2342 4467 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   label: 3.800 (id = 3)\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   input_ids: 0 83 313 16 9592 27373 196 7134 15 10 9366 4 2 2 83 313 16 9592 30274 7134 15 41 16511 30134 9366 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   label: 3.800 (id = 3)\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   input_ids: 0 2873 604 32 816 25109 4 2 2 1596 604 32 816 25109 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   label: 2.600 (id = 2)\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   input_ids: 0 83 313 16 816 5 3551 139 4 2 2 83 313 17630 16 816 5 3551 139 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:26:01 - INFO - transformers.data.processors.glue -   label: 4.250 (id = 4)\n",
            "04/05/2020 15:26:02 - INFO - __main__ -   Saving features into cached file content//STS-B/cached_train_roberta-base_128_sts-b\n",
            "04/05/2020 15:26:03 - INFO - __main__ -   ***** Running training *****\n",
            "04/05/2020 15:26:03 - INFO - __main__ -     Num examples = 5749\n",
            "04/05/2020 15:26:03 - INFO - __main__ -     Num Epochs = 3\n",
            "04/05/2020 15:26:03 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n",
            "04/05/2020 15:26:03 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "04/05/2020 15:26:03 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "04/05/2020 15:26:03 - INFO - __main__ -     Total optimization steps = 270\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/90 [00:01<02:04,  1.40s/it]\u001b[A\n",
            "Iteration:   2% 2/90 [00:02<01:56,  1.33s/it]\u001b[A\n",
            "Iteration:   3% 3/90 [00:03<01:50,  1.27s/it]\u001b[A\n",
            "Iteration:   4% 4/90 [00:04<01:46,  1.24s/it]\u001b[A\n",
            "Iteration:   6% 5/90 [00:06<01:43,  1.21s/it]\u001b[A\n",
            "Iteration:   7% 6/90 [00:07<01:40,  1.19s/it]\u001b[A\n",
            "Iteration:   8% 7/90 [00:08<01:38,  1.18s/it]\u001b[A\n",
            "Iteration:   9% 8/90 [00:09<01:36,  1.18s/it]\u001b[A\n",
            "Iteration:  10% 9/90 [00:10<01:34,  1.17s/it]\u001b[A\n",
            "Iteration:  11% 10/90 [00:11<01:33,  1.17s/it]\u001b[A\n",
            "Iteration:  12% 11/90 [00:12<01:31,  1.16s/it]\u001b[A\n",
            "Iteration:  13% 12/90 [00:14<01:30,  1.16s/it]\u001b[A\n",
            "Iteration:  14% 13/90 [00:15<01:29,  1.16s/it]\u001b[A\n",
            "Iteration:  16% 14/90 [00:16<01:28,  1.16s/it]\u001b[A\n",
            "Iteration:  17% 15/90 [00:17<01:27,  1.16s/it]\u001b[A\n",
            "Iteration:  18% 16/90 [00:18<01:26,  1.17s/it]\u001b[A\n",
            "Iteration:  19% 17/90 [00:19<01:25,  1.17s/it]\u001b[A\n",
            "Iteration:  20% 18/90 [00:21<01:24,  1.17s/it]\u001b[A\n",
            "Iteration:  21% 19/90 [00:22<01:23,  1.17s/it]\u001b[A\n",
            "Iteration:  22% 20/90 [00:23<01:22,  1.17s/it]\u001b[A\n",
            "Iteration:  23% 21/90 [00:24<01:21,  1.18s/it]\u001b[A\n",
            "Iteration:  24% 22/90 [00:25<01:20,  1.18s/it]\u001b[A\n",
            "Iteration:  26% 23/90 [00:27<01:19,  1.18s/it]\u001b[A\n",
            "Iteration:  27% 24/90 [00:28<01:18,  1.18s/it]\u001b[A\n",
            "Iteration:  28% 25/90 [00:29<01:17,  1.18s/it]\u001b[A\n",
            "Iteration:  29% 26/90 [00:30<01:15,  1.19s/it]\u001b[A\n",
            "Iteration:  30% 27/90 [00:31<01:14,  1.19s/it]\u001b[A\n",
            "Iteration:  31% 28/90 [00:32<01:13,  1.19s/it]\u001b[A\n",
            "Iteration:  32% 29/90 [00:34<01:12,  1.19s/it]\u001b[A\n",
            "Iteration:  33% 30/90 [00:35<01:11,  1.19s/it]\u001b[A\n",
            "Iteration:  34% 31/90 [00:36<01:10,  1.19s/it]\u001b[A\n",
            "Iteration:  36% 32/90 [00:37<01:09,  1.20s/it]\u001b[A\n",
            "Iteration:  37% 33/90 [00:38<01:08,  1.20s/it]\u001b[A\n",
            "Iteration:  38% 34/90 [00:40<01:07,  1.20s/it]\u001b[A\n",
            "Iteration:  39% 35/90 [00:41<01:06,  1.20s/it]\u001b[A\n",
            "Iteration:  40% 36/90 [00:42<01:05,  1.21s/it]\u001b[A\n",
            "Iteration:  41% 37/90 [00:43<01:04,  1.21s/it]\u001b[A\n",
            "Iteration:  42% 38/90 [00:45<01:02,  1.21s/it]\u001b[A\n",
            "Iteration:  43% 39/90 [00:46<01:01,  1.21s/it]\u001b[A\n",
            "Iteration:  44% 40/90 [00:47<01:00,  1.21s/it]\u001b[A\n",
            "Iteration:  46% 41/90 [00:48<00:59,  1.22s/it]\u001b[A\n",
            "Iteration:  47% 42/90 [00:49<00:58,  1.22s/it]\u001b[A\n",
            "Iteration:  48% 43/90 [00:51<00:57,  1.22s/it]\u001b[A\n",
            "Iteration:  49% 44/90 [00:52<00:56,  1.23s/it]\u001b[A\n",
            "Iteration:  50% 45/90 [00:53<00:55,  1.23s/it]\u001b[A\n",
            "Iteration:  51% 46/90 [00:54<00:54,  1.23s/it]\u001b[A\n",
            "Iteration:  52% 47/90 [00:56<00:53,  1.24s/it]\u001b[A\n",
            "Iteration:  53% 48/90 [00:57<00:52,  1.24s/it]\u001b[A\n",
            "Iteration:  54% 49/90 [00:58<00:51,  1.25s/it]\u001b[A\n",
            "Iteration:  56% 50/90 [00:59<00:50,  1.25s/it]\u001b[A\n",
            "Iteration:  57% 51/90 [01:01<00:48,  1.25s/it]\u001b[A\n",
            "Iteration:  58% 52/90 [01:02<00:47,  1.25s/it]\u001b[A\n",
            "Iteration:  59% 53/90 [01:03<00:46,  1.25s/it]\u001b[A\n",
            "Iteration:  60% 54/90 [01:04<00:45,  1.26s/it]\u001b[A\n",
            "Iteration:  61% 55/90 [01:06<00:43,  1.26s/it]\u001b[A\n",
            "Iteration:  62% 56/90 [01:07<00:42,  1.26s/it]\u001b[A\n",
            "Iteration:  63% 57/90 [01:08<00:41,  1.26s/it]\u001b[A\n",
            "Iteration:  64% 58/90 [01:09<00:40,  1.27s/it]\u001b[A\n",
            "Iteration:  66% 59/90 [01:11<00:39,  1.27s/it]\u001b[A\n",
            "Iteration:  67% 60/90 [01:12<00:38,  1.27s/it]\u001b[A\n",
            "Iteration:  68% 61/90 [01:13<00:37,  1.28s/it]\u001b[A\n",
            "Iteration:  69% 62/90 [01:15<00:35,  1.28s/it]\u001b[A\n",
            "Iteration:  70% 63/90 [01:16<00:34,  1.29s/it]\u001b[A\n",
            "Iteration:  71% 64/90 [01:17<00:33,  1.29s/it]\u001b[A\n",
            "Iteration:  72% 65/90 [01:19<00:32,  1.29s/it]\u001b[A\n",
            "Iteration:  73% 66/90 [01:20<00:31,  1.30s/it]\u001b[A\n",
            "Iteration:  74% 67/90 [01:21<00:29,  1.30s/it]\u001b[A\n",
            "Iteration:  76% 68/90 [01:22<00:28,  1.30s/it]\u001b[A\n",
            "Iteration:  77% 69/90 [01:24<00:27,  1.31s/it]\u001b[A\n",
            "Iteration:  78% 70/90 [01:25<00:26,  1.31s/it]\u001b[A\n",
            "Iteration:  79% 71/90 [01:26<00:24,  1.31s/it]\u001b[A\n",
            "Iteration:  80% 72/90 [01:28<00:23,  1.32s/it]\u001b[A\n",
            "Iteration:  81% 73/90 [01:29<00:22,  1.32s/it]\u001b[A\n",
            "Iteration:  82% 74/90 [01:30<00:21,  1.32s/it]\u001b[A\n",
            "Iteration:  83% 75/90 [01:32<00:19,  1.33s/it]\u001b[A\n",
            "Iteration:  84% 76/90 [01:33<00:18,  1.33s/it]\u001b[A\n",
            "Iteration:  86% 77/90 [01:34<00:17,  1.33s/it]\u001b[A\n",
            "Iteration:  87% 78/90 [01:36<00:15,  1.33s/it]\u001b[A\n",
            "Iteration:  88% 79/90 [01:37<00:14,  1.33s/it]\u001b[A\n",
            "Iteration:  89% 80/90 [01:38<00:13,  1.33s/it]\u001b[A\n",
            "Iteration:  90% 81/90 [01:40<00:11,  1.33s/it]\u001b[A\n",
            "Iteration:  91% 82/90 [01:41<00:10,  1.33s/it]\u001b[A\n",
            "Iteration:  92% 83/90 [01:42<00:09,  1.33s/it]\u001b[A\n",
            "Iteration:  93% 84/90 [01:44<00:07,  1.32s/it]\u001b[A\n",
            "Iteration:  94% 85/90 [01:45<00:06,  1.32s/it]\u001b[A\n",
            "Iteration:  96% 86/90 [01:46<00:05,  1.32s/it]\u001b[A\n",
            "Iteration:  97% 87/90 [01:48<00:03,  1.32s/it]\u001b[A\n",
            "Iteration:  98% 88/90 [01:49<00:02,  1.31s/it]\u001b[A\n",
            "Iteration:  99% 89/90 [01:50<00:01,  1.31s/it]\u001b[A\n",
            "Iteration: 100% 90/90 [01:51<00:00,  1.24s/it]\n",
            "Epoch:  33% 1/3 [01:51<03:43, 111.82s/it]\n",
            "Iteration:   0% 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/90 [00:01<01:55,  1.30s/it]\u001b[A\n",
            "Iteration:   2% 2/90 [00:02<01:54,  1.30s/it]\u001b[A\n",
            "Iteration:   3% 3/90 [00:03<01:53,  1.30s/it]\u001b[A\n",
            "Iteration:   4% 4/90 [00:05<01:51,  1.30s/it]\u001b[A\n",
            "Iteration:   6% 5/90 [00:06<01:50,  1.30s/it]\u001b[A\n",
            "Iteration:   7% 6/90 [00:07<01:48,  1.30s/it]\u001b[A\n",
            "Iteration:   8% 7/90 [00:09<01:47,  1.30s/it]\u001b[A\n",
            "Iteration:   9% 8/90 [00:10<01:46,  1.29s/it]\u001b[A\n",
            "Iteration:  10% 9/90 [00:11<01:44,  1.29s/it]\u001b[A\n",
            "Iteration:  11% 10/90 [00:12<01:43,  1.29s/it]\u001b[A\n",
            "Iteration:  12% 11/90 [00:14<01:42,  1.29s/it]\u001b[A\n",
            "Iteration:  13% 12/90 [00:15<01:40,  1.29s/it]\u001b[A\n",
            "Iteration:  14% 13/90 [00:16<01:39,  1.29s/it]\u001b[A\n",
            "Iteration:  16% 14/90 [00:18<01:37,  1.29s/it]\u001b[A\n",
            "Iteration:  17% 15/90 [00:19<01:36,  1.28s/it]\u001b[A\n",
            "Iteration:  18% 16/90 [00:20<01:34,  1.28s/it]\u001b[A\n",
            "Iteration:  19% 17/90 [00:21<01:33,  1.28s/it]\u001b[A\n",
            "Iteration:  20% 18/90 [00:23<01:32,  1.28s/it]\u001b[A\n",
            "Iteration:  21% 19/90 [00:24<01:31,  1.28s/it]\u001b[A\n",
            "Iteration:  22% 20/90 [00:25<01:29,  1.28s/it]\u001b[A\n",
            "Iteration:  23% 21/90 [00:27<01:28,  1.28s/it]\u001b[A\n",
            "Iteration:  24% 22/90 [00:28<01:27,  1.28s/it]\u001b[A\n",
            "Iteration:  26% 23/90 [00:29<01:25,  1.28s/it]\u001b[A\n",
            "Iteration:  27% 24/90 [00:30<01:24,  1.28s/it]\u001b[A\n",
            "Iteration:  28% 25/90 [00:32<01:23,  1.28s/it]\u001b[A\n",
            "Iteration:  29% 26/90 [00:33<01:22,  1.28s/it]\u001b[A\n",
            "Iteration:  30% 27/90 [00:34<01:20,  1.28s/it]\u001b[A\n",
            "Iteration:  31% 28/90 [00:36<01:19,  1.28s/it]\u001b[A\n",
            "Iteration:  32% 29/90 [00:37<01:18,  1.28s/it]\u001b[A\n",
            "Iteration:  33% 30/90 [00:38<01:16,  1.28s/it]\u001b[A\n",
            "Iteration:  34% 31/90 [00:39<01:15,  1.28s/it]\u001b[A\n",
            "Iteration:  36% 32/90 [00:41<01:14,  1.28s/it]\u001b[A\n",
            "Iteration:  37% 33/90 [00:42<01:13,  1.29s/it]\u001b[A\n",
            "Iteration:  38% 34/90 [00:43<01:12,  1.29s/it]\u001b[A\n",
            "Iteration:  39% 35/90 [00:45<01:10,  1.29s/it]\u001b[A\n",
            "Iteration:  40% 36/90 [00:46<01:09,  1.29s/it]\u001b[A\n",
            "Iteration:  41% 37/90 [00:47<01:08,  1.29s/it]\u001b[A\n",
            "Iteration:  42% 38/90 [00:48<01:07,  1.29s/it]\u001b[A\n",
            "Iteration:  43% 39/90 [00:50<01:05,  1.29s/it]\u001b[A\n",
            "Iteration:  44% 40/90 [00:51<01:04,  1.29s/it]\u001b[A\n",
            "Iteration:  46% 41/90 [00:52<01:03,  1.29s/it]\u001b[A\n",
            "Iteration:  47% 42/90 [00:54<01:02,  1.29s/it]\u001b[A\n",
            "Iteration:  48% 43/90 [00:55<01:00,  1.29s/it]\u001b[A\n",
            "Iteration:  49% 44/90 [00:56<00:59,  1.30s/it]\u001b[A\n",
            "Iteration:  50% 45/90 [00:57<00:58,  1.30s/it]\u001b[A\n",
            "Iteration:  51% 46/90 [00:59<00:57,  1.30s/it]\u001b[A\n",
            "Iteration:  52% 47/90 [01:00<00:55,  1.30s/it]\u001b[A\n",
            "Iteration:  53% 48/90 [01:01<00:54,  1.30s/it]\u001b[A\n",
            "Iteration:  54% 49/90 [01:03<00:53,  1.30s/it]\u001b[A\n",
            "Iteration:  56% 50/90 [01:04<00:51,  1.30s/it]\u001b[A\n",
            "Iteration:  57% 51/90 [01:05<00:50,  1.30s/it]\u001b[A\n",
            "Iteration:  58% 52/90 [01:07<00:49,  1.30s/it]\u001b[A\n",
            "Iteration:  59% 53/90 [01:08<00:48,  1.30s/it]\u001b[A\n",
            "Iteration:  60% 54/90 [01:09<00:46,  1.30s/it]\u001b[A\n",
            "Iteration:  61% 55/90 [01:10<00:45,  1.30s/it]\u001b[A\n",
            "Iteration:  62% 56/90 [01:12<00:44,  1.30s/it]\u001b[A\n",
            "Iteration:  63% 57/90 [01:13<00:43,  1.31s/it]\u001b[A\n",
            "Iteration:  64% 58/90 [01:14<00:41,  1.31s/it]\u001b[A\n",
            "Iteration:  66% 59/90 [01:16<00:40,  1.30s/it]\u001b[A\n",
            "Iteration:  67% 60/90 [01:17<00:39,  1.30s/it]\u001b[A\n",
            "Iteration:  68% 61/90 [01:18<00:37,  1.30s/it]\u001b[A\n",
            "Iteration:  69% 62/90 [01:20<00:36,  1.30s/it]\u001b[A\n",
            "Iteration:  70% 63/90 [01:21<00:35,  1.30s/it]\u001b[A\n",
            "Iteration:  71% 64/90 [01:22<00:33,  1.30s/it]\u001b[A\n",
            "Iteration:  72% 65/90 [01:24<00:32,  1.30s/it]\u001b[A\n",
            "Iteration:  73% 66/90 [01:25<00:31,  1.30s/it]\u001b[A\n",
            "Iteration:  74% 67/90 [01:26<00:29,  1.30s/it]\u001b[A\n",
            "Iteration:  76% 68/90 [01:27<00:28,  1.30s/it]\u001b[A\n",
            "Iteration:  77% 69/90 [01:29<00:27,  1.30s/it]\u001b[A\n",
            "Iteration:  78% 70/90 [01:30<00:25,  1.30s/it]\u001b[A\n",
            "Iteration:  79% 71/90 [01:31<00:24,  1.30s/it]\u001b[A\n",
            "Iteration:  80% 72/90 [01:33<00:23,  1.30s/it]\u001b[A\n",
            "Iteration:  81% 73/90 [01:34<00:22,  1.30s/it]\u001b[A\n",
            "Iteration:  82% 74/90 [01:35<00:20,  1.30s/it]\u001b[A\n",
            "Iteration:  83% 75/90 [01:36<00:19,  1.30s/it]\u001b[A\n",
            "Iteration:  84% 76/90 [01:38<00:18,  1.29s/it]\u001b[A\n",
            "Iteration:  86% 77/90 [01:39<00:16,  1.30s/it]\u001b[A\n",
            "Iteration:  87% 78/90 [01:40<00:15,  1.30s/it]\u001b[A\n",
            "Iteration:  88% 79/90 [01:42<00:14,  1.30s/it]\u001b[A\n",
            "Iteration:  89% 80/90 [01:43<00:12,  1.30s/it]\u001b[A\n",
            "Iteration:  90% 81/90 [01:44<00:11,  1.30s/it]\u001b[A\n",
            "Iteration:  91% 82/90 [01:46<00:10,  1.30s/it]\u001b[A\n",
            "Iteration:  92% 83/90 [01:47<00:09,  1.30s/it]\u001b[A\n",
            "Iteration:  93% 84/90 [01:48<00:07,  1.30s/it]\u001b[A\n",
            "Iteration:  94% 85/90 [01:49<00:06,  1.30s/it]\u001b[A\n",
            "Iteration:  96% 86/90 [01:51<00:05,  1.30s/it]\u001b[A\n",
            "Iteration:  97% 87/90 [01:52<00:03,  1.30s/it]\u001b[A\n",
            "Iteration:  98% 88/90 [01:53<00:02,  1.30s/it]\u001b[A\n",
            "Iteration:  99% 89/90 [01:55<00:01,  1.30s/it]\u001b[A\n",
            "Iteration: 100% 90/90 [01:56<00:00,  1.29s/it]\n",
            "Epoch:  67% 2/3 [03:48<01:53, 113.14s/it]\n",
            "Iteration:   0% 0/90 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/90 [00:01<01:55,  1.30s/it]\u001b[A\n",
            "Iteration:   2% 2/90 [00:02<01:53,  1.30s/it]\u001b[A\n",
            "Iteration:   3% 3/90 [00:03<01:52,  1.29s/it]\u001b[A\n",
            "Iteration:   4% 4/90 [00:05<01:51,  1.30s/it]\u001b[A\n",
            "Iteration:   6% 5/90 [00:06<01:50,  1.30s/it]\u001b[A\n",
            "Iteration:   7% 6/90 [00:07<01:48,  1.30s/it]\u001b[A\n",
            "Iteration:   8% 7/90 [00:09<01:47,  1.30s/it]\u001b[A\n",
            "Iteration:   9% 8/90 [00:10<01:46,  1.30s/it]\u001b[A\n",
            "Iteration:  10% 9/90 [00:11<01:45,  1.30s/it]\u001b[A\n",
            "Iteration:  11% 10/90 [00:12<01:43,  1.30s/it]\u001b[A\n",
            "Iteration:  12% 11/90 [00:14<01:42,  1.30s/it]\u001b[A\n",
            "Iteration:  13% 12/90 [00:15<01:41,  1.30s/it]\u001b[A\n",
            "Iteration:  14% 13/90 [00:16<01:39,  1.30s/it]\u001b[A\n",
            "Iteration:  16% 14/90 [00:18<01:38,  1.29s/it]\u001b[A\n",
            "Iteration:  17% 15/90 [00:19<01:36,  1.29s/it]\u001b[A\n",
            "Iteration:  18% 16/90 [00:20<01:35,  1.30s/it]\u001b[A\n",
            "Iteration:  19% 17/90 [00:22<01:34,  1.29s/it]\u001b[A\n",
            "Iteration:  20% 18/90 [00:23<01:33,  1.29s/it]\u001b[A\n",
            "Iteration:  21% 19/90 [00:24<01:32,  1.30s/it]\u001b[A\n",
            "Iteration:  22% 20/90 [00:25<01:30,  1.30s/it]\u001b[A\n",
            "Iteration:  23% 21/90 [00:27<01:29,  1.30s/it]\u001b[A\n",
            "Iteration:  24% 22/90 [00:28<01:28,  1.30s/it]\u001b[A\n",
            "Iteration:  26% 23/90 [00:29<01:26,  1.30s/it]\u001b[A\n",
            "Iteration:  27% 24/90 [00:31<01:25,  1.30s/it]\u001b[A\n",
            "Iteration:  28% 25/90 [00:32<01:24,  1.30s/it]\u001b[A\n",
            "Iteration:  29% 26/90 [00:33<01:22,  1.30s/it]\u001b[A\n",
            "Iteration:  30% 27/90 [00:35<01:21,  1.30s/it]\u001b[A\n",
            "Iteration:  31% 28/90 [00:36<01:20,  1.30s/it]\u001b[A\n",
            "Iteration:  32% 29/90 [00:37<01:19,  1.30s/it]\u001b[A\n",
            "Iteration:  33% 30/90 [00:38<01:17,  1.30s/it]\u001b[A\n",
            "Iteration:  34% 31/90 [00:40<01:16,  1.30s/it]\u001b[A\n",
            "Iteration:  36% 32/90 [00:41<01:15,  1.30s/it]\u001b[A\n",
            "Iteration:  37% 33/90 [00:42<01:13,  1.30s/it]\u001b[A\n",
            "Iteration:  38% 34/90 [00:44<01:12,  1.30s/it]\u001b[A\n",
            "Iteration:  39% 35/90 [00:45<01:11,  1.30s/it]\u001b[A\n",
            "Iteration:  40% 36/90 [00:46<01:09,  1.30s/it]\u001b[A\n",
            "Iteration:  41% 37/90 [00:47<01:08,  1.30s/it]\u001b[A\n",
            "Iteration:  42% 38/90 [00:49<01:07,  1.30s/it]\u001b[A\n",
            "Iteration:  43% 39/90 [00:50<01:06,  1.30s/it]\u001b[A\n",
            "Iteration:  44% 40/90 [00:51<01:04,  1.30s/it]\u001b[A\n",
            "Iteration:  46% 41/90 [00:53<01:03,  1.30s/it]\u001b[A\n",
            "Iteration:  47% 42/90 [00:54<01:02,  1.30s/it]\u001b[A\n",
            "Iteration:  48% 43/90 [00:55<01:01,  1.30s/it]\u001b[A\n",
            "Iteration:  49% 44/90 [00:57<00:59,  1.30s/it]\u001b[A\n",
            "Iteration:  50% 45/90 [00:58<00:58,  1.30s/it]\u001b[A\n",
            "Iteration:  51% 46/90 [00:59<00:57,  1.30s/it]\u001b[A\n",
            "Iteration:  52% 47/90 [01:00<00:55,  1.30s/it]\u001b[A\n",
            "Iteration:  53% 48/90 [01:02<00:54,  1.30s/it]\u001b[A\n",
            "Iteration:  54% 49/90 [01:03<00:53,  1.29s/it]\u001b[A\n",
            "Iteration:  56% 50/90 [01:04<00:51,  1.30s/it]\u001b[A\n",
            "Iteration:  57% 51/90 [01:06<00:50,  1.30s/it]\u001b[A\n",
            "Iteration:  58% 52/90 [01:07<00:49,  1.30s/it]\u001b[A\n",
            "Iteration:  59% 53/90 [01:08<00:48,  1.30s/it]\u001b[A\n",
            "Iteration:  60% 54/90 [01:10<00:46,  1.30s/it]\u001b[A\n",
            "Iteration:  61% 55/90 [01:11<00:45,  1.30s/it]\u001b[A\n",
            "Iteration:  62% 56/90 [01:12<00:44,  1.30s/it]\u001b[A\n",
            "Iteration:  63% 57/90 [01:13<00:42,  1.30s/it]\u001b[A\n",
            "Iteration:  64% 58/90 [01:15<00:41,  1.30s/it]\u001b[A\n",
            "Iteration:  66% 59/90 [01:16<00:40,  1.30s/it]\u001b[A\n",
            "Iteration:  67% 60/90 [01:17<00:38,  1.30s/it]\u001b[A\n",
            "Iteration:  68% 61/90 [01:19<00:37,  1.30s/it]\u001b[A\n",
            "Iteration:  69% 62/90 [01:20<00:36,  1.30s/it]\u001b[A\n",
            "Iteration:  70% 63/90 [01:21<00:35,  1.30s/it]\u001b[A\n",
            "Iteration:  71% 64/90 [01:22<00:33,  1.30s/it]\u001b[A\n",
            "Iteration:  72% 65/90 [01:24<00:32,  1.30s/it]\u001b[A\n",
            "Iteration:  73% 66/90 [01:25<00:31,  1.30s/it]\u001b[A\n",
            "Iteration:  74% 67/90 [01:26<00:29,  1.30s/it]\u001b[A\n",
            "Iteration:  76% 68/90 [01:28<00:28,  1.30s/it]\u001b[A\n",
            "Iteration:  77% 69/90 [01:29<00:27,  1.30s/it]\u001b[A\n",
            "Iteration:  78% 70/90 [01:30<00:25,  1.30s/it]\u001b[A\n",
            "Iteration:  79% 71/90 [01:32<00:24,  1.30s/it]\u001b[A\n",
            "Iteration:  80% 72/90 [01:33<00:23,  1.30s/it]\u001b[A\n",
            "Iteration:  81% 73/90 [01:34<00:22,  1.30s/it]\u001b[A\n",
            "Iteration:  82% 74/90 [01:35<00:20,  1.30s/it]\u001b[A\n",
            "Iteration:  83% 75/90 [01:37<00:19,  1.30s/it]\u001b[A\n",
            "Iteration:  84% 76/90 [01:38<00:18,  1.30s/it]\u001b[A\n",
            "Iteration:  86% 77/90 [01:39<00:16,  1.30s/it]\u001b[A\n",
            "Iteration:  87% 78/90 [01:41<00:15,  1.30s/it]\u001b[A\n",
            "Iteration:  88% 79/90 [01:42<00:14,  1.30s/it]\u001b[A\n",
            "Iteration:  89% 80/90 [01:43<00:12,  1.30s/it]\u001b[A\n",
            "Iteration:  90% 81/90 [01:45<00:11,  1.30s/it]\u001b[A\n",
            "Iteration:  91% 82/90 [01:46<00:10,  1.30s/it]\u001b[A\n",
            "Iteration:  92% 83/90 [01:47<00:09,  1.30s/it]\u001b[A\n",
            "Iteration:  93% 84/90 [01:48<00:07,  1.30s/it]\u001b[A\n",
            "Iteration:  94% 85/90 [01:50<00:06,  1.30s/it]\u001b[A\n",
            "Iteration:  96% 86/90 [01:51<00:05,  1.30s/it]\u001b[A\n",
            "Iteration:  97% 87/90 [01:52<00:03,  1.30s/it]\u001b[A\n",
            "Iteration:  98% 88/90 [01:54<00:02,  1.30s/it]\u001b[A\n",
            "Iteration:  99% 89/90 [01:55<00:01,  1.29s/it]\u001b[A\n",
            "Iteration: 100% 90/90 [01:56<00:00,  1.29s/it]\n",
            "Epoch: 100% 3/3 [05:44<00:00, 114.85s/it]\n",
            "04/05/2020 15:31:47 - INFO - __main__ -    global_step = 270, average loss = 0.9166097754681551\n",
            "04/05/2020 15:31:47 - INFO - __main__ -   Saving model checkpoint to ../-B_run\n",
            "04/05/2020 15:31:47 - INFO - transformers.configuration_utils -   Configuration saved in ../-B_run/config.json\n",
            "04/05/2020 15:31:49 - INFO - transformers.modeling_utils -   Model weights saved in ../-B_run/pytorch_model.bin\n",
            "04/05/2020 15:31:49 - INFO - transformers.configuration_utils -   loading configuration file ../-B_run/config.json\n",
            "04/05/2020 15:31:49 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 1,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sts-b\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 15:31:49 - INFO - transformers.modeling_utils -   loading weights file ../-B_run/pytorch_model.bin\n",
            "04/05/2020 15:31:52 - INFO - transformers.configuration_utils -   loading configuration file ../-B_run/config.json\n",
            "04/05/2020 15:31:52 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 1,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sts-b\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   Model name '../-B_run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../-B_run' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   Didn't find file ../-B_run/added_tokens.json. We won't load it.\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file ../-B_run/vocab.json\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file ../-B_run/merges.txt\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file ../-B_run/special_tokens_map.json\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file ../-B_run/tokenizer_config.json\n",
            "04/05/2020 15:31:52 - INFO - transformers.configuration_utils -   loading configuration file ../-B_run/config.json\n",
            "04/05/2020 15:31:52 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 1,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sts-b\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   Model name '../-B_run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../-B_run' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   Didn't find file ../-B_run/added_tokens.json. We won't load it.\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file ../-B_run/vocab.json\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file ../-B_run/merges.txt\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file ../-B_run/special_tokens_map.json\n",
            "04/05/2020 15:31:52 - INFO - transformers.tokenization_utils -   loading file ../-B_run/tokenizer_config.json\n",
            "04/05/2020 15:31:52 - INFO - __main__ -   Evaluate the following checkpoints: ['../-B_run']\n",
            "04/05/2020 15:31:52 - INFO - transformers.configuration_utils -   loading configuration file ../-B_run/config.json\n",
            "04/05/2020 15:31:52 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 1,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"sts-b\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 15:31:52 - INFO - transformers.modeling_utils -   loading weights file ../-B_run/pytorch_model.bin\n",
            "04/05/2020 15:31:56 - INFO - __main__ -   Creating features from dataset file at content//STS-B\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   Writing example 0/1500\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   input_ids: 0 83 313 19 10 543 3988 16 7950 4 2 2 83 313 2498 10 543 3988 16 7950 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   label: 5.000 (id = 5)\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   input_ids: 0 83 664 920 16 5793 10 5253 4 2 2 83 920 16 5793 10 5253 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   label: 4.750 (id = 4)\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   input_ids: 0 83 313 16 10943 10 18292 7 10 16173 4 2 2 20 313 16 10943 10 18292 7 5 16173 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   label: 5.000 (id = 5)\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   input_ids: 0 83 693 16 816 5 8669 4 2 2 83 313 16 816 8669 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   label: 2.400 (id = 2)\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   input_ids: 0 83 693 16 816 5 2342 4467 4 2 2 83 313 16 816 10 2342 4467 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 15:31:56 - INFO - transformers.data.processors.glue -   label: 2.750 (id = 2)\n",
            "04/05/2020 15:31:56 - INFO - __main__ -   Saving features into cached file content//STS-B/cached_dev_roberta-base_128_sts-b\n",
            "04/05/2020 15:31:57 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "04/05/2020 15:31:57 - INFO - __main__ -     Num examples = 1500\n",
            "04/05/2020 15:31:57 - INFO - __main__ -     Batch size = 64\n",
            "Evaluating: 100% 24/24 [00:09<00:00,  2.44it/s]\n",
            "04/05/2020 15:32:06 - INFO - __main__ -   ***** Eval results  *****\n",
            "04/05/2020 15:32:06 - INFO - __main__ -     corr = 0.8968578858954471\n",
            "04/05/2020 15:32:06 - INFO - __main__ -     pearson = 0.8975786541536713\n",
            "04/05/2020 15:32:06 - INFO - __main__ -     spearmanr = 0.8961371176372229\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}