{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoLA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmnJe06SfVMN",
        "colab_type": "text"
      },
      "source": [
        "# Install transformers 2.8.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rtNjBbdY7RM",
        "colab_type": "code",
        "outputId": "d6bc2335-0b0d-4441-c0a2-97f5d6f641b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 3.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 18.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.13.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.16.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.8.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3d91cebae19aa9dd151f00c9c9f1bb487103920ef1cdc7e77354b717de26fe68\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.90 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1u_y8dJAm-Y",
        "colab_type": "code",
        "outputId": "29bf78c8-13b0-49d0-cdd4-feee31af14d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May 15 16:18:24 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_moWpok-qkX",
        "colab_type": "text"
      },
      "source": [
        "# Set working directory to the directory containing `download_glue_data.py` and `run_glue.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0eeqGf45sYQ",
        "colab_type": "code",
        "outputId": "e1a9d140-994a-4376-9b3d-2183d6aaca0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0VFWNoOAfT-",
        "colab_type": "code",
        "outputId": "294f965c-2588-42d9-ef08-4bb408c8f83c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pwd\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/NLU project')\n",
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/drive/My Drive/NLU project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INV4cGxyf2f5",
        "colab_type": "text"
      },
      "source": [
        "# Download GLUE data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWYcFKd9f6Fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLUE_DIR=\"data/glue\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AJp5KgQglVh",
        "colab_type": "code",
        "outputId": "dce76432-c0d1-49ed-b32b-fd5e66aa6500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo $GLUE_DIR"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data/glue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFhfVflFdaVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TASK_NAME=\"CoLA\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_1EtLRQ8-WF",
        "colab_type": "code",
        "outputId": "8623c479-e0e4-436f-e2a2-56d77f1d4b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!echo $TASK_NAME"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CoLA\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24BX_hKwFhdU",
        "colab_type": "code",
        "outputId": "a5eee7be-5199-4eae-f0dc-ceffff1adb09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!python download_glue_data.py --help"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: download_glue_data.py [-h] [--data_dir DATA_DIR] [--tasks TASKS]\n",
            "                             [--path_to_mrpc PATH_TO_MRPC]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_dir DATA_DIR   directory to save data to\n",
            "  --tasks TASKS         tasks to download data for as a comma separated string\n",
            "  --path_to_mrpc PATH_TO_MRPC\n",
            "                        path to directory containing extracted MRPC data,\n",
            "                        msr_paraphrase_train.txt and msr_paraphrase_text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAqf1Fe6cZvI",
        "colab_type": "code",
        "outputId": "6c8ca002-ad30-499e-9d98-68dca60b34bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!python download_glue_data.py --data_dir $GLUE_DIR --tasks $TASK_NAME"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aqkcWzfgFNI",
        "colab_type": "text"
      },
      "source": [
        "# Compute score for specific GLUE task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4jsBBTfnwOa",
        "colab_type": "code",
        "outputId": "643cae97-abe8-40f3-d0a1-bbfa39d41c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MODEL_DIR = os.path.join('models', 'baseline')\n",
        "os.path.exists(MODEL_DIR)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiWazMGIsMk9",
        "colab_type": "code",
        "outputId": "0dcf074c-95f8-4dfb-f8e6-f0c2514f2ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPpl0f8wAkmf",
        "colab_type": "code",
        "outputId": "25d3ba28-2f1e-406e-c11a-e24296e7cd6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_glue.py --help"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 06:02:05.223176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: run_glue.py [-h] --data_dir DATA_DIR --model_type MODEL_TYPE\n",
            "                   --model_name_or_path MODEL_NAME_OR_PATH --task_name\n",
            "                   TASK_NAME --output_dir OUTPUT_DIR\n",
            "                   [--config_name CONFIG_NAME]\n",
            "                   [--tokenizer_name TOKENIZER_NAME] [--cache_dir CACHE_DIR]\n",
            "                   [--max_seq_length MAX_SEQ_LENGTH] [--do_train] [--do_eval]\n",
            "                   [--evaluate_during_training] [--do_lower_case]\n",
            "                   [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
            "                   [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
            "                   [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "                   [--learning_rate LEARNING_RATE]\n",
            "                   [--weight_decay WEIGHT_DECAY] [--adam_epsilon ADAM_EPSILON]\n",
            "                   [--max_grad_norm MAX_GRAD_NORM]\n",
            "                   [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
            "                   [--max_steps MAX_STEPS] [--warmup_steps WARMUP_STEPS]\n",
            "                   [--logging_steps LOGGING_STEPS] [--save_steps SAVE_STEPS]\n",
            "                   [--eval_all_checkpoints] [--no_cuda]\n",
            "                   [--overwrite_output_dir] [--overwrite_cache] [--seed SEED]\n",
            "                   [--fp16] [--fp16_opt_level FP16_OPT_LEVEL]\n",
            "                   [--local_rank LOCAL_RANK] [--server_ip SERVER_IP]\n",
            "                   [--server_port SERVER_PORT]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --data_dir DATA_DIR   The input data dir. Should contain the .tsv files (or\n",
            "                        other data files) for the task.\n",
            "  --model_type MODEL_TYPE\n",
            "                        Model type selected in the list: distilbert, albert,\n",
            "                        camembert, xlm-roberta, bart, roberta, bert, xlnet,\n",
            "                        flaubert, xlm\n",
            "  --model_name_or_path MODEL_NAME_OR_PATH\n",
            "                        Path to pre-trained model or shortcut name selected in\n",
            "                        the list: distilbert-base-uncased, distilbert-base-\n",
            "                        uncased-distilled-squad, distilbert-base-cased,\n",
            "                        distilbert-base-cased-distilled-squad, distilbert-\n",
            "                        base-german-cased, distilbert-base-multilingual-cased,\n",
            "                        distilbert-base-uncased-finetuned-sst-2-english,\n",
            "                        albert-base-v1, albert-large-v1, albert-xlarge-v1,\n",
            "                        albert-xxlarge-v1, albert-base-v2, albert-large-v2,\n",
            "                        albert-xlarge-v2, albert-xxlarge-v2, camembert-base,\n",
            "                        umberto-commoncrawl-cased-v1, umberto-wikipedia-\n",
            "                        uncased-v1, xlm-roberta-base, xlm-roberta-large, xlm-\n",
            "                        roberta-large-finetuned-conll02-dutch, xlm-roberta-\n",
            "                        large-finetuned-conll02-spanish, xlm-roberta-large-\n",
            "                        finetuned-conll03-english, xlm-roberta-large-\n",
            "                        finetuned-conll03-german, bart-large, bart-large-mnli,\n",
            "                        bart-large-cnn, bart-large-xsum, roberta-base,\n",
            "                        roberta-large, roberta-large-mnli, distilroberta-base,\n",
            "                        roberta-base-openai-detector, roberta-large-openai-\n",
            "                        detector, bert-base-uncased, bert-large-uncased, bert-\n",
            "                        base-cased, bert-large-cased, bert-base-multilingual-\n",
            "                        uncased, bert-base-multilingual-cased, bert-base-\n",
            "                        chinese, bert-base-german-cased, bert-large-uncased-\n",
            "                        whole-word-masking, bert-large-cased-whole-word-\n",
            "                        masking, bert-large-uncased-whole-word-masking-\n",
            "                        finetuned-squad, bert-large-cased-whole-word-masking-\n",
            "                        finetuned-squad, bert-base-cased-finetuned-mrpc, bert-\n",
            "                        base-german-dbmdz-cased, bert-base-german-dbmdz-\n",
            "                        uncased, bert-base-japanese, bert-base-japanese-whole-\n",
            "                        word-masking, bert-base-japanese-char, bert-base-\n",
            "                        japanese-char-whole-word-masking, bert-base-finnish-\n",
            "                        cased-v1, bert-base-finnish-uncased-v1, bert-base-\n",
            "                        dutch-cased, xlnet-base-cased, xlnet-large-cased,\n",
            "                        flaubert-small-cased, flaubert-base-uncased, flaubert-\n",
            "                        base-cased, flaubert-large-cased, xlm-mlm-en-2048,\n",
            "                        xlm-mlm-ende-1024, xlm-mlm-enfr-1024, xlm-mlm-\n",
            "                        enro-1024, xlm-mlm-tlm-xnli15-1024, xlm-mlm-\n",
            "                        xnli15-1024, xlm-clm-enfr-1024, xlm-clm-ende-1024,\n",
            "                        xlm-mlm-17-1280, xlm-mlm-100-1280\n",
            "  --task_name TASK_NAME\n",
            "                        The name of the task to train selected in the list:\n",
            "                        cola, mnli, mnli-mm, mrpc, sst-2, sts-b, qqp, qnli,\n",
            "                        rte, wnli\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        The output directory where the model predictions and\n",
            "                        checkpoints will be written.\n",
            "  --config_name CONFIG_NAME\n",
            "                        Pretrained config name or path if not the same as\n",
            "                        model_name\n",
            "  --tokenizer_name TOKENIZER_NAME\n",
            "                        Pretrained tokenizer name or path if not the same as\n",
            "                        model_name\n",
            "  --cache_dir CACHE_DIR\n",
            "                        Where do you want to store the pre-trained models\n",
            "                        downloaded from s3\n",
            "  --max_seq_length MAX_SEQ_LENGTH\n",
            "                        The maximum total input sequence length after\n",
            "                        tokenization. Sequences longer than this will be\n",
            "                        truncated, sequences shorter will be padded.\n",
            "  --do_train            Whether to run training.\n",
            "  --do_eval             Whether to run eval on the dev set.\n",
            "  --evaluate_during_training\n",
            "                        Run evaluation during training at each logging step.\n",
            "  --do_lower_case       Set this flag if you are using an uncased model.\n",
            "  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n",
            "                        Batch size per GPU/CPU for training.\n",
            "  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n",
            "                        Batch size per GPU/CPU for evaluation.\n",
            "  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n",
            "                        Number of updates steps to accumulate before\n",
            "                        performing a backward/update pass.\n",
            "  --learning_rate LEARNING_RATE\n",
            "                        The initial learning rate for Adam.\n",
            "  --weight_decay WEIGHT_DECAY\n",
            "                        Weight decay if we apply some.\n",
            "  --adam_epsilon ADAM_EPSILON\n",
            "                        Epsilon for Adam optimizer.\n",
            "  --max_grad_norm MAX_GRAD_NORM\n",
            "                        Max gradient norm.\n",
            "  --num_train_epochs NUM_TRAIN_EPOCHS\n",
            "                        Total number of training epochs to perform.\n",
            "  --max_steps MAX_STEPS\n",
            "                        If > 0: set total number of training steps to perform.\n",
            "                        Override num_train_epochs.\n",
            "  --warmup_steps WARMUP_STEPS\n",
            "                        Linear warmup over warmup_steps.\n",
            "  --logging_steps LOGGING_STEPS\n",
            "                        Log every X updates steps.\n",
            "  --save_steps SAVE_STEPS\n",
            "                        Save checkpoint every X updates steps.\n",
            "  --eval_all_checkpoints\n",
            "                        Evaluate all checkpoints starting with the same prefix\n",
            "                        as model_name ending and ending with step number\n",
            "  --no_cuda             Avoid using CUDA when available\n",
            "  --overwrite_output_dir\n",
            "                        Overwrite the content of the output directory\n",
            "  --overwrite_cache     Overwrite the cached training and evaluation sets\n",
            "  --seed SEED           random seed for initialization\n",
            "  --fp16                Whether to use 16-bit (mixed) precision (through\n",
            "                        NVIDIA apex) instead of 32-bit\n",
            "  --fp16_opt_level FP16_OPT_LEVEL\n",
            "                        For fp16: Apex AMP optimization level selected in\n",
            "                        ['O0', 'O1', 'O2', and 'O3'].See details at\n",
            "                        https://nvidia.github.io/apex/amp.html\n",
            "  --local_rank LOCAL_RANK\n",
            "                        For distributed training: local_rank\n",
            "  --server_ip SERVER_IP\n",
            "                        For distant debugging.\n",
            "  --server_port SERVER_PORT\n",
            "                        For distant debugging.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnxryCpnczcm",
        "colab_type": "code",
        "outputId": "ec753c43-45e6-4794-a97c-ab38fa833572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_glue.py \\\n",
        "    --model_type roberta \\\n",
        "    --model_name_or_path $MODEL_DIR \\\n",
        "    --task_name $TASK_NAME \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=64   \\\n",
        "    --per_gpu_train_batch_size=64   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --output_dir run \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-15 16:24:46.091200: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "05/15/2020 16:24:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/15/2020 16:24:49 - INFO - transformers.configuration_utils -   loading configuration file models/baseline/config.json\n",
            "05/15/2020 16:24:49 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenDiscrimination\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 16:24:49 - INFO - transformers.configuration_utils -   loading configuration file models/baseline/config.json\n",
            "05/15/2020 16:24:49 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenDiscrimination\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 16:24:49 - INFO - transformers.tokenization_utils -   Model name 'models/baseline' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'models/baseline' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/15/2020 16:24:49 - INFO - transformers.tokenization_utils -   Didn't find file models/baseline/added_tokens.json. We won't load it.\n",
            "05/15/2020 16:24:49 - INFO - transformers.tokenization_utils -   loading file models/baseline/vocab.json\n",
            "05/15/2020 16:24:49 - INFO - transformers.tokenization_utils -   loading file models/baseline/merges.txt\n",
            "05/15/2020 16:24:49 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/15/2020 16:24:49 - INFO - transformers.tokenization_utils -   loading file models/baseline/special_tokens_map.json\n",
            "05/15/2020 16:24:49 - INFO - transformers.tokenization_utils -   loading file models/baseline/tokenizer_config.json\n",
            "05/15/2020 16:24:50 - INFO - transformers.modeling_utils -   loading weights file models/baseline/pytorch_model.bin\n",
            "05/15/2020 16:25:00 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "05/15/2020 16:25:00 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
            "05/15/2020 16:25:14 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='data/glue/CoLA', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='models/baseline', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='run', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "05/15/2020 16:25:14 - INFO - __main__ -   Creating features from dataset file at data/glue/CoLA\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   Writing example 0/8551\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   guid: train-0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   input_ids: 0 1541 964 351 75 907 42 1966 6 905 1937 5 220 65 52 15393 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   input_ids: 0 509 55 38283 937 1938 8 38 437 1311 62 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   input_ids: 0 509 55 38283 937 1938 50 38 437 1311 62 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   input_ids: 0 20 55 52 892 47041 6 5 26002 906 51 120 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   input_ids: 0 1053 30 183 5 4905 32 562 22802 330 906 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:25:14 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:25:16 - INFO - __main__ -   Saving features into cached file data/glue/CoLA/cached_train_baseline_128_cola\n",
            "05/15/2020 16:25:18 - INFO - __main__ -   ***** Running training *****\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Num examples = 8551\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Num Epochs = 3\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Total optimization steps = 402\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Continuing training from epoch 0\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Continuing training from global step 0\n",
            "05/15/2020 16:25:18 - INFO - __main__ -     Will skip the first 0 steps in the first epoch\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/134 [00:00<?, ?it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "\n",
            "Iteration:   1% 1/134 [00:02<05:49,  2.63s/it]\u001b[A\n",
            "Iteration:   1% 2/134 [00:04<05:26,  2.47s/it]\u001b[A\n",
            "Iteration:   2% 3/134 [00:06<05:10,  2.37s/it]\u001b[A\n",
            "Iteration:   3% 4/134 [00:08<04:58,  2.29s/it]\u001b[A\n",
            "Iteration:   4% 5/134 [00:11<04:49,  2.24s/it]\u001b[A\n",
            "Iteration:   4% 6/134 [00:13<04:42,  2.21s/it]\u001b[A\n",
            "Iteration:   5% 7/134 [00:15<04:36,  2.18s/it]\u001b[A\n",
            "Iteration:   6% 8/134 [00:17<04:32,  2.16s/it]\u001b[A\n",
            "Iteration:   7% 9/134 [00:19<04:28,  2.15s/it]\u001b[A\n",
            "Iteration:   7% 10/134 [00:21<04:26,  2.15s/it]\u001b[A\n",
            "Iteration:   8% 11/134 [00:23<04:23,  2.14s/it]\u001b[A\n",
            "Iteration:   9% 12/134 [00:25<04:20,  2.14s/it]\u001b[A\n",
            "Iteration:  10% 13/134 [00:28<04:18,  2.13s/it]\u001b[A\n",
            "Iteration:  10% 14/134 [00:30<04:15,  2.13s/it]\u001b[A\n",
            "Iteration:  11% 15/134 [00:32<04:13,  2.13s/it]\u001b[A\n",
            "Iteration:  12% 16/134 [00:34<04:11,  2.13s/it]\u001b[A\n",
            "Iteration:  13% 17/134 [00:36<04:09,  2.13s/it]\u001b[A\n",
            "Iteration:  13% 18/134 [00:38<04:07,  2.13s/it]\u001b[A\n",
            "Iteration:  14% 19/134 [00:40<04:06,  2.14s/it]\u001b[A\n",
            "Iteration:  15% 20/134 [00:43<04:04,  2.14s/it]\u001b[A\n",
            "Iteration:  16% 21/134 [00:45<04:02,  2.15s/it]\u001b[A\n",
            "Iteration:  16% 22/134 [00:47<04:00,  2.15s/it]\u001b[A\n",
            "Iteration:  17% 23/134 [00:49<03:58,  2.15s/it]\u001b[A\n",
            "Iteration:  18% 24/134 [00:51<03:56,  2.15s/it]\u001b[A\n",
            "Iteration:  19% 25/134 [00:53<03:54,  2.15s/it]\u001b[A\n",
            "Iteration:  19% 26/134 [00:55<03:52,  2.15s/it]\u001b[A\n",
            "Iteration:  20% 27/134 [00:58<03:50,  2.15s/it]\u001b[A\n",
            "Iteration:  21% 28/134 [01:00<03:48,  2.16s/it]\u001b[A\n",
            "Iteration:  22% 29/134 [01:02<03:46,  2.15s/it]\u001b[A\n",
            "Iteration:  22% 30/134 [01:04<03:44,  2.16s/it]\u001b[A\n",
            "Iteration:  23% 31/134 [01:06<03:42,  2.16s/it]\u001b[A\n",
            "Iteration:  24% 32/134 [01:08<03:39,  2.16s/it]\u001b[A\n",
            "Iteration:  25% 33/134 [01:11<03:38,  2.16s/it]\u001b[A\n",
            "Iteration:  25% 34/134 [01:13<03:36,  2.16s/it]\u001b[A\n",
            "Iteration:  26% 35/134 [01:15<03:34,  2.17s/it]\u001b[A\n",
            "Iteration:  27% 36/134 [01:17<03:32,  2.17s/it]\u001b[A\n",
            "Iteration:  28% 37/134 [01:19<03:30,  2.18s/it]\u001b[A\n",
            "Iteration:  28% 38/134 [01:21<03:28,  2.18s/it]\u001b[A\n",
            "Iteration:  29% 39/134 [01:24<03:26,  2.18s/it]\u001b[A\n",
            "Iteration:  30% 40/134 [01:26<03:24,  2.18s/it]\u001b[A\n",
            "Iteration:  31% 41/134 [01:28<03:22,  2.18s/it]\u001b[A\n",
            "Iteration:  31% 42/134 [01:30<03:20,  2.18s/it]\u001b[A\n",
            "Iteration:  32% 43/134 [01:32<03:18,  2.18s/it]\u001b[A\n",
            "Iteration:  33% 44/134 [01:35<03:16,  2.19s/it]\u001b[A\n",
            "Iteration:  34% 45/134 [01:37<03:15,  2.19s/it]\u001b[A\n",
            "Iteration:  34% 46/134 [01:39<03:12,  2.19s/it]\u001b[A\n",
            "Iteration:  35% 47/134 [01:41<03:10,  2.19s/it]\u001b[A\n",
            "Iteration:  36% 48/134 [01:43<03:08,  2.19s/it]\u001b[A\n",
            "Iteration:  37% 49/134 [01:46<03:06,  2.19s/it]\u001b[A\n",
            "Iteration:  37% 50/134 [01:48<03:04,  2.19s/it]\u001b[A\n",
            "Iteration:  38% 51/134 [01:50<03:01,  2.19s/it]\u001b[A\n",
            "Iteration:  39% 52/134 [01:52<02:59,  2.19s/it]\u001b[A\n",
            "Iteration:  40% 53/134 [01:54<02:57,  2.20s/it]\u001b[A\n",
            "Iteration:  40% 54/134 [01:57<02:55,  2.20s/it]\u001b[A\n",
            "Iteration:  41% 55/134 [01:59<02:53,  2.19s/it]\u001b[A\n",
            "Iteration:  42% 56/134 [02:01<02:51,  2.20s/it]\u001b[A\n",
            "Iteration:  43% 57/134 [02:03<02:49,  2.20s/it]\u001b[A\n",
            "Iteration:  43% 58/134 [02:05<02:46,  2.20s/it]\u001b[A\n",
            "Iteration:  44% 59/134 [02:08<02:44,  2.19s/it]\u001b[A\n",
            "Iteration:  45% 60/134 [02:10<02:42,  2.20s/it]\u001b[A\n",
            "Iteration:  46% 61/134 [02:12<02:40,  2.19s/it]\u001b[A\n",
            "Iteration:  46% 62/134 [02:14<02:38,  2.20s/it]\u001b[A\n",
            "Iteration:  47% 63/134 [02:16<02:35,  2.20s/it]\u001b[A\n",
            "Iteration:  48% 64/134 [02:19<02:33,  2.20s/it]\u001b[A\n",
            "Iteration:  49% 65/134 [02:21<02:31,  2.20s/it]\u001b[A\n",
            "Iteration:  49% 66/134 [02:23<02:29,  2.20s/it]\u001b[A\n",
            "Iteration:  50% 67/134 [02:25<02:27,  2.20s/it]\u001b[A\n",
            "Iteration:  51% 68/134 [02:27<02:24,  2.20s/it]\u001b[A\n",
            "Iteration:  51% 69/134 [02:29<02:22,  2.19s/it]\u001b[A\n",
            "Iteration:  52% 70/134 [02:32<02:20,  2.19s/it]\u001b[A\n",
            "Iteration:  53% 71/134 [02:34<02:18,  2.19s/it]\u001b[A\n",
            "Iteration:  54% 72/134 [02:36<02:15,  2.19s/it]\u001b[A\n",
            "Iteration:  54% 73/134 [02:38<02:13,  2.19s/it]\u001b[A\n",
            "Iteration:  55% 74/134 [02:40<02:11,  2.20s/it]\u001b[A\n",
            "Iteration:  56% 75/134 [02:43<02:09,  2.20s/it]\u001b[A\n",
            "Iteration:  57% 76/134 [02:45<02:07,  2.20s/it]\u001b[A\n",
            "Iteration:  57% 77/134 [02:47<02:05,  2.20s/it]\u001b[A\n",
            "Iteration:  58% 78/134 [02:49<02:02,  2.20s/it]\u001b[A\n",
            "Iteration:  59% 79/134 [02:51<02:00,  2.20s/it]\u001b[A\n",
            "Iteration:  60% 80/134 [02:54<01:58,  2.20s/it]\u001b[A\n",
            "Iteration:  60% 81/134 [02:56<01:56,  2.20s/it]\u001b[A\n",
            "Iteration:  61% 82/134 [02:58<01:54,  2.20s/it]\u001b[A\n",
            "Iteration:  62% 83/134 [03:00<01:52,  2.20s/it]\u001b[A\n",
            "Iteration:  63% 84/134 [03:02<01:49,  2.20s/it]\u001b[A\n",
            "Iteration:  63% 85/134 [03:05<01:47,  2.20s/it]\u001b[A\n",
            "Iteration:  64% 86/134 [03:07<01:45,  2.20s/it]\u001b[A\n",
            "Iteration:  65% 87/134 [03:09<01:43,  2.20s/it]\u001b[A\n",
            "Iteration:  66% 88/134 [03:11<01:41,  2.20s/it]\u001b[A\n",
            "Iteration:  66% 89/134 [03:13<01:38,  2.20s/it]\u001b[A\n",
            "Iteration:  67% 90/134 [03:16<01:36,  2.20s/it]\u001b[A\n",
            "Iteration:  68% 91/134 [03:18<01:34,  2.20s/it]\u001b[A\n",
            "Iteration:  69% 92/134 [03:20<01:32,  2.20s/it]\u001b[A\n",
            "Iteration:  69% 93/134 [03:22<01:30,  2.20s/it]\u001b[A\n",
            "Iteration:  70% 94/134 [03:24<01:28,  2.20s/it]\u001b[A\n",
            "Iteration:  71% 95/134 [03:27<01:25,  2.20s/it]\u001b[A\n",
            "Iteration:  72% 96/134 [03:29<01:23,  2.20s/it]\u001b[A\n",
            "Iteration:  72% 97/134 [03:31<01:21,  2.20s/it]\u001b[A\n",
            "Iteration:  73% 98/134 [03:33<01:19,  2.20s/it]\u001b[A\n",
            "Iteration:  74% 99/134 [03:35<01:16,  2.20s/it]\u001b[A\n",
            "Iteration:  75% 100/134 [03:38<01:14,  2.20s/it]\u001b[A\n",
            "Iteration:  75% 101/134 [03:40<01:12,  2.20s/it]\u001b[A\n",
            "Iteration:  76% 102/134 [03:42<01:10,  2.20s/it]\u001b[A\n",
            "Iteration:  77% 103/134 [03:44<01:08,  2.20s/it]\u001b[A\n",
            "Iteration:  78% 104/134 [03:46<01:05,  2.20s/it]\u001b[A\n",
            "Iteration:  78% 105/134 [03:49<01:03,  2.20s/it]\u001b[A\n",
            "Iteration:  79% 106/134 [03:51<01:01,  2.20s/it]\u001b[A\n",
            "Iteration:  80% 107/134 [03:53<00:59,  2.20s/it]\u001b[A\n",
            "Iteration:  81% 108/134 [03:55<00:57,  2.20s/it]\u001b[A\n",
            "Iteration:  81% 109/134 [03:57<00:54,  2.20s/it]\u001b[A\n",
            "Iteration:  82% 110/134 [04:00<00:52,  2.20s/it]\u001b[A\n",
            "Iteration:  83% 111/134 [04:02<00:50,  2.20s/it]\u001b[A\n",
            "Iteration:  84% 112/134 [04:04<00:48,  2.20s/it]\u001b[A\n",
            "Iteration:  84% 113/134 [04:06<00:46,  2.19s/it]\u001b[A\n",
            "Iteration:  85% 114/134 [04:08<00:43,  2.20s/it]\u001b[A\n",
            "Iteration:  86% 115/134 [04:11<00:41,  2.20s/it]\u001b[A\n",
            "Iteration:  87% 116/134 [04:13<00:39,  2.20s/it]\u001b[A\n",
            "Iteration:  87% 117/134 [04:15<00:37,  2.20s/it]\u001b[A\n",
            "Iteration:  88% 118/134 [04:17<00:35,  2.20s/it]\u001b[A\n",
            "Iteration:  89% 119/134 [04:19<00:32,  2.20s/it]\u001b[A\n",
            "Iteration:  90% 120/134 [04:22<00:30,  2.20s/it]\u001b[A\n",
            "Iteration:  90% 121/134 [04:24<00:28,  2.20s/it]\u001b[A\n",
            "Iteration:  91% 122/134 [04:26<00:26,  2.20s/it]\u001b[A\n",
            "Iteration:  92% 123/134 [04:28<00:24,  2.20s/it]\u001b[A\n",
            "Iteration:  93% 124/134 [04:30<00:21,  2.20s/it]\u001b[A\n",
            "Iteration:  93% 125/134 [04:33<00:19,  2.20s/it]\u001b[A\n",
            "Iteration:  94% 126/134 [04:35<00:17,  2.20s/it]\u001b[A\n",
            "Iteration:  95% 127/134 [04:37<00:15,  2.20s/it]\u001b[A\n",
            "Iteration:  96% 128/134 [04:39<00:13,  2.20s/it]\u001b[A\n",
            "Iteration:  96% 129/134 [04:41<00:11,  2.20s/it]\u001b[A\n",
            "Iteration:  97% 130/134 [04:44<00:08,  2.20s/it]\u001b[A\n",
            "Iteration:  98% 131/134 [04:46<00:06,  2.20s/it]\u001b[A\n",
            "Iteration:  99% 132/134 [04:48<00:04,  2.20s/it]\u001b[A\n",
            "Iteration:  99% 133/134 [04:50<00:02,  2.20s/it]\u001b[A\n",
            "Iteration: 100% 134/134 [04:52<00:00,  2.18s/it]\n",
            "Epoch:  33% 1/3 [04:52<09:44, 292.10s/it]\n",
            "Iteration:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/134 [00:02<04:52,  2.20s/it]\u001b[A\n",
            "Iteration:   1% 2/134 [00:04<04:49,  2.20s/it]\u001b[A\n",
            "Iteration:   2% 3/134 [00:06<04:47,  2.20s/it]\u001b[A\n",
            "Iteration:   3% 4/134 [00:08<04:45,  2.20s/it]\u001b[A\n",
            "Iteration:   4% 5/134 [00:11<04:43,  2.20s/it]\u001b[A\n",
            "Iteration:   4% 6/134 [00:13<04:41,  2.20s/it]\u001b[A\n",
            "Iteration:   5% 7/134 [00:15<04:39,  2.20s/it]\u001b[A\n",
            "Iteration:   6% 8/134 [00:17<04:37,  2.20s/it]\u001b[A\n",
            "Iteration:   7% 9/134 [00:19<04:35,  2.20s/it]\u001b[A\n",
            "Iteration:   7% 10/134 [00:22<04:32,  2.20s/it]\u001b[A\n",
            "Iteration:   8% 11/134 [00:24<04:30,  2.20s/it]\u001b[A\n",
            "Iteration:   9% 12/134 [00:26<04:28,  2.20s/it]\u001b[A\n",
            "Iteration:  10% 13/134 [00:28<04:26,  2.20s/it]\u001b[A\n",
            "Iteration:  10% 14/134 [00:30<04:24,  2.20s/it]\u001b[A\n",
            "Iteration:  11% 15/134 [00:33<04:21,  2.20s/it]\u001b[A\n",
            "Iteration:  12% 16/134 [00:35<04:19,  2.20s/it]\u001b[A\n",
            "Iteration:  13% 17/134 [00:37<04:17,  2.20s/it]\u001b[A\n",
            "Iteration:  13% 18/134 [00:39<04:15,  2.20s/it]\u001b[A\n",
            "Iteration:  14% 19/134 [00:41<04:13,  2.20s/it]\u001b[A\n",
            "Iteration:  15% 20/134 [00:43<04:10,  2.20s/it]\u001b[A\n",
            "Iteration:  16% 21/134 [00:46<04:08,  2.20s/it]\u001b[A\n",
            "Iteration:  16% 22/134 [00:48<04:06,  2.20s/it]\u001b[A\n",
            "Iteration:  17% 23/134 [00:50<04:04,  2.20s/it]\u001b[A\n",
            "Iteration:  18% 24/134 [00:52<04:02,  2.20s/it]\u001b[A\n",
            "Iteration:  19% 25/134 [00:55<03:59,  2.20s/it]\u001b[A\n",
            "Iteration:  19% 26/134 [00:57<03:57,  2.20s/it]\u001b[A\n",
            "Iteration:  20% 27/134 [00:59<03:55,  2.20s/it]\u001b[A\n",
            "Iteration:  21% 28/134 [01:01<03:53,  2.20s/it]\u001b[A\n",
            "Iteration:  22% 29/134 [01:03<03:51,  2.20s/it]\u001b[A\n",
            "Iteration:  22% 30/134 [01:06<03:48,  2.20s/it]\u001b[A\n",
            "Iteration:  23% 31/134 [01:08<03:46,  2.20s/it]\u001b[A\n",
            "Iteration:  24% 32/134 [01:10<03:44,  2.20s/it]\u001b[A\n",
            "Iteration:  25% 33/134 [01:12<03:42,  2.20s/it]\u001b[A\n",
            "Iteration:  25% 34/134 [01:14<03:40,  2.20s/it]\u001b[A\n",
            "Iteration:  26% 35/134 [01:17<03:37,  2.20s/it]\u001b[A\n",
            "Iteration:  27% 36/134 [01:19<03:35,  2.20s/it]\u001b[A\n",
            "Iteration:  28% 37/134 [01:21<03:33,  2.20s/it]\u001b[A\n",
            "Iteration:  28% 38/134 [01:23<03:31,  2.20s/it]\u001b[A\n",
            "Iteration:  29% 39/134 [01:25<03:28,  2.20s/it]\u001b[A\n",
            "Iteration:  30% 40/134 [01:28<03:26,  2.20s/it]\u001b[A\n",
            "Iteration:  31% 41/134 [01:30<03:24,  2.20s/it]\u001b[A\n",
            "Iteration:  31% 42/134 [01:32<03:22,  2.20s/it]\u001b[A\n",
            "Iteration:  32% 43/134 [01:34<03:20,  2.21s/it]\u001b[A\n",
            "Iteration:  33% 44/134 [01:36<03:18,  2.20s/it]\u001b[A\n",
            "Iteration:  34% 45/134 [01:39<03:16,  2.20s/it]\u001b[A\n",
            "Iteration:  34% 46/134 [01:41<03:13,  2.20s/it]\u001b[A\n",
            "Iteration:  35% 47/134 [01:43<03:11,  2.20s/it]\u001b[A\n",
            "Iteration:  36% 48/134 [01:45<03:09,  2.20s/it]\u001b[A\n",
            "Iteration:  37% 49/134 [01:47<03:07,  2.20s/it]\u001b[A\n",
            "Iteration:  37% 50/134 [01:50<03:04,  2.20s/it]\u001b[A\n",
            "Iteration:  38% 51/134 [01:52<03:02,  2.20s/it]\u001b[A\n",
            "Iteration:  39% 52/134 [01:54<03:00,  2.20s/it]\u001b[A\n",
            "Iteration:  40% 53/134 [01:56<02:58,  2.20s/it]\u001b[A\n",
            "Iteration:  40% 54/134 [01:58<02:55,  2.20s/it]\u001b[A\n",
            "Iteration:  41% 55/134 [02:01<02:53,  2.20s/it]\u001b[A\n",
            "Iteration:  42% 56/134 [02:03<02:51,  2.20s/it]\u001b[A\n",
            "Iteration:  43% 57/134 [02:05<02:49,  2.20s/it]\u001b[A\n",
            "Iteration:  43% 58/134 [02:07<02:47,  2.20s/it]\u001b[A\n",
            "Iteration:  44% 59/134 [02:09<02:44,  2.19s/it]\u001b[A\n",
            "Iteration:  45% 60/134 [02:12<02:42,  2.20s/it]\u001b[A\n",
            "Iteration:  46% 61/134 [02:14<02:40,  2.20s/it]\u001b[A\n",
            "Iteration:  46% 62/134 [02:16<02:38,  2.20s/it]\u001b[A\n",
            "Iteration:  47% 63/134 [02:18<02:36,  2.20s/it]\u001b[A\n",
            "Iteration:  48% 64/134 [02:20<02:34,  2.20s/it]\u001b[A\n",
            "Iteration:  49% 65/134 [02:23<02:31,  2.20s/it]\u001b[A\n",
            "Iteration:  49% 66/134 [02:25<02:29,  2.20s/it]\u001b[A\n",
            "Iteration:  50% 67/134 [02:27<02:27,  2.20s/it]\u001b[A\n",
            "Iteration:  51% 68/134 [02:29<02:25,  2.20s/it]\u001b[A\n",
            "Iteration:  51% 69/134 [02:31<02:22,  2.20s/it]\u001b[A\n",
            "Iteration:  52% 70/134 [02:34<02:20,  2.20s/it]\u001b[A\n",
            "Iteration:  53% 71/134 [02:36<02:18,  2.20s/it]\u001b[A\n",
            "Iteration:  54% 72/134 [02:38<02:16,  2.20s/it]\u001b[A\n",
            "Iteration:  54% 73/134 [02:40<02:14,  2.20s/it]\u001b[A\n",
            "Iteration:  55% 74/134 [02:42<02:11,  2.20s/it]\u001b[A\n",
            "Iteration:  56% 75/134 [02:45<02:09,  2.20s/it]\u001b[A\n",
            "Iteration:  57% 76/134 [02:47<02:07,  2.20s/it]\u001b[A\n",
            "Iteration:  57% 77/134 [02:49<02:05,  2.20s/it]\u001b[A\n",
            "Iteration:  58% 78/134 [02:51<02:03,  2.20s/it]\u001b[A\n",
            "Iteration:  59% 79/134 [02:53<02:00,  2.20s/it]\u001b[A\n",
            "Iteration:  60% 80/134 [02:56<01:58,  2.20s/it]\u001b[A\n",
            "Iteration:  60% 81/134 [02:58<01:56,  2.20s/it]\u001b[A\n",
            "Iteration:  61% 82/134 [03:00<01:54,  2.20s/it]\u001b[A\n",
            "Iteration:  62% 83/134 [03:02<01:52,  2.20s/it]\u001b[A\n",
            "Iteration:  63% 84/134 [03:04<01:50,  2.20s/it]\u001b[A\n",
            "Iteration:  63% 85/134 [03:07<01:47,  2.20s/it]\u001b[A\n",
            "Iteration:  64% 86/134 [03:09<01:45,  2.20s/it]\u001b[A\n",
            "Iteration:  65% 87/134 [03:11<01:43,  2.20s/it]\u001b[A\n",
            "Iteration:  66% 88/134 [03:13<01:41,  2.20s/it]\u001b[A\n",
            "Iteration:  66% 89/134 [03:15<01:39,  2.20s/it]\u001b[A\n",
            "Iteration:  67% 90/134 [03:18<01:36,  2.20s/it]\u001b[A\n",
            "Iteration:  68% 91/134 [03:20<01:34,  2.20s/it]\u001b[A\n",
            "Iteration:  69% 92/134 [03:22<01:32,  2.20s/it]\u001b[A\n",
            "Iteration:  69% 93/134 [03:24<01:30,  2.20s/it]\u001b[A\n",
            "Iteration:  70% 94/134 [03:26<01:28,  2.20s/it]\u001b[A\n",
            "Iteration:  71% 95/134 [03:29<01:25,  2.20s/it]\u001b[A\n",
            "Iteration:  72% 96/134 [03:31<01:23,  2.20s/it]\u001b[A\n",
            "Iteration:  72% 97/134 [03:33<01:21,  2.20s/it]\u001b[A\n",
            "Iteration:  73% 98/134 [03:35<01:19,  2.20s/it]\u001b[A\n",
            "Iteration:  74% 99/134 [03:37<01:16,  2.20s/it]\u001b[A\n",
            "Iteration:  75% 100/134 [03:40<01:14,  2.20s/it]\u001b[A\n",
            "Iteration:  75% 101/134 [03:42<01:12,  2.20s/it]\u001b[A\n",
            "Iteration:  76% 102/134 [03:44<01:10,  2.20s/it]\u001b[A\n",
            "Iteration:  77% 103/134 [03:46<01:08,  2.20s/it]\u001b[A\n",
            "Iteration:  78% 104/134 [03:48<01:05,  2.20s/it]\u001b[A\n",
            "Iteration:  78% 105/134 [03:50<01:03,  2.19s/it]\u001b[A\n",
            "Iteration:  79% 106/134 [03:53<01:01,  2.19s/it]\u001b[A\n",
            "Iteration:  80% 107/134 [03:55<00:59,  2.20s/it]\u001b[A\n",
            "Iteration:  81% 108/134 [03:57<00:57,  2.20s/it]\u001b[A\n",
            "Iteration:  81% 109/134 [03:59<00:54,  2.20s/it]\u001b[A\n",
            "Iteration:  82% 110/134 [04:01<00:52,  2.20s/it]\u001b[A\n",
            "Iteration:  83% 111/134 [04:04<00:50,  2.20s/it]\u001b[A\n",
            "Iteration:  84% 112/134 [04:06<00:48,  2.20s/it]\u001b[A\n",
            "Iteration:  84% 113/134 [04:08<00:46,  2.20s/it]\u001b[A\n",
            "Iteration:  85% 114/134 [04:10<00:43,  2.20s/it]\u001b[A\n",
            "Iteration:  86% 115/134 [04:12<00:41,  2.20s/it]\u001b[A\n",
            "Iteration:  87% 116/134 [04:15<00:39,  2.20s/it]\u001b[A\n",
            "Iteration:  87% 117/134 [04:17<00:37,  2.20s/it]\u001b[A\n",
            "Iteration:  88% 118/134 [04:19<00:35,  2.20s/it]\u001b[A\n",
            "Iteration:  89% 119/134 [04:21<00:33,  2.20s/it]\u001b[A\n",
            "Iteration:  90% 120/134 [04:23<00:30,  2.20s/it]\u001b[A\n",
            "Iteration:  90% 121/134 [04:26<00:28,  2.20s/it]\u001b[A\n",
            "Iteration:  91% 122/134 [04:28<00:26,  2.20s/it]\u001b[A\n",
            "Iteration:  92% 123/134 [04:30<00:24,  2.20s/it]\u001b[A\n",
            "Iteration:  93% 124/134 [04:32<00:21,  2.20s/it]\u001b[A\n",
            "Iteration:  93% 125/134 [04:34<00:19,  2.20s/it]\u001b[A\n",
            "Iteration:  94% 126/134 [04:37<00:17,  2.20s/it]\u001b[A\n",
            "Iteration:  95% 127/134 [04:39<00:15,  2.20s/it]\u001b[A\n",
            "Iteration:  96% 128/134 [04:41<00:13,  2.20s/it]\u001b[A\n",
            "Iteration:  96% 129/134 [04:43<00:11,  2.20s/it]\u001b[A\n",
            "Iteration:  97% 130/134 [04:45<00:08,  2.20s/it]\u001b[A\n",
            "Iteration:  98% 131/134 [04:48<00:06,  2.20s/it]\u001b[A\n",
            "Iteration:  99% 132/134 [04:50<00:04,  2.20s/it]\u001b[A\n",
            "Iteration:  99% 133/134 [04:52<00:02,  2.20s/it]\u001b[A\n",
            "Iteration: 100% 134/134 [04:53<00:00,  2.19s/it]\n",
            "Epoch:  67% 2/3 [09:46<04:52, 292.67s/it]\n",
            "Iteration:   0% 0/134 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/134 [00:02<04:51,  2.20s/it]\u001b[A\n",
            "Iteration:   1% 2/134 [00:04<04:49,  2.20s/it]\u001b[A\n",
            "Iteration:   2% 3/134 [00:06<04:47,  2.20s/it]\u001b[A\n",
            "Iteration:   3% 4/134 [00:08<04:45,  2.20s/it]\u001b[A\n",
            "Iteration:   4% 5/134 [00:10<04:43,  2.20s/it]\u001b[A\n",
            "Iteration:   4% 6/134 [00:13<04:41,  2.20s/it]\u001b[A\n",
            "Iteration:   5% 7/134 [00:15<04:39,  2.20s/it]\u001b[A\n",
            "Iteration:   6% 8/134 [00:17<04:36,  2.20s/it]\u001b[A\n",
            "Iteration:   7% 9/134 [00:19<04:34,  2.20s/it]\u001b[A\n",
            "Iteration:   7% 10/134 [00:21<04:32,  2.20s/it]\u001b[A\n",
            "Iteration:   8% 11/134 [00:24<04:30,  2.20s/it]\u001b[A\n",
            "Iteration:   9% 12/134 [00:26<04:28,  2.20s/it]\u001b[A\n",
            "Iteration:  10% 13/134 [00:28<04:26,  2.20s/it]\u001b[A\n",
            "Iteration:  10% 14/134 [00:30<04:24,  2.20s/it]\u001b[A\n",
            "Iteration:  11% 15/134 [00:32<04:22,  2.20s/it]\u001b[A\n",
            "Iteration:  12% 16/134 [00:35<04:19,  2.20s/it]\u001b[A\n",
            "Iteration:  13% 17/134 [00:37<04:17,  2.20s/it]\u001b[A\n",
            "Iteration:  13% 18/134 [00:39<04:15,  2.20s/it]\u001b[A\n",
            "Iteration:  14% 19/134 [00:41<04:13,  2.20s/it]\u001b[A\n",
            "Iteration:  15% 20/134 [00:44<04:11,  2.20s/it]\u001b[A\n",
            "Iteration:  16% 21/134 [00:46<04:08,  2.20s/it]\u001b[A\n",
            "Iteration:  16% 22/134 [00:48<04:06,  2.20s/it]\u001b[A\n",
            "Iteration:  17% 23/134 [00:50<04:04,  2.20s/it]\u001b[A\n",
            "Iteration:  18% 24/134 [00:52<04:02,  2.20s/it]\u001b[A\n",
            "Iteration:  19% 25/134 [00:54<03:59,  2.20s/it]\u001b[A\n",
            "Iteration:  19% 26/134 [00:57<03:57,  2.20s/it]\u001b[A\n",
            "Iteration:  20% 27/134 [00:59<03:55,  2.20s/it]\u001b[A\n",
            "Iteration:  21% 28/134 [01:01<03:53,  2.20s/it]\u001b[A\n",
            "Iteration:  22% 29/134 [01:03<03:51,  2.20s/it]\u001b[A\n",
            "Iteration:  22% 30/134 [01:05<03:48,  2.20s/it]\u001b[A\n",
            "Iteration:  23% 31/134 [01:08<03:46,  2.20s/it]\u001b[A\n",
            "Iteration:  24% 32/134 [01:10<03:44,  2.20s/it]\u001b[A\n",
            "Iteration:  25% 33/134 [01:12<03:42,  2.20s/it]\u001b[A\n",
            "Iteration:  25% 34/134 [01:14<03:39,  2.20s/it]\u001b[A\n",
            "Iteration:  26% 35/134 [01:16<03:37,  2.20s/it]\u001b[A\n",
            "Iteration:  27% 36/134 [01:19<03:35,  2.20s/it]\u001b[A\n",
            "Iteration:  28% 37/134 [01:21<03:33,  2.20s/it]\u001b[A\n",
            "Iteration:  28% 38/134 [01:23<03:31,  2.20s/it]\u001b[A\n",
            "Iteration:  29% 39/134 [01:25<03:28,  2.20s/it]\u001b[A\n",
            "Iteration:  30% 40/134 [01:27<03:26,  2.20s/it]\u001b[A\n",
            "Iteration:  31% 41/134 [01:30<03:24,  2.20s/it]\u001b[A\n",
            "Iteration:  31% 42/134 [01:32<03:22,  2.20s/it]\u001b[A\n",
            "Iteration:  32% 43/134 [01:34<03:20,  2.20s/it]\u001b[A\n",
            "Iteration:  33% 44/134 [01:36<03:17,  2.20s/it]\u001b[A\n",
            "Iteration:  34% 45/134 [01:38<03:15,  2.20s/it]\u001b[A\n",
            "Iteration:  34% 46/134 [01:41<03:13,  2.20s/it]\u001b[A\n",
            "Iteration:  35% 47/134 [01:43<03:11,  2.20s/it]\u001b[A\n",
            "Iteration:  36% 48/134 [01:45<03:09,  2.20s/it]\u001b[A\n",
            "Iteration:  37% 49/134 [01:47<03:06,  2.20s/it]\u001b[A\n",
            "Iteration:  37% 50/134 [01:49<03:04,  2.20s/it]\u001b[A\n",
            "Iteration:  38% 51/134 [01:52<03:02,  2.20s/it]\u001b[A\n",
            "Iteration:  39% 52/134 [01:54<03:00,  2.20s/it]\u001b[A\n",
            "Iteration:  40% 53/134 [01:56<02:58,  2.20s/it]\u001b[A\n",
            "Iteration:  40% 54/134 [01:58<02:55,  2.20s/it]\u001b[A\n",
            "Iteration:  41% 55/134 [02:00<02:53,  2.20s/it]\u001b[A\n",
            "Iteration:  42% 56/134 [02:03<02:51,  2.20s/it]\u001b[A\n",
            "Iteration:  43% 57/134 [02:05<02:49,  2.20s/it]\u001b[A\n",
            "Iteration:  43% 58/134 [02:07<02:46,  2.20s/it]\u001b[A\n",
            "Iteration:  44% 59/134 [02:09<02:44,  2.20s/it]\u001b[A\n",
            "Iteration:  45% 60/134 [02:11<02:42,  2.20s/it]\u001b[A\n",
            "Iteration:  46% 61/134 [02:14<02:40,  2.20s/it]\u001b[A\n",
            "Iteration:  46% 62/134 [02:16<02:38,  2.20s/it]\u001b[A\n",
            "Iteration:  47% 63/134 [02:18<02:36,  2.20s/it]\u001b[A\n",
            "Iteration:  48% 64/134 [02:20<02:33,  2.20s/it]\u001b[A\n",
            "Iteration:  49% 65/134 [02:22<02:31,  2.20s/it]\u001b[A\n",
            "Iteration:  49% 66/134 [02:25<02:29,  2.20s/it]\u001b[A\n",
            "Iteration:  50% 67/134 [02:27<02:26,  2.19s/it]\u001b[A\n",
            "Iteration:  51% 68/134 [02:29<02:24,  2.19s/it]\u001b[A\n",
            "Iteration:  51% 69/134 [02:31<02:22,  2.19s/it]\u001b[A\n",
            "Iteration:  52% 70/134 [02:33<02:20,  2.20s/it]\u001b[A\n",
            "Iteration:  53% 71/134 [02:36<02:18,  2.19s/it]\u001b[A\n",
            "Iteration:  54% 72/134 [02:38<02:16,  2.19s/it]\u001b[A\n",
            "Iteration:  54% 73/134 [02:40<02:13,  2.19s/it]\u001b[A\n",
            "Iteration:  55% 74/134 [02:42<02:11,  2.19s/it]\u001b[A\n",
            "Iteration:  56% 75/134 [02:44<02:09,  2.20s/it]\u001b[A\n",
            "Iteration:  57% 76/134 [02:47<02:07,  2.19s/it]\u001b[A\n",
            "Iteration:  57% 77/134 [02:49<02:05,  2.20s/it]\u001b[A\n",
            "Iteration:  58% 78/134 [02:51<02:02,  2.20s/it]\u001b[A\n",
            "Iteration:  59% 79/134 [02:53<02:00,  2.19s/it]\u001b[A\n",
            "Iteration:  60% 80/134 [02:55<01:58,  2.19s/it]\u001b[A\n",
            "Iteration:  60% 81/134 [02:58<01:56,  2.19s/it]\u001b[A\n",
            "Iteration:  61% 82/134 [03:00<01:54,  2.19s/it]\u001b[A\n",
            "Iteration:  62% 83/134 [03:02<01:51,  2.19s/it]\u001b[A\n",
            "Iteration:  63% 84/134 [03:04<01:49,  2.20s/it]\u001b[A\n",
            "Iteration:  63% 85/134 [03:06<01:47,  2.20s/it]\u001b[A\n",
            "Iteration:  64% 86/134 [03:09<01:45,  2.20s/it]\u001b[A\n",
            "Iteration:  65% 87/134 [03:11<01:43,  2.20s/it]\u001b[A\n",
            "Iteration:  66% 88/134 [03:13<01:41,  2.20s/it]\u001b[A\n",
            "Iteration:  66% 89/134 [03:15<01:38,  2.20s/it]\u001b[A\n",
            "Iteration:  67% 90/134 [03:17<01:36,  2.20s/it]\u001b[A\n",
            "Iteration:  68% 91/134 [03:20<01:34,  2.20s/it]\u001b[A\n",
            "Iteration:  69% 92/134 [03:22<01:32,  2.20s/it]\u001b[A\n",
            "Iteration:  69% 93/134 [03:24<01:30,  2.20s/it]\u001b[A\n",
            "Iteration:  70% 94/134 [03:26<01:28,  2.20s/it]\u001b[A\n",
            "Iteration:  71% 95/134 [03:28<01:25,  2.20s/it]\u001b[A\n",
            "Iteration:  72% 96/134 [03:31<01:23,  2.20s/it]\u001b[A\n",
            "Iteration:  72% 97/134 [03:33<01:21,  2.20s/it]\u001b[A\n",
            "Iteration:  73% 98/134 [03:35<01:19,  2.20s/it]\u001b[A\n",
            "Iteration:  74% 99/134 [03:37<01:17,  2.20s/it]\u001b[A\n",
            "Iteration:  75% 100/134 [03:39<01:14,  2.20s/it]\u001b[A\n",
            "Iteration:  75% 101/134 [03:42<01:12,  2.20s/it]\u001b[A\n",
            "Iteration:  76% 102/134 [03:44<01:10,  2.20s/it]\u001b[A\n",
            "Iteration:  77% 103/134 [03:46<01:08,  2.20s/it]\u001b[A\n",
            "Iteration:  78% 104/134 [03:48<01:05,  2.20s/it]\u001b[A\n",
            "Iteration:  78% 105/134 [03:50<01:03,  2.20s/it]\u001b[A\n",
            "Iteration:  79% 106/134 [03:53<01:01,  2.20s/it]\u001b[A\n",
            "Iteration:  80% 107/134 [03:55<00:59,  2.20s/it]\u001b[A\n",
            "Iteration:  81% 108/134 [03:57<00:57,  2.20s/it]\u001b[A\n",
            "Iteration:  81% 109/134 [03:59<00:55,  2.20s/it]\u001b[A\n",
            "Iteration:  82% 110/134 [04:01<00:52,  2.20s/it]\u001b[A\n",
            "Iteration:  83% 111/134 [04:04<00:50,  2.20s/it]\u001b[A\n",
            "Iteration:  84% 112/134 [04:06<00:48,  2.20s/it]\u001b[A\n",
            "Iteration:  84% 113/134 [04:08<00:46,  2.20s/it]\u001b[A\n",
            "Iteration:  85% 114/134 [04:10<00:43,  2.20s/it]\u001b[A\n",
            "Iteration:  86% 115/134 [04:12<00:41,  2.20s/it]\u001b[A\n",
            "Iteration:  87% 116/134 [04:15<00:39,  2.20s/it]\u001b[A\n",
            "Iteration:  87% 117/134 [04:17<00:37,  2.20s/it]\u001b[A\n",
            "Iteration:  88% 118/134 [04:19<00:35,  2.20s/it]\u001b[A\n",
            "Iteration:  89% 119/134 [04:21<00:32,  2.20s/it]\u001b[A\n",
            "Iteration:  90% 120/134 [04:23<00:30,  2.20s/it]\u001b[A\n",
            "Iteration:  90% 121/134 [04:26<00:28,  2.20s/it]\u001b[A\n",
            "Iteration:  91% 122/134 [04:28<00:26,  2.20s/it]\u001b[A\n",
            "Iteration:  92% 123/134 [04:30<00:24,  2.20s/it]\u001b[A\n",
            "Iteration:  93% 124/134 [04:32<00:22,  2.20s/it]\u001b[A\n",
            "Iteration:  93% 125/134 [04:34<00:19,  2.20s/it]\u001b[A\n",
            "Iteration:  94% 126/134 [04:37<00:17,  2.20s/it]\u001b[A\n",
            "Iteration:  95% 127/134 [04:39<00:15,  2.20s/it]\u001b[A\n",
            "Iteration:  96% 128/134 [04:41<00:13,  2.20s/it]\u001b[A\n",
            "Iteration:  96% 129/134 [04:43<00:11,  2.20s/it]\u001b[A\n",
            "Iteration:  97% 130/134 [04:45<00:08,  2.20s/it]\u001b[A\n",
            "Iteration:  98% 131/134 [04:48<00:06,  2.20s/it]\u001b[A\n",
            "Iteration:  99% 132/134 [04:50<00:04,  2.20s/it]\u001b[A\n",
            "Iteration:  99% 133/134 [04:52<00:02,  2.20s/it]\u001b[A\n",
            "Iteration: 100% 134/134 [04:53<00:00,  2.19s/it]\n",
            "Epoch: 100% 3/3 [14:39<00:00, 293.32s/it]\n",
            "05/15/2020 16:39:58 - INFO - __main__ -    global_step = 402, average loss = 0.4378391570638661\n",
            "05/15/2020 16:39:58 - INFO - __main__ -   Saving model checkpoint to run\n",
            "05/15/2020 16:39:58 - INFO - transformers.configuration_utils -   Configuration saved in run/config.json\n",
            "05/15/2020 16:40:06 - INFO - transformers.modeling_utils -   Model weights saved in run/pytorch_model.bin\n",
            "05/15/2020 16:40:08 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n",
            "05/15/2020 16:40:08 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 16:40:08 - INFO - transformers.modeling_utils -   loading weights file run/pytorch_model.bin\n",
            "05/15/2020 16:40:14 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n",
            "05/15/2020 16:40:14 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   Model name 'run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'run' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   Didn't find file run/added_tokens.json. We won't load it.\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file run/vocab.json\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file run/merges.txt\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file run/special_tokens_map.json\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file run/tokenizer_config.json\n",
            "05/15/2020 16:40:14 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n",
            "05/15/2020 16:40:14 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   Model name 'run' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'run' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   Didn't find file run/added_tokens.json. We won't load it.\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file run/vocab.json\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file run/merges.txt\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file None\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file run/special_tokens_map.json\n",
            "05/15/2020 16:40:14 - INFO - transformers.tokenization_utils -   loading file run/tokenizer_config.json\n",
            "05/15/2020 16:40:14 - INFO - __main__ -   Evaluate the following checkpoints: ['run']\n",
            "05/15/2020 16:40:14 - INFO - transformers.configuration_utils -   loading configuration file run/config.json\n",
            "05/15/2020 16:40:14 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "05/15/2020 16:40:14 - INFO - transformers.modeling_utils -   loading weights file run/pytorch_model.bin\n",
            "05/15/2020 16:40:22 - INFO - __main__ -   Creating features from dataset file at data/glue/CoLA\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   Writing example 0/1043\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   guid: dev-0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   input_ids: 0 20 21362 12783 5 24572 699 9 5 10889 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   input_ids: 0 20 23341 156 5 17434 4140 81 5 25578 607 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   input_ids: 0 20 12418 19495 14902 11702 1329 1495 7082 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   input_ids: 0 318 47 56 18804 55 6 47 74 236 540 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   input_ids: 0 287 47 3529 5 144 6 47 236 5 513 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "05/15/2020 16:40:22 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "05/15/2020 16:40:23 - INFO - __main__ -   Saving features into cached file data/glue/CoLA/cached_dev_baseline_128_cola\n",
            "05/15/2020 16:40:23 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "05/15/2020 16:40:23 - INFO - __main__ -     Num examples = 1043\n",
            "05/15/2020 16:40:23 - INFO - __main__ -     Batch size = 64\n",
            "Evaluating: 100% 17/17 [00:13<00:00,  1.28it/s]\n",
            "05/15/2020 16:40:37 - INFO - __main__ -   ***** Eval results  *****\n",
            "05/15/2020 16:40:37 - INFO - __main__ -     mcc = 0.508365554162712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbrHZsJT8LVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}