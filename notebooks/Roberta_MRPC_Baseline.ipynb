{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Roberta-MRPC-Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeaXI_L0zrsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c70ee0ad-3c8e-406e-da5f-c26b167c189f"
      },
      "source": [
        "  !git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 23566 (delta 27), reused 32 (delta 13), pack-reused 23498\u001b[K\n",
            "Receiving objects: 100% (23566/23566), 14.04 MiB | 5.06 MiB/s, done.\n",
            "Resolving deltas: 100% (16689/16689), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8kGpsyo0ByJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a68093ec-3e30-4e27-9892-723641b422cf"
      },
      "source": [
        "cd transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC80P8b_0FTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e93d03c5-edb5-4754-bf46-aa66ce9d0f14"
      },
      "source": [
        "!pip install -r ./examples/requirements.txt\n",
        "!pip install boto3 filelock requests tqdm sentencepiece sacremoses tokenizers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 24.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 27.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 29.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 31.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 31.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 31.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 26.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 5)) (5.4.8)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/58/5c6cc352ea6271125325950715cf8b59b77abe5e93cf29f6e60b491a31d9/sacrebleu-1.4.6-py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.5MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/6d/2b9a64cba1e4e6ecd4effbf6834b2592b54dc813654f84029758e5daeeb5/rouge_score-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r ./examples/requirements.txt (line 8)) (2.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r ./examples/requirements.txt (line 1)) (1.18.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.27.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (46.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.6.0.post2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r ./examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r ./examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r ./examples/requirements.txt (line 4)) (2.2.5)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/64/03/9abfb3374d67838daf24f1a388528714bec1debb1d13749f0abd7fb07cfb/portalocker-1.6.0-py2.py3-none-any.whl\n",
            "Collecting mecab-python3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/49/b55a839a77189042960bf96490640c44816073f917d489acbc5d79fa5cc3/mecab_python3-0.996.5-cp36-cp36m-manylinux2010_x86_64.whl (17.1MB)\n",
            "\u001b[K     |████████████████████████████████| 17.1MB 212kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r ./examples/requirements.txt (line 6)) (3.6.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->-r ./examples/requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (4.38.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (19.3.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.3.1.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (0.21.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r ./examples/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r ./examples/requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r ./examples/requirements.txt (line 8)) (1.51.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r ./examples/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r ./examples/requirements.txt (line 2)) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=0f24f9cd921df9c80e8487847c2b479daaeb20ff6ee41cdedc99116706fc923c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tensorboardX, seqeval, portalocker, mecab-python3, sacrebleu, rouge-score\n",
            "Successfully installed mecab-python3-0.996.5 portalocker-1.6.0 rouge-score-0.0.3 sacrebleu-1.4.6 seqeval-0.0.12 tensorboardX-2.0\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.12.33)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 24.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 58.8MB/s \n",
            "\u001b[?25hCollecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/de/ec55e2d5a8720557b25100dd7dd4a63108a44b6b303978ce2587666931cf/tokenizers-0.6.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.15.33)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.3.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=7e9c95fcdad1ba5d99b590e2aa15edf418d4b2bb3a84abe2f0585eea60f801fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7Q3nMTc0GUS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "fe54c364-0b54-4c53-9432-cd9ca7460135"
      },
      "source": [
        "GLUE_DIR=\"content/\"\n",
        "!python ./utils/download_glue_data.py --data_dir $GLUE_DIR --tasks MRPC"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing MRPC...\n",
            "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb7T3qct0Rsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TASK_NAME=\"MRPC\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd11nN8h0TVI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07554435-7e81-4d95-9bca-62c3645fba41"
      },
      "source": [
        "%set_env PYTHONPATH=/content/transformers/src:/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=/content/transformers/src:/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkRlpVLl0U3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2396aeda-d315-4b96-e65c-30a8fc2c9ce7"
      },
      "source": [
        "!python ./examples/run_glue.py \\\n",
        "    --model_type roberta \\\n",
        "    --model_name_or_path roberta-base \\\n",
        "    --task_name $TASK_NAME \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --data_dir $GLUE_DIR/$TASK_NAME \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=64   \\\n",
        "    --per_gpu_train_batch_size=64   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --output_dir ../${TASK_NAME}_run \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-05 14:54:50.007541: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "04/05/2020 14:54:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/05/2020 14:54:52 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "04/05/2020 14:54:52 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 14:54:53 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "04/05/2020 14:54:53 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 14:54:55 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "04/05/2020 14:54:55 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "04/05/2020 14:54:56 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "04/05/2020 14:54:59 - INFO - transformers.modeling_utils -   Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "04/05/2020 14:54:59 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "04/05/2020 14:55:04 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='content//MRPC', device=device(type='cuda'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='../', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=64, save_steps=500, seed=42, server_ip='', server_port='', task_name='mrpc', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "04/05/2020 14:55:04 - INFO - __main__ -   Creating features from dataset file at content//MRPC\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   LOOKING AT content//MRPC/train.tsv\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   Writing example 0/3668\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   guid: train-1\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   input_ids: 0 1918 1001 6182 1238 39 2138 2156 2661 37 373 22 5 4562 22 2156 9 12507 7018 23817 39 1283 479 2 2 24551 4506 7 123 25 129 22 5 4562 22 2156 1918 1001 6182 1238 39 2138 9 12507 7018 23817 39 1283 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   guid: train-2\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   input_ids: 0 854 26802 1588 102 2164 13976 1758 128 29 137 2183 5 3206 7 11881 10564 11 6708 13 68 132 4 245 325 479 2 2 854 26802 1588 102 2162 13976 1758 128 29 11 7969 13 68 231 6478 153 8 1088 24 7 11881 10564 13 68 112 4 398 325 11 6708 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   guid: train-3\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   input_ids: 0 252 56 1027 41 6859 15 5 3742 15 502 158 2156 1839 5 9145 13 1392 2156 37 355 479 2 2 374 502 158 2156 5 3627 128 29 2203 56 1027 41 6859 15 5 3742 2156 1839 5 16174 13 1392 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   guid: train-4\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   input_ids: 0 8582 321 28020 5050 2156 12765 327 58 62 753 3205 2156 50 204 4 306 7606 2156 23 83 68 204 4 4419 2156 519 656 278 10 638 239 9 83 68 204 4 4390 479 2 2 12765 327 4262 291 3205 2156 50 204 4 401 7606 2156 7 278 10 638 3172 239 23 83 68 204 4 4390 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   guid: train-5\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   input_ids: 0 20 388 1458 68 132 4 1225 2156 50 59 365 135 2156 7 593 273 23 68 733 4 4708 15 5 188 469 3412 3080 479 2 2 14499 359 381 1913 4 327 4262 68 112 4 5449 50 290 135 7 68 733 4 3933 15 5 188 469 3412 3080 15 273 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:55:04 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "04/05/2020 14:55:06 - INFO - __main__ -   Saving features into cached file content//MRPC/cached_train_roberta-base_128_mrpc\n",
            "04/05/2020 14:55:07 - INFO - __main__ -   ***** Running training *****\n",
            "04/05/2020 14:55:07 - INFO - __main__ -     Num examples = 3668\n",
            "04/05/2020 14:55:07 - INFO - __main__ -     Num Epochs = 3\n",
            "04/05/2020 14:55:07 - INFO - __main__ -     Instantaneous batch size per GPU = 64\n",
            "04/05/2020 14:55:07 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "04/05/2020 14:55:07 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "04/05/2020 14:55:07 - INFO - __main__ -     Total optimization steps = 174\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/58 [00:01<01:20,  1.41s/it]\u001b[A\n",
            "Iteration:   3% 2/58 [00:02<01:15,  1.34s/it]\u001b[A\n",
            "Iteration:   5% 3/58 [00:03<01:11,  1.30s/it]\u001b[A\n",
            "Iteration:   7% 4/58 [00:04<01:08,  1.27s/it]\u001b[A\n",
            "Iteration:   9% 5/58 [00:06<01:06,  1.25s/it]\u001b[A\n",
            "Iteration:  10% 6/58 [00:07<01:04,  1.23s/it]\u001b[A\n",
            "Iteration:  12% 7/58 [00:08<01:02,  1.22s/it]\u001b[A\n",
            "Iteration:  14% 8/58 [00:09<01:00,  1.22s/it]\u001b[A\n",
            "Iteration:  16% 9/58 [00:10<00:59,  1.21s/it]\u001b[A\n",
            "Iteration:  17% 10/58 [00:12<00:58,  1.21s/it]\u001b[A\n",
            "Iteration:  19% 11/58 [00:13<00:56,  1.21s/it]\u001b[A\n",
            "Iteration:  21% 12/58 [00:14<00:55,  1.21s/it]\u001b[A\n",
            "Iteration:  22% 13/58 [00:15<00:54,  1.21s/it]\u001b[A\n",
            "Iteration:  24% 14/58 [00:17<00:53,  1.21s/it]\u001b[A\n",
            "Iteration:  26% 15/58 [00:18<00:52,  1.22s/it]\u001b[A\n",
            "Iteration:  28% 16/58 [00:19<00:51,  1.22s/it]\u001b[A\n",
            "Iteration:  29% 17/58 [00:20<00:49,  1.22s/it]\u001b[A\n",
            "Iteration:  31% 18/58 [00:21<00:48,  1.22s/it]\u001b[A\n",
            "Iteration:  33% 19/58 [00:23<00:47,  1.22s/it]\u001b[A\n",
            "Iteration:  34% 20/58 [00:24<00:46,  1.22s/it]\u001b[A\n",
            "Iteration:  36% 21/58 [00:25<00:45,  1.22s/it]\u001b[A\n",
            "Iteration:  38% 22/58 [00:26<00:43,  1.22s/it]\u001b[A\n",
            "Iteration:  40% 23/58 [00:28<00:42,  1.23s/it]\u001b[A\n",
            "Iteration:  41% 24/58 [00:29<00:41,  1.23s/it]\u001b[A\n",
            "Iteration:  43% 25/58 [00:30<00:40,  1.23s/it]\u001b[A\n",
            "Iteration:  45% 26/58 [00:31<00:39,  1.24s/it]\u001b[A\n",
            "Iteration:  47% 27/58 [00:32<00:38,  1.23s/it]\u001b[A\n",
            "Iteration:  48% 28/58 [00:34<00:37,  1.23s/it]\u001b[A\n",
            "Iteration:  50% 29/58 [00:35<00:35,  1.23s/it]\u001b[A\n",
            "Iteration:  52% 30/58 [00:36<00:34,  1.23s/it]\u001b[A\n",
            "Iteration:  53% 31/58 [00:37<00:33,  1.24s/it]\u001b[A\n",
            "Iteration:  55% 32/58 [00:39<00:32,  1.24s/it]\u001b[A\n",
            "Iteration:  57% 33/58 [00:40<00:30,  1.24s/it]\u001b[A\n",
            "Iteration:  59% 34/58 [00:41<00:29,  1.24s/it]\u001b[A\n",
            "Iteration:  60% 35/58 [00:42<00:28,  1.24s/it]\u001b[A\n",
            "Iteration:  62% 36/58 [00:44<00:27,  1.24s/it]\u001b[A\n",
            "Iteration:  64% 37/58 [00:45<00:26,  1.25s/it]\u001b[A\n",
            "Iteration:  66% 38/58 [00:46<00:25,  1.25s/it]\u001b[A\n",
            "Iteration:  67% 39/58 [00:47<00:23,  1.25s/it]\u001b[A\n",
            "Iteration:  69% 40/58 [00:49<00:22,  1.26s/it]\u001b[A\n",
            "Iteration:  71% 41/58 [00:50<00:21,  1.26s/it]\u001b[A\n",
            "Iteration:  72% 42/58 [00:51<00:20,  1.26s/it]\u001b[A\n",
            "Iteration:  74% 43/58 [00:53<00:19,  1.27s/it]\u001b[A\n",
            "Iteration:  76% 44/58 [00:54<00:17,  1.27s/it]\u001b[A\n",
            "Iteration:  78% 45/58 [00:55<00:16,  1.27s/it]\u001b[A\n",
            "Iteration:  79% 46/58 [00:56<00:15,  1.28s/it]\u001b[A\n",
            "Iteration:  81% 47/58 [00:58<00:14,  1.28s/it]\u001b[A\n",
            "Iteration:  83% 48/58 [00:59<00:12,  1.28s/it]\u001b[A\n",
            "Iteration:  84% 49/58 [01:00<00:11,  1.28s/it]\u001b[A\n",
            "Iteration:  86% 50/58 [01:02<00:10,  1.29s/it]\u001b[A\n",
            "Iteration:  88% 51/58 [01:03<00:09,  1.29s/it]\u001b[A\n",
            "Iteration:  90% 52/58 [01:04<00:07,  1.30s/it]\u001b[A\n",
            "Iteration:  91% 53/58 [01:05<00:06,  1.30s/it]\u001b[A\n",
            "Iteration:  93% 54/58 [01:07<00:05,  1.30s/it]\u001b[A\n",
            "Iteration:  95% 55/58 [01:08<00:03,  1.30s/it]\u001b[A\n",
            "Iteration:  97% 56/58 [01:09<00:02,  1.30s/it]\u001b[A\n",
            "Iteration:  98% 57/58 [01:11<00:01,  1.30s/it]\u001b[A\n",
            "Iteration: 100% 58/58 [01:11<00:00,  1.23s/it]\n",
            "Epoch:  33% 1/3 [01:11<02:23, 71.59s/it]\n",
            "Iteration:   0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/58 [00:01<01:14,  1.31s/it]\u001b[A\n",
            "Iteration:   3% 2/58 [00:02<01:13,  1.31s/it]\u001b[A\n",
            "Iteration:   5% 3/58 [00:03<01:11,  1.31s/it]\u001b[A\n",
            "Iteration:   7% 4/58 [00:05<01:10,  1.31s/it]\u001b[A\n",
            "Iteration:   9% 5/58 [00:06<01:09,  1.31s/it]\u001b[A\n",
            "Iteration:  10% 6/58 [00:07<01:08,  1.31s/it]\u001b[A\n",
            "Iteration:  12% 7/58 [00:09<01:07,  1.32s/it]\u001b[A\n",
            "Iteration:  14% 8/58 [00:10<01:05,  1.32s/it]\u001b[A\n",
            "Iteration:  16% 9/58 [00:11<01:04,  1.32s/it]\u001b[A\n",
            "Iteration:  17% 10/58 [00:13<01:03,  1.33s/it]\u001b[A\n",
            "Iteration:  19% 11/58 [00:14<01:02,  1.33s/it]\u001b[A\n",
            "Iteration:  21% 12/58 [00:15<01:01,  1.33s/it]\u001b[A\n",
            "Iteration:  22% 13/58 [00:17<01:00,  1.34s/it]\u001b[A\n",
            "Iteration:  24% 14/58 [00:18<00:58,  1.34s/it]\u001b[A\n",
            "Iteration:  26% 15/58 [00:19<00:57,  1.34s/it]\u001b[A\n",
            "Iteration:  28% 16/58 [00:21<00:56,  1.35s/it]\u001b[A\n",
            "Iteration:  29% 17/58 [00:22<00:55,  1.35s/it]\u001b[A\n",
            "Iteration:  31% 18/58 [00:23<00:54,  1.35s/it]\u001b[A\n",
            "Iteration:  33% 19/58 [00:25<00:52,  1.35s/it]\u001b[A\n",
            "Iteration:  34% 20/58 [00:26<00:51,  1.36s/it]\u001b[A\n",
            "Iteration:  36% 21/58 [00:28<00:50,  1.36s/it]\u001b[A\n",
            "Iteration:  38% 22/58 [00:29<00:49,  1.37s/it]\u001b[A\n",
            "Iteration:  40% 23/58 [00:30<00:47,  1.37s/it]\u001b[A\n",
            "Iteration:  41% 24/58 [00:32<00:46,  1.37s/it]\u001b[A\n",
            "Iteration:  43% 25/58 [00:33<00:45,  1.38s/it]\u001b[A\n",
            "Iteration:  45% 26/58 [00:34<00:44,  1.38s/it]\u001b[A\n",
            "Iteration:  47% 27/58 [00:36<00:42,  1.38s/it]\u001b[A\n",
            "Iteration:  48% 28/58 [00:37<00:41,  1.38s/it]\u001b[A\n",
            "Iteration:  50% 29/58 [00:39<00:40,  1.38s/it]\u001b[A\n",
            "Iteration:  52% 30/58 [00:40<00:38,  1.38s/it]\u001b[A\n",
            "Iteration:  53% 31/58 [00:41<00:37,  1.38s/it]\u001b[A\n",
            "Iteration:  55% 32/58 [00:43<00:35,  1.38s/it]\u001b[A\n",
            "Iteration:  57% 33/58 [00:44<00:34,  1.38s/it]\u001b[A\n",
            "Iteration:  59% 34/58 [00:46<00:33,  1.38s/it]\u001b[A\n",
            "Iteration:  60% 35/58 [00:47<00:31,  1.37s/it]\u001b[A\n",
            "Iteration:  62% 36/58 [00:48<00:30,  1.37s/it]\u001b[A\n",
            "Iteration:  64% 37/58 [00:50<00:28,  1.37s/it]\u001b[A\n",
            "Iteration:  66% 38/58 [00:51<00:27,  1.37s/it]\u001b[A\n",
            "Iteration:  67% 39/58 [00:52<00:25,  1.36s/it]\u001b[A\n",
            "Iteration:  69% 40/58 [00:54<00:24,  1.36s/it]\u001b[A\n",
            "Iteration:  71% 41/58 [00:55<00:23,  1.36s/it]\u001b[A\n",
            "Iteration:  72% 42/58 [00:56<00:21,  1.36s/it]\u001b[A\n",
            "Iteration:  74% 43/58 [00:58<00:20,  1.35s/it]\u001b[A\n",
            "Iteration:  76% 44/58 [00:59<00:18,  1.35s/it]\u001b[A\n",
            "Iteration:  78% 45/58 [01:00<00:17,  1.35s/it]\u001b[A\n",
            "Iteration:  79% 46/58 [01:02<00:16,  1.35s/it]\u001b[A\n",
            "Iteration:  81% 47/58 [01:03<00:14,  1.35s/it]\u001b[A\n",
            "Iteration:  83% 48/58 [01:04<00:13,  1.35s/it]\u001b[A\n",
            "Iteration:  84% 49/58 [01:06<00:12,  1.34s/it]\u001b[A\n",
            "Iteration:  86% 50/58 [01:07<00:10,  1.34s/it]\u001b[A\n",
            "Iteration:  88% 51/58 [01:08<00:09,  1.34s/it]\u001b[A\n",
            "Iteration:  90% 52/58 [01:10<00:08,  1.34s/it]\u001b[A\n",
            "Iteration:  91% 53/58 [01:11<00:06,  1.34s/it]\u001b[A\n",
            "Iteration:  93% 54/58 [01:13<00:05,  1.34s/it]\u001b[A\n",
            "Iteration:  95% 55/58 [01:14<00:04,  1.34s/it]\u001b[A\n",
            "Iteration:  97% 56/58 [01:15<00:02,  1.34s/it]\u001b[A\n",
            "Iteration:  98% 57/58 [01:17<00:01,  1.34s/it]\u001b[A\n",
            "Iteration: 100% 58/58 [01:17<00:00,  1.34s/it]\n",
            "Epoch:  67% 2/3 [02:29<01:13, 73.36s/it]\n",
            "Iteration:   0% 0/58 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   2% 1/58 [00:01<01:15,  1.33s/it]\u001b[A\n",
            "Iteration:   3% 2/58 [00:02<01:14,  1.33s/it]\u001b[A\n",
            "Iteration:   5% 3/58 [00:04<01:13,  1.33s/it]\u001b[A\n",
            "Iteration:   7% 4/58 [00:05<01:12,  1.34s/it]\u001b[A\n",
            "Iteration:   9% 5/58 [00:06<01:10,  1.34s/it]\u001b[A\n",
            "Iteration:  10% 6/58 [00:08<01:09,  1.34s/it]\u001b[A\n",
            "Iteration:  12% 7/58 [00:09<01:08,  1.34s/it]\u001b[A\n",
            "Iteration:  14% 8/58 [00:10<01:07,  1.34s/it]\u001b[A\n",
            "Iteration:  16% 9/58 [00:12<01:05,  1.34s/it]\u001b[A\n",
            "Iteration:  17% 10/58 [00:13<01:04,  1.34s/it]\u001b[A\n",
            "Iteration:  19% 11/58 [00:14<01:03,  1.34s/it]\u001b[A\n",
            "Iteration:  21% 12/58 [00:16<01:01,  1.34s/it]\u001b[A\n",
            "Iteration:  22% 13/58 [00:17<01:00,  1.34s/it]\u001b[A\n",
            "Iteration:  24% 14/58 [00:18<00:59,  1.34s/it]\u001b[A\n",
            "Iteration:  26% 15/58 [00:20<00:57,  1.35s/it]\u001b[A\n",
            "Iteration:  28% 16/58 [00:21<00:56,  1.35s/it]\u001b[A\n",
            "Iteration:  29% 17/58 [00:22<00:55,  1.35s/it]\u001b[A\n",
            "Iteration:  31% 18/58 [00:24<00:53,  1.35s/it]\u001b[A\n",
            "Iteration:  33% 19/58 [00:25<00:52,  1.35s/it]\u001b[A\n",
            "Iteration:  34% 20/58 [00:26<00:51,  1.35s/it]\u001b[A\n",
            "Iteration:  36% 21/58 [00:28<00:49,  1.35s/it]\u001b[A\n",
            "Iteration:  38% 22/58 [00:29<00:48,  1.35s/it]\u001b[A\n",
            "Iteration:  40% 23/58 [00:30<00:47,  1.35s/it]\u001b[A\n",
            "Iteration:  41% 24/58 [00:32<00:45,  1.35s/it]\u001b[A\n",
            "Iteration:  43% 25/58 [00:33<00:44,  1.35s/it]\u001b[A\n",
            "Iteration:  45% 26/58 [00:34<00:43,  1.35s/it]\u001b[A\n",
            "Iteration:  47% 27/58 [00:36<00:41,  1.35s/it]\u001b[A\n",
            "Iteration:  48% 28/58 [00:37<00:40,  1.36s/it]\u001b[A\n",
            "Iteration:  50% 29/58 [00:39<00:39,  1.35s/it]\u001b[A\n",
            "Iteration:  52% 30/58 [00:40<00:37,  1.35s/it]\u001b[A\n",
            "Iteration:  53% 31/58 [00:41<00:36,  1.35s/it]\u001b[A\n",
            "Iteration:  55% 32/58 [00:43<00:35,  1.36s/it]\u001b[A\n",
            "Iteration:  57% 33/58 [00:44<00:33,  1.35s/it]\u001b[A\n",
            "Iteration:  59% 34/58 [00:45<00:32,  1.35s/it]\u001b[A\n",
            "Iteration:  60% 35/58 [00:47<00:31,  1.35s/it]\u001b[A\n",
            "Iteration:  62% 36/58 [00:48<00:29,  1.36s/it]\u001b[A\n",
            "Iteration:  64% 37/58 [00:49<00:28,  1.36s/it]\u001b[A\n",
            "Iteration:  66% 38/58 [00:51<00:27,  1.36s/it]\u001b[A\n",
            "Iteration:  67% 39/58 [00:52<00:25,  1.36s/it]\u001b[A\n",
            "Iteration:  69% 40/58 [00:53<00:24,  1.36s/it]\u001b[A\n",
            "Iteration:  71% 41/58 [00:55<00:23,  1.36s/it]\u001b[A\n",
            "Iteration:  72% 42/58 [00:56<00:21,  1.36s/it]\u001b[A\n",
            "Iteration:  74% 43/58 [00:58<00:20,  1.36s/it]\u001b[A\n",
            "Iteration:  76% 44/58 [00:59<00:19,  1.36s/it]\u001b[A\n",
            "Iteration:  78% 45/58 [01:00<00:17,  1.36s/it]\u001b[A\n",
            "Iteration:  79% 46/58 [01:02<00:16,  1.36s/it]\u001b[A\n",
            "Iteration:  81% 47/58 [01:03<00:14,  1.36s/it]\u001b[A\n",
            "Iteration:  83% 48/58 [01:04<00:13,  1.35s/it]\u001b[A\n",
            "Iteration:  84% 49/58 [01:06<00:12,  1.36s/it]\u001b[A\n",
            "Iteration:  86% 50/58 [01:07<00:10,  1.36s/it]\u001b[A\n",
            "Iteration:  88% 51/58 [01:08<00:09,  1.35s/it]\u001b[A\n",
            "Iteration:  90% 52/58 [01:10<00:08,  1.35s/it]\u001b[A\n",
            "Iteration:  91% 53/58 [01:11<00:06,  1.35s/it]\u001b[A\n",
            "Iteration:  93% 54/58 [01:12<00:05,  1.35s/it]\u001b[A\n",
            "Iteration:  95% 55/58 [01:14<00:04,  1.35s/it]\u001b[A\n",
            "Iteration:  97% 56/58 [01:15<00:02,  1.35s/it]\u001b[A\n",
            "Iteration:  98% 57/58 [01:17<00:01,  1.35s/it]\u001b[A\n",
            "Iteration: 100% 58/58 [01:17<00:00,  1.34s/it]\n",
            "Epoch: 100% 3/3 [03:46<00:00, 75.52s/it]\n",
            "04/05/2020 14:58:53 - INFO - __main__ -    global_step = 174, average loss = 0.4440720050499357\n",
            "04/05/2020 14:58:53 - INFO - __main__ -   Saving model checkpoint to ../\n",
            "04/05/2020 14:58:53 - INFO - transformers.configuration_utils -   Configuration saved in ../config.json\n",
            "04/05/2020 14:58:54 - INFO - transformers.modeling_utils -   Model weights saved in ../pytorch_model.bin\n",
            "04/05/2020 14:58:55 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
            "04/05/2020 14:58:55 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 14:58:55 - INFO - transformers.modeling_utils -   loading weights file ../pytorch_model.bin\n",
            "04/05/2020 14:58:58 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
            "04/05/2020 14:58:58 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   Model name '../' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   Didn't find file ../added_tokens.json. We won't load it.\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file ../vocab.json\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file ../merges.txt\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file ../special_tokens_map.json\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file ../tokenizer_config.json\n",
            "04/05/2020 14:58:58 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
            "04/05/2020 14:58:58 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   Model name '../' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming '../' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   Didn't find file ../added_tokens.json. We won't load it.\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file ../vocab.json\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file ../merges.txt\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file None\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file ../special_tokens_map.json\n",
            "04/05/2020 14:58:58 - INFO - transformers.tokenization_utils -   loading file ../tokenizer_config.json\n",
            "04/05/2020 14:58:58 - INFO - __main__ -   Evaluate the following checkpoints: ['../']\n",
            "04/05/2020 14:58:58 - INFO - transformers.configuration_utils -   loading configuration file ../config.json\n",
            "04/05/2020 14:58:58 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"_num_labels\": 2,\n",
            "  \"architectures\": [\n",
            "    \"RobertaForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/05/2020 14:58:58 - INFO - transformers.modeling_utils -   loading weights file ../pytorch_model.bin\n",
            "04/05/2020 14:59:02 - INFO - __main__ -   Creating features from dataset file at content//MRPC\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   Writing example 0/408\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   guid: dev-1\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   input_ids: 0 91 26 5 689 11131 11637 265 630 128 90 2564 5 138 128 29 251 12 1279 434 1860 479 2 2 22 20 689 11131 11637 265 473 45 2564 84 251 12 1279 434 1860 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   guid: dev-2\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   input_ids: 0 15683 1322 10054 26 14822 636 1242 19975 5 7780 4304 8 1415 556 7 634 39 251 107 9 1058 11 5 997 479 2 2 832 1141 26 37 21 22 727 135 639 1655 3516 22 8 1415 556 7 634 39 107 9 1058 11 5 997 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   guid: dev-3\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   input_ids: 0 20 1404 21 23 15966 4 6617 4796 136 5 4796 2156 3269 15 5 1852 2156 8 23 112 4 2517 6468 136 5 5092 13638 2156 67 3269 479 2 2 20 1404 21 23 15966 4 5479 4796 14621 975 5457 2156 8077 3269 15 5 1852 2156 8 23 112 4 2517 5339 136 5 5092 13638 3858 597 5457 2156 159 321 4 134 135 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   guid: dev-4\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   input_ids: 0 20 16305 12 347 6454 16 2445 454 779 7 2845 114 24 40 18839 10 1984 479 2 2 20 16305 12 347 6454 585 307 14 24 40 2845 11 779 549 7 18839 10 1984 137 5 19050 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   label: 1 (id = 1)\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   *** Example ***\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   guid: dev-5\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   input_ids: 0 440 5461 33 57 278 13 5 2366 50 5 1837 1500 479 2 2 440 5461 33 57 278 13 5 1837 50 2366 1200 2156 53 18966 607 34 4407 45 2181 479 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   attention_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   token_type_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/05/2020 14:59:02 - INFO - transformers.data.processors.glue -   label: 0 (id = 0)\n",
            "04/05/2020 14:59:02 - INFO - __main__ -   Saving features into cached file content//MRPC/cached_dev_roberta-base_128_mrpc\n",
            "04/05/2020 14:59:02 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "04/05/2020 14:59:02 - INFO - __main__ -     Num examples = 408\n",
            "04/05/2020 14:59:02 - INFO - __main__ -     Batch size = 64\n",
            "Evaluating: 100% 7/7 [00:02<00:00,  2.49it/s]\n",
            "04/05/2020 14:59:05 - INFO - __main__ -   ***** Eval results  *****\n",
            "04/05/2020 14:59:05 - INFO - __main__ -     acc = 0.8627450980392157\n",
            "04/05/2020 14:59:05 - INFO - __main__ -     acc_and_f1 = 0.8819025843552969\n",
            "04/05/2020 14:59:05 - INFO - __main__ -     f1 = 0.901060070671378\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}